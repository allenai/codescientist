id	batch_name	experiment_name	original_idea	runtime_minutes	total_cost	num_iterations_run	code_num_lines	code_num_tokens	status	status_numerical	interesting_results	faithfullness_category	hypothesis_category	results_summary	summary_medium	hypothesis
158925847589	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	wordnet-cooking-exploration-copy1	Using WordNet's food-related semantic hierarchies to guide exploration in cooking tasks.	40.99612568333334	0.7664184	4	355	2923	completed	1	0	faithful	inconclusive	WordNet-guided agent performed slightly but not significantly better than random baseline in CookingWorld tasks.	A WordNet-guided agent was compared to a random baseline on CookingWorld tasks across 20 episodes. The WordNet agent achieved a mean score of 0.161 versus 0.139 for random baseline, but this difference was not statistically significant (p=0.2682). The results suggest potential but inconclusive benefits of WordNet guidance for this task.	Using WordNet-guided exploration to identify and prioritize food-related actions will improve task efficiency in CookingWorld environments compared to random exploration.
851595307701	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	wordnet-cooking-exploration-copy2	Using WordNet's food-related semantic hierarchies to guide exploration in cooking tasks.	185.68604783333333	0.9186184500000001	5	287	2432	completed	1	1	faithful	support	WordNet-guided agent showed trending better performance than random baseline in CookingWorld tasks.	A WordNet-guided agent was compared against a random baseline on CookingWorld tasks across 20 episodes. The WordNet agent achieved higher average scores (0.258 vs 0.153) with the difference trending toward significance (p=0.052), suggesting WordNet guidance may improve task efficiency, though neither agent achieved full task success.	WordNet-guided exploration improves efficiency in CookingWorld tasks compared to random exploration.
77410108965	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	wordnet-cooking-exploration-copy3	Using WordNet's food-related semantic hierarchies to guide exploration in cooking tasks.	192.64544208333334	6.5486691	26	642	5474	failed (too many debug iterations)	0	0			None		
482301678001	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	wordnet-cooking-exploration-copy4	Using WordNet's food-related semantic hierarchies to guide exploration in cooking tasks.	178.24231856666665	6.15718755	26	487	4002	failed (too many debug iterations)	0	0			None		
382906223642	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	wordnet-cooking-exploration-copy5	Using WordNet's food-related semantic hierarchies to guide exploration in cooking tasks.	99.51873038333333	6.61122915	26	581	4892	failed (too many debug iterations)	0	0			None		
692511972995	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-graph-alignment-copy1	Compare different similarity metrics for aligning text descriptions with graph representations of cooking game states.	9.548203366666666	0.7298150999999999	5	275	2355	completed	1	1	deviations	support	Jaccard similarity outperformed other metrics for text-graph alignment in cooking game states (p<0.001).	In a comparison of text-graph alignment metrics for TextWorldExpress cooking games (n=30), Jaccard similarity significantly outperformed random baseline (0.80 vs 0.52, p<0.001), while word overlap (0.09) and custom metrics (0.45) did not show improvement. The results suggest that object-based Jaccard similarity is effective for basic game state representation, though the implementation was limited to initial states only.	Different similarity metrics will vary in their effectiveness at aligning text descriptions with graph representations of game states in TextWorldExpress cooking games, with structured metrics outperforming simple baselines.
886454208931	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-graph-alignment-copy2	Compare different similarity metrics for aligning text descriptions with graph representations of cooking game states.	19.1679432	1.6829431500000003	9	376	3239	completed	1	1	faithful	reject	Simple lexical similarity metrics failed to effectively align game text with graph representations.	Three text-graph similarity metrics (random, word overlap, Jaccard) were compared on 30 text-graph pairs from TextWorldExpress cooking games. No metric showed significant correlation with game progress (all p>0.65), and similarity scores were consistently low (typically <0.1) for both word overlap and Jaccard metrics, suggesting these simple lexical matching approaches may be insufficient for meaningful text-graph alignment in this domain.	Different similarity metrics (word overlap and Jaccard) will perform better than random matching for aligning text observations with graph representations in TextWorldExpress games, and will show meaningful correlation with game progress.
641570661716	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-graph-alignment-copy3	Compare different similarity metrics for aligning text descriptions with graph representations of cooking game states.	70.01839801666667	5.737467	24	591	5066	completed	1	1	deviations	support	Custom graph-text similarity metric outperforms baselines in TextWorldExpress cooking game state alignment.	In a comparison of text-graph alignment metrics for TextWorldExpress cooking games, a custom similarity metric incorporating structural features significantly outperformed baseline approaches, achieving mean similarity scores of 0.52 compared to 0.20 for word overlap and 0.09 for Jaccard similarity across 30 test pairs. The custom metric showed consistent performance across different game states, suggesting robust alignment between textual descriptions and graph representations.	Different similarity metrics will vary in their effectiveness at aligning textual descriptions with graph representations of game states, with structurally-aware metrics performing better than simple text-based comparisons.
867045194182	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-graph-alignment-copy4	Compare different similarity metrics for aligning text descriptions with graph representations of cooking game states.	21.172490149999998	1.4878624500000002	8	411	3652	completed	1	1	deviations	support	Custom weighted similarity metric outperformed baseline methods for text-graph alignment in cooking games.	The experiment compared three text-graph similarity metrics (word overlap, Jaccard, and custom weighted) on 30 text-graph pairs from TextWorldExpress cooking games. The custom metric significantly outperformed both baseline and Jaccard (mean 0.3 vs 0.137, p<0.001), while Jaccard showed no improvement over the baseline.	Different similarity metrics will vary in their effectiveness for aligning text descriptions with graph representations in TextWorldExpress games, with more sophisticated metrics performing better than simple word overlap.
445778117853	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-graph-alignment-copy5	Compare different similarity metrics for aligning text descriptions with graph representations of cooking game states.	159.54783495	1.5406327499999999	8	421	3906	completed	1	1	deviations	reject	Graph-text similarity metrics performed worse than random baseline in TextWorldExpress cooking game alignment task.	The experiment compared three graph-text similarity metrics (word overlap, Jaccard, edge-based) against a random baseline in TextWorldExpress cooking games, running 30 episodes across 3 games. All experimental metrics performed significantly worse than random (means: 0.087, 0.086, 0.065 vs 0.521 baseline, p=1.0), with negative progress correlations (around -0.25), suggesting fundamental issues with either the similarity computations or graph representations.	Different similarity metrics (word overlap, Jaccard, edge-based) can effectively measure alignment between game state text descriptions and their graph representations in TextWorldExpress cooking games.
127181842956	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	llm-graph-verification-copy1	Evaluate if LLM-based verification improves knowledge graph accuracy in text-based games.	50.318175	1.3656674	7	331	3156	completed	1	1	deviations	inconclusive	LLM verification found knowledge graph errors and showed trending performance improvements in CookingWorld games.	The experiment compared knowledge graph generation with and without LLM verification in CookingWorld across 10 episodes. The experimental condition with verification found and corrected an average of 10.4 inconsistencies per episode and showed higher but non-significant performance (mean score 0.125 vs 0.077, p=0.155), suggesting potential benefits that warrant further investigation.	LLM-based verification improves the accuracy of knowledge graphs generated from text-based game states, leading to better game performance.
80563508185	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	llm-graph-verification-copy2	Evaluate if LLM-based verification improves knowledge graph accuracy in text-based games.	105.5349883	1.8715874000000001	9	456	3778	completed	1	0	errors	inconclusive	LLM verification for game knowledge graphs was implemented but experiment execution failed to generate results.	An experiment comparing baseline and LLM-verified knowledge graph construction in CookingWorld text games was implemented but failed to generate results. The code structure suggests a comprehensive experimental design, but empty results and log files indicate execution problems prevented meaningful analysis.	LLM-based verification improves the accuracy of knowledge graphs constructed from text-based game states.
751752587139	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	llm-graph-verification-copy3	Evaluate if LLM-based verification improves knowledge graph accuracy in text-based games.	61.15627435	1.6947268	9	325	2859	completed	1	1	deviations	support	LLM verification improved knowledge graph accuracy and game performance in text-based games (p=0.001).	The experiment compared knowledge graph generation with and without LLM verification in CookingWorld across 10 episodes. The experimental condition with LLM verification achieved significantly higher game scores (0.152 vs 0.067, p=0.001) and demonstrated high verification accuracy (79.5%), suggesting that LLM verification meaningfully improves knowledge graph quality and game performance.	LLM-based verification improves the accuracy and utility of knowledge graphs generated from text-based game states.
439602457324	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	llm-graph-verification-copy4	Evaluate if LLM-based verification improves knowledge graph accuracy in text-based games.	182.06804621666666	5.9998973	26	566	5481	failed (too many debug iterations)	0	0			None		
315763595765	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	llm-graph-verification-copy5	Evaluate if LLM-based verification improves knowledge graph accuracy in text-based games.	90.9440766	3.50226495	16	438	3875	completed	1	1	deviations	support	LLM verification improved game scores and knowledge graph quality, with results trending toward significance (p=0.059).	LLM-based verification was tested for improving knowledge graph accuracy in text games, comparing baseline vs GPT-4-mini verification across 10 episodes. The experimental condition showed better performance (0.129 vs 0.075 score, p=0.059) and produced more focused graphs (2.4 vs 4.3 nodes) with 82.4% verification accuracy, suggesting LLM verification may help create more accurate knowledge representations despite not reaching statistical significance.	LLM-based verification improves the accuracy and quality of knowledge graphs extracted from text-based game states.
447867258952	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	react-knowledge-retrieval-copy1	Evaluate if adding ConceptNet knowledge retrieval to a ReAct agent improves performance on simple text-based tasks.	321.7393966	7.342160650000001	26	643	5134	failed (too many debug iterations)	0	0			None		
616456688144	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	react-knowledge-retrieval-copy2	Evaluate if adding ConceptNet knowledge retrieval to a ReAct agent improves performance on simple text-based tasks.	414.46632746666666	3.4646887	14	461	4070	failed (hard experiment runtime limit reached)	0	0			None		
142079912287	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	react-knowledge-retrieval-copy3	Evaluate if adding ConceptNet knowledge retrieval to a ReAct agent improves performance on simple text-based tasks.	483.06577885	4.57976925	16	570	4818	failed (hard experiment runtime limit reached)	0	0			None		
535901763905	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	react-knowledge-retrieval-copy4	Evaluate if adding ConceptNet knowledge retrieval to a ReAct agent improves performance on simple text-based tasks.	307.5860174666667	6.5688402	25	549	5040	completed	1	1	deviations	reject	ReAct agents outperformed random baseline, but ConceptNet knowledge integration showed no additional benefit.	The experiment compared random, ReAct, and ConceptNet-augmented ReAct agents on TextWorldExpress tasks, with both ReAct variants achieving mean scores of 0.5625 versus 0.2125 for random (p<0.001). However, ConceptNet knowledge integration showed no additional benefit over standard ReAct (p=0.5321), though implementation issues with knowledge retrieval may have affected these results.	Adding ConceptNet knowledge retrieval to a ReAct agent improves its performance on TextWorldExpress common sense tasks.
893170370286	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	react-knowledge-retrieval-copy5	Evaluate if adding ConceptNet knowledge retrieval to a ReAct agent improves performance on simple text-based tasks.	106.48636101666666	6.2393446500000005	26	583	4880	failed (too many debug iterations)	0	0			None		
466829187549	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-property-verification-copy1	System to verify basic physical properties of objects against ConceptNet knowledge in ScienceWorld	65.77587053333333	5.65622445	26	526	4537	failed (too many debug iterations)	0	0			None		
853669666867	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-property-verification-copy2	System to verify basic physical properties of objects against ConceptNet knowledge in ScienceWorld	60.02644458333334	5.385204450000002	26	497	4125	failed (too many debug iterations)	0	0			None		
82160339943	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-property-verification-copy3	System to verify basic physical properties of objects against ConceptNet knowledge in ScienceWorld	71.1205957	5.472626100000001	26	431	3817	failed (too many debug iterations)	0	0			None		
812142650023	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-property-verification-copy4	System to verify basic physical properties of objects against ConceptNet knowledge in ScienceWorld	78.33326266666667	5.286599099999999	26	437	3810	failed (too many debug iterations)	0	0			None		
227648351788	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-property-verification-copy5	System to verify basic physical properties of objects against ConceptNet knowledge in ScienceWorld	71.33110253333332	5.57312595	26	505	3997	failed (too many debug iterations)	0	0			None		
831073090305	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-memory-pruning-copy1	Compare time-based versus frequency-based memory pruning strategies in a ScienceWorld agent.	382.5396666	5.79462015	22	600	5545	failed (hard experiment runtime limit reached)	0	0			None		
840145548015	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-memory-pruning-copy2	Compare time-based versus frequency-based memory pruning strategies in a ScienceWorld agent.	126.88458184999999	19.6849781	26	603	5588	failed (too many debug iterations)	0	0			None		
301215483676	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-memory-pruning-copy3	Compare time-based versus frequency-based memory pruning strategies in a ScienceWorld agent.	82.90174648333333	11.1278699	26	476	4271	failed (too many debug iterations)	0	0			None		
454604412120	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-memory-pruning-copy4	Compare time-based versus frequency-based memory pruning strategies in a ScienceWorld agent.	223.60310798333333	9.39410775	23	586	5395	completed	1	0	errors	inconclusive	Memory pruning comparison showed no pruning performed best, but results limited by implementation issues.	An experiment comparing four memory pruning strategies (time-based, frequency-based, no pruning, random) for a ReAct agent found no pruning performed best (score 1.5/10) compared to other strategies (0.7-0.9/10), though differences were not statistically significant (p=1.0). The experiment was limited by LLM failures and agent action selection issues.	Different memory pruning strategies (time-based, frequency-based) will affect ReAct agent performance differently than baseline strategies (no pruning, random).
749769960661	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-memory-pruning-copy5	Compare time-based versus frequency-based memory pruning strategies in a ScienceWorld agent.	138.28719571666667	24.221433049999995	26	363	3225	failed (too many debug iterations)	0	0			None		
60637110575	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-goal-explorer-copy1	Develop an agent that tracks predefined cooking-related goal hypotheses during game exploration.	367.9692276833333	3.52910425	13	545	5036	completed	1	1	deviations	support	Goal-tracking ReAct agent outperformed random baseline in cooking games (0.334 vs 0.128 mean score).	A goal-tracking ReAct agent was compared against a random baseline across 25 episodes of TextWorldExpress cooking games, achieving mean scores of 0.334 vs 0.128 respectively (p < 0.001 via bootstrap analysis). The agent successfully demonstrated goal tracking and confidence updating, though it occasionally exhibited repetitive behavior patterns.	A ReAct agent augmented with explicit goal tracking will perform better at cooking games than a random baseline agent.
837913403868	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-goal-explorer-copy2	Develop an agent that tracks predefined cooking-related goal hypotheses during game exploration.	184.9123827	7.012000650000001	26	691	5751	failed (too many debug iterations)	0	0			None		
98132395184	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-goal-explorer-copy3	Develop an agent that tracks predefined cooking-related goal hypotheses during game exploration.	63.95814181666666	0.9959452	4	297	2657	completed	1	0	faithful	inconclusive	Goal-tracking ReAct agent performed similarly to random baseline in cooking game, despite maintaining goal awareness.	A goal-tracking ReAct agent was compared against a random baseline across 25 episodes of TextWorldExpress cooking games. While the ReAct agent showed slightly higher performance (mean=0.147) compared to random (mean=0.134), the difference was not statistically significant (p=0.323). The goal tracking system successfully maintained goal confidences but did not lead to significantly improved task completion.	A ReAct agent enhanced with explicit goal tracking will perform better than a random baseline in TextWorldExpress cooking games.
722891225969	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-goal-explorer-copy4	Develop an agent that tracks predefined cooking-related goal hypotheses during game exploration.	188.65677473333332	1.4109526	6	375	3121	completed	1	1	deviations	support	Goal-tracking agent outperformed random baseline but showed limitations in action selection and absolute performance.	A goal-tracking ReAct agent was compared to a random baseline across 25 episodes of TextWorldExpress cooking games. The goal-tracking agent achieved significantly better performance (mean score 0.127 vs 0.055, p=0.033) but showed repetitive behaviors and poor absolute performance, suggesting that while goal tracking provided benefits, the action selection strategy needs improvement.	A ReAct agent with explicit goal tracking will perform better at cooking tasks than a random baseline agent.
184821369614	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-goal-explorer-copy5	Develop an agent that tracks predefined cooking-related goal hypotheses during game exploration.	45.39703096666666	0.8227338000000001	4	286	2450	completed	1	1	deviations	support	Goal-tracking ReAct agent outperformed random baseline on cooking games (0.198 vs 0.074 mean score).	A goal-tracking ReAct agent was compared against a random baseline on TextWorldExpress cooking games in a pilot study with 5 games and 5 episodes per game. The ReAct agent achieved a significantly higher mean score of 0.198 compared to 0.074 for random (p < 0.001), demonstrating that goal tracking can improve performance, though absolute scores remained relatively low.	Adding goal tracking capabilities to a ReAct agent will improve its performance on TextWorldExpress cooking games compared to random action selection.
235899831457	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	hierarchical-elimination-copy1	Create a hierarchical filtering system that eliminates irrelevant information at multiple levels of abstraction.	25.53033138333333	1.69500035	9	522	4386	completed	1	1	deviations	inconclusive	Hierarchical filtering was faster than flat filtering in ScienceWorld, but difference wasn't statistically significant.	A comparison of hierarchical vs flat filtering approaches in ScienceWorld showed hierarchical filtering was faster (5.43s vs 21.06s) but not significantly different (p=0.964, n=10 episodes). Both approaches effectively identified task-relevant objects, with the hierarchical system successfully using room-level filtering to reduce the search space.	Hierarchical filtering approaches are more efficient than flat filtering approaches for identifying task-relevant objects in virtual environments.
520802735162	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	hierarchical-elimination-copy2	Create a hierarchical filtering system that eliminates irrelevant information at multiple levels of abstraction.	133.38513686666667	5.4563064500000005	26	557	4764	failed (too many debug iterations)	0	0			None		
359589638349	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	hierarchical-elimination-copy3	Create a hierarchical filtering system that eliminates irrelevant information at multiple levels of abstraction.	66.46181315	1.0967947500000002	6	372	3260	completed	1	1	deviations	support	Hierarchical filtering outperformed baselines in both task score and efficiency, but needs more testing.	A hierarchical filtering system was compared against flat, random, and no filtering baselines in ScienceWorld across 10 episodes. The hierarchical approach achieved the highest average score (6.8 vs 5.16/2.44/2.92) and better computational efficiency (6.78s vs 8.20s for flat filtering), suggesting benefits of hierarchical filtering, though with limited statistical confidence due to small sample size.	Hierarchical filtering using room-level then object-level evaluation will be more effective and efficient than flat or random filtering approaches.
198884073892	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	hierarchical-elimination-copy4	Create a hierarchical filtering system that eliminates irrelevant information at multiple levels of abstraction.	166.11028683333333	3.40680535	16	461	3887	completed	1	0	deviations	reject	Hierarchical filtering showed similar performance to baselines but added computational overhead in ScienceWorld tasks.	A hierarchical filtering system using GPT-4-mini was compared to flat, random and no filtering across 10 episodes in ScienceWorld tasks. The hierarchical system showed similar task scores to baselines (ranging from 0-3) but with added computational overhead of 15-30 seconds per episode, providing no clear evidence that hierarchical filtering improved performance.	A hierarchical filtering system that first evaluates rooms then objects within promising rooms will improve task performance compared to flat or no filtering approaches.
756482072322	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	hierarchical-elimination-copy5	Create a hierarchical filtering system that eliminates irrelevant information at multiple levels of abstraction.	21.432352816666665	1.2123918	7	457	3806	completed	1	0	deviations	inconclusive	Hierarchical and flat filtering showed similar performance in ScienceWorld, with no clear advantage for either approach.	A comparison of hierarchical vs flat filtering in ScienceWorld across 10 episodes showed no significant performance advantage for hierarchical filtering, with both approaches achieving similar mean scores (~0.8/3.0) and object reduction rates (7.8 vs 7.4 objects). While the hierarchical approach showed more consistent computation times (mean 14.3s) compared to flat filtering (mean 27.8s), the high variance in scores and small sample size prevent definitive conclusions.	Hierarchical filtering (evaluating rooms, then objects) will be more efficient and effective than flat filtering or baseline approaches in ScienceWorld tasks.
9401332898	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	two-level-cooking-planner-copy1	Create a two-level planner combining recipe planning with action execution for cooking tasks in TextWorldExpress.	78.6496519	4.1693988	13	669	5974	failed (code parsing issue)	0	0			None		
573257119541	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	two-level-cooking-planner-copy2	Create a two-level planner combining recipe planning with action execution for cooking tasks in TextWorldExpress.	116.7861882	7.443998650000001	26	666	5923	failed (too many debug iterations)	0	0			None		
672790379184	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	two-level-cooking-planner-copy3	Create a two-level planner combining recipe planning with action execution for cooking tasks in TextWorldExpress.	52.21985001666667	3.56259225	12	655	6068	failed (code parsing issue)	0	0			None		
167253196046	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	two-level-cooking-planner-copy4	Create a two-level planner combining recipe planning with action execution for cooking tasks in TextWorldExpress.	28.868514983333334	1.8427482	6	656	6093	failed (code parsing issue)	0	0			None		
750258422615	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	two-level-cooking-planner-copy5	Create a two-level planner combining recipe planning with action execution for cooking tasks in TextWorldExpress.	30.711787533333332	2.1648073500000002	8	678	6198	failed (code parsing issue)	0	0			None		
307951423425	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-planning-agent-copy1	Create and evaluate a basic planning agent that can break down simple cooking tasks into 2-3 step sequences and execute them.	52.99440025	4.128828	16	675	6220	failed (code parsing issue)	0	0			None		
291788186424	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-planning-agent-copy2	Create and evaluate a basic planning agent that can break down simple cooking tasks into 2-3 step sequences and execute them.	26.21074186666667	1.4687236000000001	7	422	3855	completed	1	1	faithful	inconclusive	Planning agent matched ReAct performance but was more efficient; both outperformed random baseline.	A comparison of Planning, ReAct, and Random agents in CookingWorld showed Planning (0.188±0.060) and ReAct (0.197±0.069) significantly outperformed Random (0.096±0.092), but were not significantly different from each other (p=0.715). Planning agent was more step-efficient (9.05 vs 13.40 steps), though no agent achieved full task success.	A planning-based agent that first generates a high-level plan before execution will perform better than standard ReAct and random baselines in CookingWorld tasks.
20450292902	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-planning-agent-copy3	Create and evaluate a basic planning agent that can break down simple cooking tasks into 2-3 step sequences and execute them.	37.60721888333333	0.84188485	4	348	2867	completed	1	1	faithful	support	Planning agent outperformed ReAct and Random baselines in cooking game, with 33% higher scores and fewer steps.	A Planning agent was compared to ReAct and Random baselines in a cooking game environment across 20 episodes. The Planning agent significantly outperformed both baselines (mean score 0.363 vs 0.273 and 0.111 respectively, p<0.005), while also requiring fewer steps to complete tasks (15.85 vs 22.45 and 25.65 steps), supporting the hypothesis that adding a planning phase improves performance.	Adding an explicit planning phase to a ReAct agent will improve its performance in sequential decision-making tasks by providing better strategic guidance.
792212591952	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-planning-agent-copy4	Create and evaluate a basic planning agent that can break down simple cooking tasks into 2-3 step sequences and execute them.	77.2495047	6.24128595	26	550	4853	failed (too many debug iterations)	0	0			None		
478265929555	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-planning-agent-copy5	Create and evaluate a basic planning agent that can break down simple cooking tasks into 2-3 step sequences and execute them.	27.14887643333333	0.9096442000000001	4	425	3710	completed	1	1	faithful	reject	Adding planning to ReAct agents showed no performance improvement in CookingWorld tasks.	A comparison of Planning-augmented ReAct vs standard ReAct agents in CookingWorld showed no benefit from explicit planning (Planning: 0.167 ± 0.099, ReAct: 0.210 ± 0.075, p=0.930), though both outperformed random behavior (0.056 ± 0.074). The ReAct baseline completed tasks in fewer steps (14.35 vs 23.60), suggesting planning may add unnecessary complexity.	Adding an explicit planning phase to a ReAct agent will improve its performance in structured environments like CookingWorld.
232279592942	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simulation-confidence-analysis-copy1	Investigate LLM ability to assess confidence in state predictions and correlation with accuracy.	120.49846845	1.8718302000000002	10	333	2978	completed	1	1	deviations	support	LLM confidence scores showed moderate correlation (0.587) with prediction accuracy in TextWorldExpress environment interactions.	The experiment tested LLM confidence calibration in TextWorldExpress, analyzing 20 episodes of environment interactions. Results showed a moderate positive correlation (mean ≈ 0.587) between confidence scores and prediction accuracy, with individual episode correlations ranging from 0.49 to 0.75, suggesting LLMs have some ability to calibrate their confidence but with significant room for improvement.	LLMs can meaningfully calibrate their confidence scores to reflect their actual prediction accuracy in sequential decision-making environments.
568592485353	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simulation-confidence-analysis-copy2	Investigate LLM ability to assess confidence in state predictions and correlation with accuracy.	122.66615	0.60254555	3	296	2257	completed	1	1	deviations	inconclusive	LLM confidence scores showed moderate but inconsistent correlation with prediction accuracy in TextWorldExpress environments.	An experiment testing LLM confidence-accuracy correlations in TextWorldExpress found a moderate positive mean correlation of 0.311 (SD=0.396) across 20 episodes, but with high variance ranging from -0.543 to 0.870. While some episodes showed strong positive correlations (>0.85), the inconsistency and presence of negative correlations suggests LLM confidence scores may not be reliably indicative of prediction accuracy.	LLM confidence scores meaningfully correlate with prediction accuracy in TextWorldExpress environments, suggesting they could be useful indicators of prediction reliability.
100526801365	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simulation-confidence-analysis-copy3	Investigate LLM ability to assess confidence in state predictions and correlation with accuracy.	98.85754655000001	0.8386413000000001	4	407	3324	completed	1	1	deviations	support	LLM confidence scores showed weak but significant correlation (r=0.110) with prediction accuracy in TextWorldExpress.	The experiment tested whether LLM confidence scores correlate with prediction accuracy in TextWorldExpress, analyzing 20 episodes in PILOT mode. Results showed a statistically significant but weak positive correlation (mean r=0.110, p<0.001), indicating that while LLMs show some calibration between confidence and accuracy, the relationship is not strong enough to be practically reliable.	LLM confidence scores meaningfully correlate with prediction accuracy in text-based game environments.
831058631624	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simulation-confidence-analysis-copy4	Investigate LLM ability to assess confidence in state predictions and correlation with accuracy.	84.80192835	0.8180817	4	319	2613	completed	1	1	deviations	support	LLM confidence scores showed variable but generally positive correlation with prediction accuracy in TextWorldExpress.	The experiment tested correlation between LLM confidence scores and prediction accuracy in TextWorldExpress, collecting data across 20 episodes with 25 steps each. Episode-level correlations varied widely (-0.958 to 0.933), with most episodes showing positive but moderate correlations, suggesting a noisy but generally positive relationship between LLM confidence and accuracy.	LLM confidence scores meaningfully correlate with prediction accuracy in TextWorldExpress environments.
622091164648	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simulation-confidence-analysis-copy5	Investigate LLM ability to assess confidence in state predictions and correlation with accuracy.	138.13108626666664	1.4028839	6	430	3526	completed	1	1	deviations	support	LLM confidence moderately correlates with prediction accuracy (r=0.33) but relationship is highly variable across episodes.	The experiment tested LLM confidence-accuracy correlations in TextWorldExpress, finding a moderate positive correlation (mean r=0.33, 95% CI [0.06, 0.55]) and ROC AUC of 0.67 across 20 episodes. While these results suggest LLMs have some ability to assess their prediction confidence, the high variability in episode-level correlations (-0.65 to 0.69) indicates this metacognitive ability is inconsistent.	LLM confidence scores meaningfully correlate with prediction accuracy in TextWorldExpress environments, indicating metacognitive awareness.
618535868101	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	location-graph-cooking-copy1	Using location-tracking graphs to improve efficiency in TextWorldExpress cooking games.	76.20600241666666	1.4649299	5	437	3705	completed	1	1	faithful	inconclusive	Location-tracking graph showed promising but inconclusive improvements to ReAct agent's cooking game performance.	A pilot study (n=25 episodes) compared ReAct agents with and without location tracking in cooking games. The experimental agent with location tracking achieved higher average scores (0.38 vs 0.28) and success rates (12% vs 8%), but high variance and small sample size prevent definitive conclusions.	Adding a location-tracking graph to a ReAct agent will improve its performance in TextWorldExpress cooking games by helping it better understand and navigate the environment.
278723845411	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	location-graph-cooking-copy2	Using location-tracking graphs to improve efficiency in TextWorldExpress cooking games.	84.81662338333334	6.01510605	22	724	6016	failed (code parsing issue)	0	0			None		
90439647588	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	location-graph-cooking-copy3	Using location-tracking graphs to improve efficiency in TextWorldExpress cooking games.	80.84845285	2.9892995	12	463	4018	completed	1	1	faithful	support	Location-tracking graph significantly improved ReAct agent scores in cooking game tasks (p=0.015).	A pilot experiment with 25 episodes tested whether adding a location-tracking graph to a ReAct agent improved performance in TextWorldExpress cooking games. The experimental agent achieved significantly higher scores (mean=0.328 vs 0.158, p=0.015) compared to baseline, though steps taken were similar (18.83 vs 19.42, p=0.524). Results suggest the location graph meaningfully improved agent performance.	Adding a location-tracking graph to a ReAct agent will improve its performance in TextWorldExpress cooking games by helping it better navigate and track object locations.
317164048946	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	location-graph-cooking-copy4	Using location-tracking graphs to improve efficiency in TextWorldExpress cooking games.	80.43558978333334	6.093841000000001	26	616	5443	failed (too many debug iterations)	0	0			None		
80473279791	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	location-graph-cooking-copy5	Using location-tracking graphs to improve efficiency in TextWorldExpress cooking games.	201.0443477	7.3835549500000015	26	629	5788	failed (too many debug iterations)	0	0			None		
755955293195	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	two-stage-game-generation-copy1	Comparing single-stage versus two-stage text game generation approaches	293.42163493333334	2.6459048	15	494	4290	completed	1	1	faithful	reject	Single-stage and two-stage game generation both achieved 100% success, but single-stage was twice as fast.	A comparison of single-stage vs two-stage text game generation showed both methods achieved 100% success rates across 15 generations each, but two-stage generation took significantly longer (19.25s vs 9.92s average). Bootstrap analysis (10,000 resamples) showed no significant difference in success rates (p=1.0), suggesting single-stage generation is more efficient while maintaining equal quality.	Two-stage game generation (separating basic mechanics from scoring/win conditions) produces more reliable and complete text game implementations compared to single-stage generation.
782415327844	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	two-stage-game-generation-copy2	Comparing single-stage versus two-stage text game generation approaches	241.29370413333334	2.5430519999999994	16	325	3285	completed	1	0	deviations	inconclusive	Comparison of single vs. two-stage game generation showed both approaches viable, but results aggregation failed.	The experiment compared single-stage vs. two-stage text game generation using gpt-4o-mini, running in PILOT mode with 5 games and 3 generations per method. While the generation and evaluation framework worked successfully, with logs showing both methods capable of producing functional games, the results aggregation failed (null results.json), preventing definitive statistical comparison between the methods.	Two-stage game generation (separating basic mechanics from scoring/win conditions) produces more reliable and complete game implementations compared to single-stage generation.
320926979841	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	two-stage-game-generation-copy3	Comparing single-stage versus two-stage text game generation approaches	36.63504176666667	1.8195961500000002	11	372	3539	completed	1	1	faithful	support	Two-stage game generation was more complete but slower than single-stage, with both achieving 100% execution success.	Comparing single-stage vs two-stage text game generation, both methods achieved 100% execution success, but two-stage showed higher mechanics completion (93.3% vs 86.7%). Two-stage generation took approximately twice as long (32.1s vs 15.8s), suggesting a trade-off between completeness and generation speed.	Two-stage text game generation produces more complete and reliable game implementations compared to single-stage generation.
230389161857	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	two-stage-game-generation-copy4	Comparing single-stage versus two-stage text game generation approaches	18.22651408333333	0.42300715000000005	3	324	2657	completed	1	1	deviations	support	Two-stage game generation produces more complete games than single-stage, but takes 2.5x longer.	Comparing single-stage vs two-stage text game generation, the two-stage approach achieved 100% success in both execution and mechanics completion, while single-stage achieved 93.3% execution success but only 66.7% mechanics completion. The two-stage approach required approximately 2.5 times longer to generate (34.27s vs 13.76s), suggesting a clear trade-off between reliability and generation speed.	Breaking down complex game generation into multiple stages (two-stage approach) will result in more reliable and complete game implementations compared to generating the entire game in a single stage.
216723723933	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	two-stage-game-generation-copy5	Comparing single-stage versus two-stage text game generation approaches	24.29567248333333	1.3303066	7	397	3648	completed	1	1	deviations	support	Two-stage game generation showed better consistency but slower performance than single-stage approach.	In a comparison of single-stage vs two-stage text game generation (n=30), both methods achieved 100% execution success but failed to properly implement the drop() method. The two-stage approach showed better consistency in implementing the take() method (100% vs 53.3%) but required approximately twice the generation time (~14s vs ~7s).	Two-stage text game generation produces more reliable and complete game implementations compared to single-stage generation.
208240251429	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-template-discovery-copy1	System for identifying common action patterns in successful TextWorldExpress CookingWorld trajectories.	62.90637366666667	2.266803	11	21	140	completed	1	0	deviations	inconclusive	Discovered action templates performed similarly to baseline and manual templates in cooking game tasks.	The experiment compared automatically discovered action templates against baseline and manual templates in TextWorldExpress CookingWorld, using 50 training trajectories and 25 test episodes. Results showed similar performance across conditions (discovered template: 0.072 mean score, 28% success; baseline: 0.069 mean score, 20% success; manual template: 0.056 mean score, 28% success) with no statistically significant differences (p>0.47).	Automatically discovered action templates from successful trajectories can improve agent performance in TextWorldExpress CookingWorld compared to baseline random and manual template approaches.
786377357129	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-template-discovery-copy2	System for identifying common action patterns in successful TextWorldExpress CookingWorld trajectories.	43.45785746666666	3.9676874999999994	15	674	6112	failed (code parsing issue)	0	0			None		
998862805383	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-template-discovery-copy3	System for identifying common action patterns in successful TextWorldExpress CookingWorld trajectories.	99.1303499	5.100021450000001	26	498	4425	failed (too many debug iterations)	0	0			None		
692580791223	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-template-discovery-copy4	System for identifying common action patterns in successful TextWorldExpress CookingWorld trajectories.	26.4944615	2.5944627000000002	12	503	4322	completed	1	1	faithful	support	Discovered action templates significantly improved agent performance in CookingWorld, outperforming both baseline and manual templates.	The experiment compared random baseline, manual template, and discovered template agents in TextWorldExpress CookingWorld, with 25 test episodes. The discovered template system significantly outperformed the baseline (29.6% vs 6.4% average score, p<0.001) and achieved an 88% success rate, while also matching the manual template system's efficiency (16 steps to reward vs baseline's 28 steps).	Action patterns (templates) can be automatically discovered from successful trajectories and used to improve agent performance in text-based games.
461530005923	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-template-discovery-copy5	System for identifying common action patterns in successful TextWorldExpress CookingWorld trajectories.	32.17952878333333	3.1949547000000003	14	692	6092	failed (code parsing issue)	0	0			None		
660168784635	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	reactive-graph-confidence-copy1	Study if explicit reasoning about confidence improves belief graph accuracy in CookingWorld.	29.514343333333333	1.43026625	7	378	3270	completed	1	1	deviations	support	Confidence-based agent built more accurate environment graphs than baseline, but showed poor confidence calibration.	Comparing baseline and confidence-based graph-building agents in CookingWorld, the confidence-based agent achieved higher edge verification rates (77% vs 65%) but showed poor calibration (correlation -0.44 between confidence and verification). The confidence-based agent built larger, more detailed graphs (31 vs 24 edges on average) while maintaining higher verification rates.	A graph-building agent that reasons about its confidence in relationships will build more accurate and reliable knowledge graphs than one that doesn't.
651245282804	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	reactive-graph-confidence-copy2	Study if explicit reasoning about confidence improves belief graph accuracy in CookingWorld.	56.823658183333336	1.5772863000000001	7	49	404	completed	1	0	deviations	reject	Confidence-based graph building showed no significant improvement over baseline in CookingWorld environment.	A comparison of baseline and confidence-based graph-building agents across 10 episodes showed no significant difference in verified edge counts (30.7 vs 29.5, p=0.2965), with negligible correlation between confidence scores and edge verification (r=0.033). The results suggest that confidence scoring does not meaningfully improve graph building accuracy in this context.	Adding confidence scoring to a graph-building agent will improve the accuracy and reliability of the constructed environmental knowledge graph.
734953986323	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	reactive-graph-confidence-copy3	Study if explicit reasoning about confidence improves belief graph accuracy in CookingWorld.	132.74240043333333	6.212834500000001	26	446	4172	failed (too many debug iterations)	0	0			None		
34126299968	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	reactive-graph-confidence-copy4	Study if explicit reasoning about confidence improves belief graph accuracy in CookingWorld.	0.0	0.0	0			interrupted	0	0			None		
42714548288	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	reactive-graph-confidence-copy5	Study if explicit reasoning about confidence improves belief graph accuracy in CookingWorld.	33.35601015	2.1198714499999998	10	354	3034	completed	1	1	deviations	inconclusive	Confidence-based graph building showed trending improvement in edge verification rates over baseline approach.	A comparison of baseline and confidence-based graph-building agents in CookingWorld showed the confidence agent achieved higher verified edge proportions (0.233 vs 0.166), trending towards significance (p=0.069). While this suggests confidence scoring may improve graph accuracy, the small sample size (10 episodes) and incomplete implementation of some analyses limit strong conclusions.	Using LLM-generated confidence scores to filter graph updates will lead to more accurate environment graphs compared to direct observation-based updates.
734557640845	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	basic-confidence-simulation-copy1	Evaluate LLM ability to predict action success and assign meaningful confidence scores in a cooking game environment.	10.235119333333333	0.6700133	4	351	3007	completed	1	1	faithful	inconclusive	LLM predicts game action outcomes with 68% accuracy, but simple heuristic performs better at 86%.	In a pilot study of 50 actions in TextWorldExpress's CookingWorld, GPT-4o-mini achieved 68% accuracy in predicting action outcomes, with a weak but significant confidence-accuracy correlation (r=0.34, p=0.016). While outperforming random guessing (44%), the LLM underperformed compared to always predicting success (86%), suggesting limited practical utility.	Language models can effectively predict the success/failure of actions in text-based environments and provide meaningful confidence scores for their predictions.
444903457359	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	basic-confidence-simulation-copy2	Evaluate LLM ability to predict action success and assign meaningful confidence scores in a cooking game environment.	17.1818808	1.2260328500000002	6	341	3336	completed	1	1	faithful	support	LLM predicted game action outcomes with 74% accuracy but showed poor confidence calibration and suboptimal performance.	An LLM-based system for predicting action outcomes in a text game environment achieved 74% accuracy with moderate confidence-accuracy correlation (r=0.27), but was outperformed by a constant success predictor (84% accuracy). The LLM's confidence scores showed minimal discrimination between correct (0.87) and incorrect (0.85) predictions, suggesting poor calibration despite above-chance performance.	LLMs can effectively predict the success/failure of actions in text-based games while providing meaningful confidence scores that correlate with prediction accuracy.
98980868691	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	basic-confidence-simulation-copy3	Evaluate LLM ability to predict action success and assign meaningful confidence scores in a cooking game environment.	7.6510934	0.4849417	3	299	2408	completed	1	1	faithful	support	LLM achieves 78% action prediction accuracy but shows overconfidence and lacks calibration in confidence scores.	An LLM-based action prediction system achieved 78% accuracy in predicting action outcomes in TextWorldExpress's CookingWorld, significantly outperforming random guessing (52%, p<0.001) but falling short of a simple always-success baseline (82%). The LLM showed high but undiscriminating confidence (mean ~0.88) for both correct and incorrect predictions, suggesting overconfidence.	LLMs can effectively predict action outcomes in text-based games and provide meaningful confidence scores that correlate with prediction accuracy.
510187767935	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	basic-confidence-simulation-copy4	Evaluate LLM ability to predict action success and assign meaningful confidence scores in a cooking game environment.	11.634842366666668	0.7857698499999999	4	324	3004	completed	1	1	faithful	support	GPT-4o-mini predicted game action outcomes with 76% accuracy, showing good confidence calibration.	In a pilot study of 50 action predictions in TextWorldExpress's CookingWorld, GPT-4o-mini achieved 76% accuracy in predicting action outcomes, significantly above the 50% baseline (p=0.0), with a moderate positive correlation between confidence and accuracy (r=0.523, p<0.001). The results demonstrate that the LLM can effectively predict action outcomes and provide meaningful confidence scores in text-based game environments.	Large language models can effectively predict the success/failure of actions in text-based environments while providing meaningful confidence scores that correlate with prediction accuracy.
386054315386	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	basic-confidence-simulation-copy5	Evaluate LLM ability to predict action success and assign meaningful confidence scores in a cooking game environment.	14.24533145	1.0664782499999998	6	316	2835	completed	1	1	faithful	support	LLM achieved 69% accuracy predicting game actions but showed overconfidence in both correct and incorrect predictions.	In a text-based cooking game environment, GPT-4o-mini achieved 68.75% accuracy in predicting action outcomes, significantly outperforming the 56.25% random baseline. The system showed moderate confidence-accuracy correlation (r=0.394), but exhibited overconfidence with similar confidence scores for both correct (0.89) and incorrect (0.86) predictions.	Language models can effectively predict action outcomes and assign meaningful confidence scores in interactive text environments.
184564987747	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	cooking-graph-explorer-copy1	Develop an agent that builds and uses container-relationship knowledge graphs in CookingWorld environments.	313.38199019999996	4.58655075	13	59	554	completed	1	1	faithful	reject	Container-augmented ReAct agent performed worse than standard ReAct in CookingWorld tasks.	A container-knowledge-augmented ReAct agent was compared against standard ReAct and random agents across 25 episodes in CookingWorld tasks. The container agent performed significantly worse (mean=0.206) than the standard ReAct agent (mean=0.321, p=0.9975), suggesting that the additional container knowledge did not improve, and may have hindered, task performance.	Augmenting a ReAct agent with container-relationship knowledge will improve its performance on CookingWorld tasks.
893453181732	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	cooking-graph-explorer-copy2	Develop an agent that builds and uses container-relationship knowledge graphs in CookingWorld environments.	51.621931366666665	1.0414108	5	332	3098	completed	1	0	deviations	inconclusive	Container-knowledge agent performed slightly but not significantly better than random baseline on cooking tasks.	A container-knowledge-augmented ReAct agent was compared to a random baseline on CookingWorld tasks over 25 episodes. The container agent achieved a mean score of 0.222 vs 0.202 for random, but this difference was not statistically significant (p=0.340), suggesting that container knowledge provides at most a modest benefit in this context.	Augmenting a ReAct agent with explicit container-relationship knowledge will improve its performance on CookingWorld tasks compared to baseline agents.
273802986520	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	cooking-graph-explorer-copy3	Develop an agent that builds and uses container-relationship knowledge graphs in CookingWorld environments.	34.64077835	1.253279	6	346	2936	completed	1	1	deviations	support	Container-knowledge-augmented ReAct agent outperformed random baseline in CookingWorld tasks (p=0.0138).	A container-knowledge-augmented ReAct agent was compared to a random baseline over 25 episodes in CookingWorld tasks. The container agent significantly outperformed the baseline (mean scores 0.275 vs 0.189, p=0.0138), while successfully building a knowledge graph of container relationships, suggesting that container knowledge improves task performance.	A ReAct agent augmented with container-relationship knowledge will perform better than baseline agents at CookingWorld tasks.
929133254919	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	cooking-graph-explorer-copy4	Develop an agent that builds and uses container-relationship knowledge graphs in CookingWorld environments.	48.47477193333334	1.1172904	5	336	3036	completed	1	1	deviations	support	Container-knowledge ReAct agent outperforms random baseline in CookingWorld tasks (0.343 vs 0.175 score).	A container-knowledge-augmented ReAct agent was compared to a random baseline agent on CookingWorld tasks across 25 episodes. The container agent significantly outperformed the random baseline (mean scores 0.343 vs 0.175, p=0.0), demonstrating that container relationship knowledge improves task performance.	A ReAct agent augmented with container-relationship knowledge will perform better than baseline agents at CookingWorld tasks.
939407811683	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	cooking-graph-explorer-copy5	Develop an agent that builds and uses container-relationship knowledge graphs in CookingWorld environments.	152.98293568333332	6.003510900000001	26	464	4168	failed (too many debug iterations)	0	0			None		
266825649377	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-task-reflection-copy1	Study if providing agents with their past successful experiences improves reflection quality in cooking tasks	438.9015041666667	4.6015386	16	549	4868	failed (hard experiment runtime limit reached)	0	0			None		
433345045625	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-task-reflection-copy2	Study if providing agents with their past successful experiences improves reflection quality in cooking tasks	127.00292420000001	4.2211559	18	572	5247	completed	1	0	errors	inconclusive	Experience-guided reflection experiment for ReAct agents failed to execute; no results available for analysis.	An experiment testing experience-guided reflection in ReAct agents for cooking tasks was implemented but failed to execute successfully, as evidenced by null results and empty log files. While the implementation was comprehensive, including experience collection and similarity-based retrieval, the execution failure prevents any empirical conclusions about the effectiveness of the approach.	Providing a ReAct agent with access to its past successful experiences will improve its reflection process and lead to better performance on TextWorldExpress cooking tasks.
630832356012	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-task-reflection-copy3	Study if providing agents with their past successful experiences improves reflection quality in cooking tasks	385.65259625	5.951901849999999	17	604	5658	failed (hard experiment runtime limit reached)	0	0			None		
28951364640	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-task-reflection-copy4	Study if providing agents with their past successful experiences improves reflection quality in cooking tasks	441.2751366333333	6.2332546	20	443	3864	failed (hard experiment runtime limit reached)	0	0			None		
430423353615	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-task-reflection-copy5	Study if providing agents with their past successful experiences improves reflection quality in cooking tasks	235.44679585	4.21609785	16	674	6181	failed (code parsing issue)	0	0			None		
330822172710	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-graph-cooking-simulation-copy1	Test if a static cooking knowledge graph improves LLM action prediction in CookingWorld tasks.	190.3580679	5.934444600000001	22	682	6121	failed (code parsing issue)	0	0			None		
158178470842	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-graph-cooking-simulation-copy2	Test if a static cooking knowledge graph improves LLM action prediction in CookingWorld tasks.	62.93437818333334	4.7742568499999996	17	654	6076	failed (code parsing issue)	0	0			None		
475354793586	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-graph-cooking-simulation-copy3	Test if a static cooking knowledge graph improves LLM action prediction in CookingWorld tasks.	88.96806791666667	6.454587999999999	26	679	5846	failed (too many debug iterations)	0	0			None		
13789695246	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-graph-cooking-simulation-copy4	Test if a static cooking knowledge graph improves LLM action prediction in CookingWorld tasks.	109.17579123333333	7.943972900000001	26	710	6284	failed (too many debug iterations)	0	0			None		
204057691405	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-graph-cooking-simulation-copy5	Test if a static cooking knowledge graph improves LLM action prediction in CookingWorld tasks.	102.94188731666667	7.672721750000003	26	679	5977	failed (too many debug iterations)	0	0			None		
893029982817	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-graph-state-tracking-copy1	Create an agent that uses simple graph-based state tracking to improve planning in CookingWorld environments.	134.70234983333333	7.1307772	26	534	4823	failed (too many debug iterations)	0	0			None		
762449417307	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-graph-state-tracking-copy2	Create an agent that uses simple graph-based state tracking to improve planning in CookingWorld environments.	45.831264450000006	1.6451254000000002	8	388	3523	completed	1	0	deviations	inconclusive	Graph-based state tracking showed minimal improvement over baseline ReAct agent in CookingWorld tasks.	A comparative experiment testing graph-based state tracking vs standard ReAct agents in CookingWorld showed minimal performance differences, with experimental condition scoring slightly higher (0.302 vs 0.272) but not significantly (p=0.311). Both conditions had low success rates (10%), with graph tracking potentially adding overhead (28.5 vs 22.3 average steps).	Graph-based state tracking of objects, locations, and their relationships will improve ReAct agent performance in CookingWorld by providing better environmental awareness and decision-making context.
91845092339	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-graph-state-tracking-copy3	Create an agent that uses simple graph-based state tracking to improve planning in CookingWorld environments.	83.16388688333333	5.8432125	19	718	6338	failed (code parsing issue)	0	0			None		
749198516943	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-graph-state-tracking-copy4	Create an agent that uses simple graph-based state tracking to improve planning in CookingWorld environments.	118.45223231666667	6.68402415	22	671	6031	failed (code parsing issue)	0	0			None		
548910599369	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-graph-state-tracking-copy5	Create an agent that uses simple graph-based state tracking to improve planning in CookingWorld environments.	57.85810955	4.149547050000001	15	687	6149	failed (code parsing issue)	0	0			None		
758908263525	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	failure-pattern-learning-copy1	Agent that learns to recognize and avoid common failure patterns in text-based games.	77.39905676666667	1.4585348500000002	7	433	3857	completed	1	0	deviations	reject	ReAct agent with failure memory performed similarly to baseline on TWC games, showing no significant improvement.	A comparison of ReAct agents with and without failure memory on TWC games showed no significant difference in performance (experimental mean=0.225 vs baseline mean=0.25, p=0.8169). Both agents struggled with repeated failures in object placement tasks, achieving 0% success rates across 30 episodes, suggesting the failure memory mechanism did not effectively improve learning.	A ReAct agent that learns from its failures will perform better than a baseline ReAct agent in TextWorld Commonsense games.
394647068336	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	failure-pattern-learning-copy2	Agent that learns to recognize and avoid common failure patterns in text-based games.	58.5417913	4.62736925	26	403	3392	failed (too many debug iterations)	0	0			None		
329740626865	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	failure-pattern-learning-copy3	Agent that learns to recognize and avoid common failure patterns in text-based games.	207.44665433333333	8.434405850000001	26	662	5749	failed (too many debug iterations)	0	0			None		
816496193496	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	failure-pattern-learning-copy4	Agent that learns to recognize and avoid common failure patterns in text-based games.	157.89045821666664	6.22399135	26	621	5948	failed (too many debug iterations)	0	0			None		
853446788393	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	failure-pattern-learning-copy5	Agent that learns to recognize and avoid common failure patterns in text-based games.	130.9186216	5.65827695	26	442	3973	failed (too many debug iterations)	0	0			None		
715131414163	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	action-outcome-tracking-copy1	Create agents that track and learn from their action successes and failures in text games.	34.24910396666667	1.0268569	5	357	3033	completed	1	1	deviations	inconclusive	ReAct agents outperformed random in CookingWorld, but action tracking didn't significantly improve performance.	In a 10-episode pilot comparing Random, ReAct, and ReAct+ActionTracking agents in CookingWorld, both ReAct variants significantly outperformed random (scores: 0.160 vs 0.460/0.420), but showed no significant difference between themselves (p=0.701). The ReAct+ActionTracking showed zero action repetition compared to baseline's 0.003, suggesting more efficient exploration, though neither achieved task completion.	Action tracking in ReAct agents improves performance by learning from past experiences and avoiding repeated mistakes.
735222549881	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	action-outcome-tracking-copy2	Create agents that track and learn from their action successes and failures in text games.	76.8847137	1.1409291499999998	6	371	3185	completed	1	1	faithful	inconclusive	ReAct+ActionTracking outperformed baselines in cooking task but differences weren't statistically significant in pilot study.	A comparison of ReAct+ActionTracking against ReAct baseline and Random agents in TextWorldExpress CookingWorld showed the tracking agent achieved higher mean scores (0.485 vs 0.469 vs 0.164) and was significantly better than random (p<0.001) but not significantly better than the baseline ReAct (p=0.365). The results suggest action tracking may improve performance but a larger sample size is needed for conclusive evidence.	Adding action history tracking and context-based action selection to a ReAct agent will improve its performance on text-based cooking tasks by helping it make better decisions based on past successes and failures.
381380443479	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	action-outcome-tracking-copy3	Create agents that track and learn from their action successes and failures in text games.	179.26304514999998	3.20309985	13	533	4928	completed	1	1	faithful	inconclusive	ReAct+ActionTracking outperformed Random but only marginally improved over ReAct baseline in cooking tasks.	In a 10-episode pilot comparing ReAct agents in TextWorldExpress CookingWorld, the ReAct+ActionTracking agent (score=0.425) significantly outperformed Random (score=0.171, p<0.001) but showed only modest, non-significant improvements over the ReAct baseline (score=0.406, p=0.239). While action tracking helped avoid repetitive behaviors, no agent achieved successful task completion.	Action tracking will improve ReAct agent performance by helping it learn from past experiences and avoid repeating unsuccessful actions.
482473398010	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	action-outcome-tracking-copy4	Create agents that track and learn from their action successes and failures in text games.	94.12968989999999	1.5385035000000002	7	448	4052	completed	1	1	faithful	reject	Action tracking in ReAct agents reduced repetition but did not improve task performance in TextWorldExpress CookingWorld.	A comparison of ReAct+ActionTracking against ReAct baseline and Random agents in TextWorldExpress CookingWorld showed ReAct+Tracking achieved an average score of 0.369 vs 0.455 for ReAct baseline and 0.123 for Random. While both ReAct variants significantly outperformed Random (p<0.001), action tracking did not improve performance over the baseline ReAct agent (p=0.905), though it did reduce action repetition (21.2% vs 37.5%).	Adding action history tracking to a ReAct agent will improve its performance by helping it learn from past successes and failures in similar contexts.
967326509235	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	action-outcome-tracking-copy5	Create agents that track and learn from their action successes and failures in text games.	64.70942703333334	1.227689	6	317	2667	completed	1	1	deviations	support	ReAct+ActionTracking outperformed baselines in cooking game, trending better but not significantly better than ReAct.	A pilot experiment (n=10 episodes) compared Random, ReAct, and ReAct+ActionTracking agents on TextWorldExpress CookingWorld tasks. ReAct+ActionTracking achieved higher mean scores (0.54) than Random (0.16, p<0.001) and ReAct baseline (0.47, p=0.17), with success rates of 20% vs 0% and 10% respectively, suggesting action tracking may improve performance though more data is needed for statistical significance.	Adding action history tracking to a ReAct agent will improve its performance on TextWorld cooking tasks by allowing it to learn from past successes and failures in similar contexts.
813566502851	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	knowledge-graph-verification-copy1	Using an interactive agent to verify automatically constructed knowledge graphs through environment exploration and interaction.	115.5572117	6.280590000000001	26	543	4527	failed (too many debug iterations)	0	0			None		
315802759236	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	knowledge-graph-verification-copy2	Using an interactive agent to verify automatically constructed knowledge graphs through environment exploration and interaction.	59.99673069999999	4.745996999999999	26	326	2796	failed (too many debug iterations)	0	0			None		
790336820322	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	knowledge-graph-verification-copy3	Using an interactive agent to verify automatically constructed knowledge graphs through environment exploration and interaction.	88.87138631666667	2.3701511500000003	11	374	3181	completed	1	0	deviations	inconclusive	ReAct and random agents both achieved perfect knowledge graph verification rates, showing no performance difference.	A comparison of ReAct and random agents for knowledge graph verification in CookingWorld showed both agents achieving 100% verification rates across 10 episodes, with no statistical difference (p=1.0). Both agents successfully verified all triples (19-26 per episode) with high confidence (>0.9), suggesting the task may have been too simple to demonstrate the potential benefits of strategic verification.	A ReAct agent using strategic planning will be more effective at verifying knowledge graph triples than a random exploration agent.
56415914095	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	knowledge-graph-verification-copy4	Using an interactive agent to verify automatically constructed knowledge graphs through environment exploration and interaction.	59.949239633333335	5.538831400000001	26	493	4553	failed (too many debug iterations)	0	0			None		
919178726909	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	knowledge-graph-verification-copy5	Using an interactive agent to verify automatically constructed knowledge graphs through environment exploration and interaction.	155.01871556666669	6.383972150000001	26	416	3764	failed (too many debug iterations)	0	0			None		
508535943840	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-dual-reflection-copy1	Compare sequential two-agent reflection against single-agent reflection in simple cooking tasks.	18.98837275	0.7449207999999999	4	370	3123	completed	1	1	faithful	reject	Dual-agent reflection did not significantly outperform single-agent reflection on CookingWorld tasks.	The experiment compared single-agent reflection (mean score=0.215), dual-agent reflection (mean score=0.190), and random baseline (mean score=0.142) on CookingWorld tasks across 10 episodes per condition. Bootstrap analysis showed no significant differences between conditions (all p>0.67), though reflection quality was high (>0.89) for both reflection conditions.	Sequential dual-agent reflection improves performance on TextWorldExpress CookingWorld tasks compared to single-agent reflection.
658492688930	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-dual-reflection-copy2	Compare sequential two-agent reflection against single-agent reflection in simple cooking tasks.	51.15586	2.98615455	11	716	6312	failed (code parsing issue)	0	0			None		
57383006950	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-dual-reflection-copy3	Compare sequential two-agent reflection against single-agent reflection in simple cooking tasks.	87.30102466666668	6.40244765	26	647	5616	failed (too many debug iterations)	0	0			None		
501715585046	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-dual-reflection-copy4	Compare sequential two-agent reflection against single-agent reflection in simple cooking tasks.	70.51490206666668	5.939365200000001	26	657	5890	failed (too many debug iterations)	0	0			None		
363352845643	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-dual-reflection-copy5	Compare sequential two-agent reflection against single-agent reflection in simple cooking tasks.	73.84293868333333	5.13840845	19	650	5363	completed	1	1	faithful	reject	Dual-agent reflection did not significantly outperform single-agent reflection on CookingWorld tasks.	The experiment compared dual-agent vs single-agent reflection on CookingWorld tasks across 10 episodes. While both reflection methods significantly outperformed random baseline (p<0.001), dual-agent reflection (mean=0.223) did not significantly outperform single-agent reflection (mean=0.204, p=0.426), suggesting that adding a second reflective agent may not substantially improve performance.	Sequential dual-agent reflection improves performance on TextWorldExpress CookingWorld tasks compared to single-agent reflection.
462841209202	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	template-world-generation-copy1	Generating single-room text-based game environments using templates and controlled object variation.	97.98461366666668	2.3152957	12	424	4049	completed	1	0	faithful	support	Template-generated and manual text adventures showed equivalent perfect performance in pilot study.	A pilot experiment compared ReAct agent performance in 5 template-generated vs 2 manually-created text adventure environments, with 5 episodes per environment. Both conditions achieved 100% success rates with similar completion times (template: 6.56 steps, baseline: 6.4 steps, p=1.0), suggesting template generation can create viable game environments but may need more challenging scenarios to differentiate performance.	Template-based generation can create text adventure game environments that are as effective as manually created environments for ReAct agent interaction and goal completion.
148514632093	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	template-world-generation-copy2	Generating single-room text-based game environments using templates and controlled object variation.	370.3716801666667	4.6218693	23	543	5014	failed (hard experiment runtime limit reached)	0	0			None		
8776057569	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	template-world-generation-copy3	Generating single-room text-based game environments using templates and controlled object variation.	37.538767533333335	3.4689003	14	589	5927	failed (code parsing issue)	0	0			None		
969709661546	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	template-world-generation-copy4	Generating single-room text-based game environments using templates and controlled object variation.	36.2518099	2.4554967	14	338	3224	completed	1	1	deviations	support	Template-generated game environments performed slightly worse than manual ones for ReAct agent task completion.	A pilot experiment comparing template-generated vs manual text adventure environments showed template environments achieved 84% success rate vs 100% for manual environments, with slightly higher average steps to completion (1.52 vs 1.4). While generally successful, template environments exhibited some failure modes with agents getting stuck in examination loops.	Template-based generation can produce text adventure game environments that are as effective as manually created ones for ReAct agent task completion.
932803340282	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	template-world-generation-copy5	Generating single-room text-based game environments using templates and controlled object variation.	48.11364256666666	3.1357576000000007	15	22	241	completed	1	1	faithful	support	Template-generated environments matched manual ones in success rate but offered more diverse action choices.	Template-generated and manual environments both achieved 100% success rates with 3.0 average steps, but template environments offered significantly more action choices (3.55 vs 2.75 valid actions per state, p=0.005). Results indicate template generation successfully created more diverse environments while maintaining equivalent task completion performance.	Template-based generation can create text adventure game environments that are as effective as manually created ones while potentially offering greater variety.
944446605126	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-self-evaluation-copy1	Using simple self-evaluation to improve ReAct agent performance in cooking tasks.	374.43499288333334	6.3509338	19	731	6224	failed (hard experiment runtime limit reached)	0	0			None		
854054188762	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-self-evaluation-copy2	Using simple self-evaluation to improve ReAct agent performance in cooking tasks.	109.35134620000001	6.612117949999999	26	530	5387	failed (too many debug iterations)	0	0			None		
394605940947	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-self-evaluation-copy3	Using simple self-evaluation to improve ReAct agent performance in cooking tasks.	75.31818185	4.352877	15	705	6337	failed (code parsing issue)	0	0			None		
186468487863	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-self-evaluation-copy4	Using simple self-evaluation to improve ReAct agent performance in cooking tasks.	159.29967121666667	4.51148325	16	612	5657	completed	1	1	faithful	inconclusive	Self-evaluating ReAct agent showed better but not significantly better performance than baseline on cooking tasks.	An experiment testing whether adding self-evaluation to a ReAct agent improves CookingWorld task performance found the enhanced agent achieved higher mean scores (0.435) compared to baseline ReAct (0.29) and random (0.167). While the enhanced agent significantly outperformed random (p=0.002), the improvement over baseline ReAct was not statistically significant (p=0.174) in this 10-episode pilot study.	Adding an additional layer of self-evaluation to a ReAct agent improves its performance on structured sequential decision-making tasks.
475958626388	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-self-evaluation-copy5	Using simple self-evaluation to improve ReAct agent performance in cooking tasks.	75.29271811666668	1.85143385	8	525	4578	completed	1	0	deviations	reject	Adding self-evaluation to ReAct agents did not improve performance on cooking tasks (p=0.617).	The experiment compared baseline ReAct, enhanced ReAct (with self-evaluation), and random agents on CookingWorld tasks over 10 episodes each. The enhanced agent (score 0.473) performed similarly to baseline (score 0.500, p=0.617), and both significantly outperformed random (score 0.130, p<0.001), suggesting self-evaluation did not improve performance.	Adding an additional layer of self-evaluation to a ReAct agent improves its performance in TextWorldExpress CookingWorld tasks.
279718573518	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	subgoal-quality-evaluation-copy1	Evaluating whether LLM-based subgoal filtering improves hierarchical agent performance in simple text games.	150.3059209	3.8133118500000003	13	646	5985	failed (code parsing issue)	0	0			None		
144077566040	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	subgoal-quality-evaluation-copy2	Evaluating whether LLM-based subgoal filtering improves hierarchical agent performance in simple text games.	47.99232081666666	2.5557027000000003	9	554	5775	failed (code parsing issue)	0	0			None		
976965224033	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	subgoal-quality-evaluation-copy3	Evaluating whether LLM-based subgoal filtering improves hierarchical agent performance in simple text games.	337.94794068333334	3.9503463	15	505	5327	completed	1	1	faithful	inconclusive	LLM subgoal filtering showed promising but inconclusive improvements in hierarchical agent performance on TWC tasks.	The experiment tested LLM-based subgoal filtering in hierarchical agents on TWC tasks, comparing against unfiltered hierarchical, vanilla ReAct, and random baselines across 25 episodes. The filtered hierarchical approach achieved mean score 0.278 vs 0.144 for unfiltered (p=0.060), suggesting potential benefits of filtering though more samples are needed for statistical significance.	LLM-based subgoal filtering improves the performance of hierarchical agents on TextWorld Common Sense tasks by helping agents pursue more relevant and achievable subgoals.
551581859984	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	subgoal-quality-evaluation-copy4	Evaluating whether LLM-based subgoal filtering improves hierarchical agent performance in simple text games.	116.2362291	6.534118449999999	26	365	3753	failed (too many debug iterations)	0	0			None		
973459169285	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	subgoal-quality-evaluation-copy5	Evaluating whether LLM-based subgoal filtering improves hierarchical agent performance in simple text games.	87.64967758333334	4.002393	12	669	6226	failed (code parsing issue)	0	0			None		
921632548153	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-task-composition-copy1	Evaluating two-step task composition learning in CookingWorld using LLMs.	77.83106538333332	6.0185369999999985	26	536	4952	failed (too many debug iterations)	0	0			None		
575689750236	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-task-composition-copy2	Evaluating two-step task composition learning in CookingWorld using LLMs.	70.52031385000001	7.0454581	26	710	6380	failed (too many debug iterations)	0	0			None		
34845977036	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-task-composition-copy3	Evaluating two-step task composition learning in CookingWorld using LLMs.	27.120653100000002	2.8401204	11	670	6073	failed (code parsing issue)	0	0			None		
228144964700	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-task-composition-copy4	Evaluating two-step task composition learning in CookingWorld using LLMs.	77.41999343333333	7.114627750000001	26	703	6085	failed (too many debug iterations)	0	0			None		
489816282215	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-task-composition-copy5	Evaluating two-step task composition learning in CookingWorld using LLMs.	64.28074883333333	3.6156169500000006	15	615	6223	failed (code parsing issue)	0	0			None		
38504154982	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-affordance-exploration-copy1	Test if basic affordance predictions can improve exploration in simple science tasks.	3.75483565	0.30084900000000003	2	305	2531	failed (code parsing issue)	0	0			None		
616135863846	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-affordance-exploration-copy2	Test if basic affordance predictions can improve exploration in simple science tasks.	80.39011551666667	5.661174950000001	26	399	3670	failed (too many debug iterations)	0	0			None		
87632355116	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-affordance-exploration-copy3	Test if basic affordance predictions can improve exploration in simple science tasks.	101.83029108333334	6.6916935	26	572	5280	failed (too many debug iterations)	0	0			None		
125739833194	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-affordance-exploration-copy4	Test if basic affordance predictions can improve exploration in simple science tasks.	154.79767845	6.833487699999999	26	561	5409	failed (too many debug iterations)	0	0			None		
440959613477	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-affordance-exploration-copy5	Test if basic affordance predictions can improve exploration in simple science tasks.	99.96087618333333	2.6108959	11	454	4136	completed	1	1	faithful	support	LLM-guided affordance prediction significantly improved performance across three ScienceWorld tasks compared to random exploration.	An LLM-guided affordance prediction agent was compared against a random baseline on three ScienceWorld tasks with 10 episodes each. The affordance agent significantly outperformed the baseline across all tasks, with the largest improvement in 'find-living-thing' (24.2 vs 10.8, p<0.001) and 'use-thermometer' (11.1 vs 1.8, p<0.001), demonstrating that LLM guidance can substantially improve exploration efficiency.	LLM-guided affordance prediction can improve exploration efficiency in text-based environments by providing more targeted and contextually relevant action selection.
163110508808	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	basic-knowledge-sharing-copy1	Study the effectiveness of basic knowledge sharing between two ReAct agents using a shared graph structure.	81.10828741666666	5.4088755	17	716	6101	failed (code parsing issue)	0	0			None		
955403585058	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	basic-knowledge-sharing-copy2	Study the effectiveness of basic knowledge sharing between two ReAct agents using a shared graph structure.	197.97394226666665	5.041197899999999	26	484	3985	failed (too many debug iterations)	0	0			None		
447091771052	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	basic-knowledge-sharing-copy3	Study the effectiveness of basic knowledge sharing between two ReAct agents using a shared graph structure.	78.09376300000001	6.6900039	26	554	5059	failed (too many debug iterations)	0	0			None		
825739594975	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	basic-knowledge-sharing-copy4	Study the effectiveness of basic knowledge sharing between two ReAct agents using a shared graph structure.	126.07508325	1.6080322500000002	7	378	3216	completed	1	1	deviations	reject	Shared knowledge graphs did not improve two-agent performance compared to baselines in cooking game tasks.	A comparison of single-agent, two-agent independent, and two-agent shared knowledge graph conditions in a cooking game showed the single agent performed best (mean score 0.062) while both two-agent conditions performed equally poorly (mean 0.011). Knowledge graphs captured environmental information but did not improve performance, with bootstrap analysis showing no significant advantage for sharing (p=1.0).	Shared knowledge graphs facilitate effective information sharing between two agents and improve their task performance compared to independent agents or single agents.
111155593102	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	basic-knowledge-sharing-copy5	Study the effectiveness of basic knowledge sharing between two ReAct agents using a shared graph structure.	371.54364104999996	4.87095165	20	611	5309	failed (hard experiment runtime limit reached)	0	0			None		
199971145662	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	textworld-subgoal-planning-copy1	Evaluate subgoal-based planning versus direct planning in TextWorldExpress cooking tasks.	259.7226508333333	8.05487785	26	523	4472	failed (too many debug iterations)	0	0			None		
161551435529	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	textworld-subgoal-planning-copy2	Evaluate subgoal-based planning versus direct planning in TextWorldExpress cooking tasks.	45.08717058333333	4.15133535	14	719	6098	failed (code parsing issue)	0	0			None		
39689902464	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	textworld-subgoal-planning-copy3	Evaluate subgoal-based planning versus direct planning in TextWorldExpress cooking tasks.	127.76748618333333	7.0296105	26	488	4584	failed (too many debug iterations)	0	0			None		
722792364926	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	textworld-subgoal-planning-copy4	Evaluate subgoal-based planning versus direct planning in TextWorldExpress cooking tasks.	267.9753516	7.69282105	26	659	6059	failed (too many debug iterations)	0	0			None		
685813819680	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	textworld-subgoal-planning-copy5	Evaluate subgoal-based planning versus direct planning in TextWorldExpress cooking tasks.	33.5331744	2.8932780000000005	10	663	6229	failed (code parsing issue)	0	0			None		
660124511770	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	knowledge-guided-decomposition-copy1	Using knowledge graphs to guide task decomposition decisions in text-based environments.	62.5029392	5.81421915	26	533	5100	failed (too many debug iterations)	0	0			None		
323768848836	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	knowledge-guided-decomposition-copy2	Using knowledge graphs to guide task decomposition decisions in text-based environments.	147.13794045	6.619697350000001	26	577	5039	failed (too many debug iterations)	0	0			None		
591710254841	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	knowledge-guided-decomposition-copy3	Using knowledge graphs to guide task decomposition decisions in text-based environments.	266.5797191666667	6.5781404	26	531	4604	failed (too many debug iterations)	0	0			None		
908622538577	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	knowledge-guided-decomposition-copy4	Using knowledge graphs to guide task decomposition decisions in text-based environments.	38.884777633333336	2.45526105	9	709	6045	failed (code parsing issue)	0	0			None		
35985844993	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	knowledge-guided-decomposition-copy5	Using knowledge graphs to guide task decomposition decisions in text-based environments.	56.70516815	2.8461156	10	640	6021	failed (code parsing issue)	0	0			None		
420198010958	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	static-knowledge-comparison-copy1	Evaluate the relative effectiveness of ConceptNet versus LLM-derived knowledge for improving agent performance in ScienceWorld tasks.	174.23948661666665	4.76984495	17	583	5218	completed	1	1	deviations	inconclusive	LLM knowledge injection showed higher success (90%) than baseline (60%) in boiling water task, but not statistically significant.	In a comparison of knowledge injection methods for the ScienceWorld boiling water task, LLM-based knowledge achieved a 90% success rate (avg score 2.4) compared to baseline's 60% (avg score 1.6) and ConceptNet's 50% (avg score 1.4). While these differences suggest a potential advantage for LLM-based knowledge, bootstrap statistical testing did not show significant differences between conditions (all p-values = 1.0).	Different forms of knowledge injection (ConceptNet, LLM, or random selection) will improve performance on the ScienceWorld boiling water task compared to a baseline without knowledge injection.
798480593521	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	static-knowledge-comparison-copy2	Evaluate the relative effectiveness of ConceptNet versus LLM-derived knowledge for improving agent performance in ScienceWorld tasks.	250.24756948333334	8.1537932	26	400	3653	failed (too many debug iterations)	0	0			None		
125800817252	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	static-knowledge-comparison-copy3	Evaluate the relative effectiveness of ConceptNet versus LLM-derived knowledge for improving agent performance in ScienceWorld tasks.	186.81347036666665	8.691024950000001	26	582	5387	failed (too many debug iterations)	0	0			None		
393590627655	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	static-knowledge-comparison-copy4	Evaluate the relative effectiveness of ConceptNet versus LLM-derived knowledge for improving agent performance in ScienceWorld tasks.	348.8318519	9.4376796	26	454	4000	failed (too many debug iterations)	0	0			None		
353586813817	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	static-knowledge-comparison-copy5	Evaluate the relative effectiveness of ConceptNet versus LLM-derived knowledge for improving agent performance in ScienceWorld tasks.	221.47833795	6.7184684500000005	26	439	3565	failed (too many debug iterations)	0	0			None		
822321180744	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-social-graphs-copy1	Test whether simple relationship tracking using knowledge graphs improves agent social decision making.	9.516867933333334	0.54138475	3	349	2791	completed	1	1	faithful	reject	Graph-based relationship tracking did not improve agent social decision-making compared to simpler approaches.	The experiment compared a graph-based relationship tracking agent against two baselines across 5 social scenarios with 5 decisions each. The experimental agent achieved a mean appropriateness score of 4.0, slightly lower than both baselines at 4.16, with no statistical significance (p=0.96). Results suggest that explicit relationship tracking did not improve social decision-making in this context.	Simple relationship tracking using knowledge graphs improves agent social decision making compared to approaches that don't track relationships.
798375093323	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-social-graphs-copy2	Test whether simple relationship tracking using knowledge graphs improves agent social decision making.	16.574917566666667	0.8014108499999999	4	693	5387	completed	1	1	faithful	reject	Graph-based relationship tracking did not improve LLM social decision-making over baselines in simple scenarios.	The experiment compared a graph-based relationship tracking agent against two baselines across 5 social scenarios with 5 decisions each. All agents performed well (mean scores: experimental=4.32, baseline_no_graph=4.44, baseline_static=4.4) with no statistically significant differences (p=1.0), suggesting that explicit relationship tracking did not improve performance over the LLM's inherent social reasoning capabilities.	Using knowledge graphs to track relationships improves agent social decision making compared to approaches without explicit relationship tracking.
485603714609	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-social-graphs-copy3	Test whether simple relationship tracking using knowledge graphs improves agent social decision making.	8.453552066666667	0.5721020499999999	3	357	2977	completed	1	1	faithful	reject	Relationship tracking decreased agent performance in social decisions compared to simpler approaches.	An experiment comparing relationship-aware vs baseline agents across 5 social scenarios with 5 decisions each found that explicitly tracking relationships led to worse performance (experimental mean=4.19 vs baseline means of 4.90 and 4.86, p<0.001). The relationship-tracking agent made decisions that acknowledged conflicts but often handled them inappropriately by avoiding rather than resolving tensions.	Using knowledge graphs to track relationships improves agent social decision-making by providing relevant social context.
379863258608	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-social-graphs-copy4	Test whether simple relationship tracking using knowledge graphs improves agent social decision making.	20.926806916666667	1.00487455	5	452	3758	completed	1	1	faithful	reject	Graph-based relationship tracking did not improve agent social decisions compared to simpler approaches.	The experiment compared a graph-based relationship tracking agent against two baselines across 5 social scenarios with multiple decision points. The experimental agent achieved a mean appropriateness score of 4.29 compared to 4.83 for both baselines, with bootstrap analysis showing no significant difference (p=1.0). Results suggest relationship graphs may not improve social decision making, though the sample size was limited.	Using knowledge graphs to track and reason about relationships improves agent social decision making compared to approaches without explicit relationship tracking.
37447724888	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-social-graphs-copy5	Test whether simple relationship tracking using knowledge graphs improves agent social decision making.	14.66123	0.69753635	4	404	3430	completed	1	1	faithful	reject	Relationship graph-based agent performed worse than baselines in social decision making tasks.	The experiment compared three agents making social decisions: one using relationship graphs and two baselines. The experimental agent (mean score 4.28) performed significantly worse than both baseline agents (4.96 and 5.0, p=1.0 for both comparisons), suggesting that explicit relationship tracking may have led to overly cautious or biased decision making.	Using knowledge graphs to track relationships improves agent social decision making compared to agents without relationship tracking.
583347880103	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-metaphor-graph-copy1	Create and visualize knowledge graphs showing metaphorical relationships between objects in cooking game scenarios.	13.82011855	0.7197499500000001	4	333	2977	completed	1	1	deviations	inconclusive	Study found rich metaphorical relationships between kitchen objects but no significant graph structural differences from baselines.	The experiment analyzed metaphorical relationships between kitchen objects across 3 scenarios, identifying an average of 40.7 metaphors per scenario through LLM-based analysis. While rich qualitative metaphorical relationships were found (122 total), graph metrics showed no significant structural differences between baseline and experimental graphs (p=1.0, density=1.0 for all graphs), suggesting the metaphor-based connections did not create distinct network patterns.	Objects in a kitchen environment have meaningful metaphorical relationships based on their functional similarities that create distinct patterns of connectivity beyond simple co-occurrence.
976230850404	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-metaphor-graph-copy2	Create and visualize knowledge graphs showing metaphorical relationships between objects in cooking game scenarios.	11.089056983333334	0.4304889	3	249	2220	completed	1	1	deviations	support	Kitchen objects show more metaphorical relationships than physical co-occurrences in CookingWorld scenarios.	Analysis of three CookingWorld scenarios revealed that metaphorical relationships between kitchen objects (average 52.0 edges/scenario) were more numerous than physical co-occurrence relationships (average 37.0 edges/scenario). The LLM consistently identified meaningful functional similarities between objects, suggesting kitchen environments contain rich networks of both physical and metaphorical object relationships.	Objects in cooking environments have meaningful metaphorical relationships based on functional similarities that extend beyond their physical co-occurrence relationships.
487864606623	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-metaphor-graph-copy3	Create and visualize knowledge graphs showing metaphorical relationships between objects in cooking game scenarios.	20.04257833333333	0.44597590000000004	3	261	2180	completed	1	1	deviations	support	LLM-based system detected meaningful metaphorical relationships between cooking objects with 75-79% graph density versus baseline.	The experiment analyzed metaphorical relationships between cooking objects across 3 scenarios using LLM-based similarity detection, comparing co-occurrence, random, and metaphor-enhanced graphs. Results showed metaphor graphs maintained consistent density (75-79% of baseline) across scenarios, with an average of 138 meaningful functional similarities detected between objects versus 184 possible co-occurrence pairs, suggesting systematic detection of non-trivial metaphorical relationships.	Objects in cooking scenarios have systematic metaphorical relationships based on functional similarities that can be detected using language models.
204424222646	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-metaphor-graph-copy4	Create and visualize knowledge graphs showing metaphorical relationships between objects in cooking game scenarios.	11.676575183333334	0.4693793	3	274	2307	completed	1	1	deviations	inconclusive	LLM successfully identified kitchen object metaphors, but single-location design limited quantitative analysis.	The experiment analyzed metaphorical relationships between kitchen objects using LLM-based detection, finding 48-55 meaningful metaphorical connections per scenario across 3 scenarios with 12-13 objects each. While the system successfully identified plausible functional similarities between objects, the experimental design's limitation of single-location scenarios resulted in fully-connected graphs (density=1.0) for both baseline and experimental conditions, preventing meaningful statistical comparison.	Objects in cooking environments have meaningful metaphorical relationships based on their functional similarities that go beyond simple co-occurrence.
264647067112	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-metaphor-graph-copy5	Create and visualize knowledge graphs showing metaphorical relationships between objects in cooking game scenarios.	16.743857533333333	0.6995388499999999	4	328	3009	completed	1	1	deviations	support	Kitchen objects show rich metaphorical relationships, forming denser networks than random connections would predict.	The experiment compared co-occurrence, random, and metaphor-enhanced graphs of kitchen object relationships across 3 scenarios. Both co-occurrence and metaphor graphs showed perfect density (1.0) and clustering (1.0), significantly higher than random graphs (density ~0.73-0.82, clustering ~0.66-0.84), suggesting kitchen objects form meaningful functional networks beyond chance relationships.	Objects in cooking environments have meaningful metaphorical relationships based on functional similarities that go beyond simple co-occurrence patterns.
94054039765	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	rule-guided-action-validation-copy1	Evaluate whether simple cooking rules can improve action selection validity in TextWorldExpress cooking tasks.	151.77086183333333	8.474610499999999	26	641	6171	failed (too many debug iterations)	0	0			None		
968554790773	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	rule-guided-action-validation-copy2	Evaluate whether simple cooking rules can improve action selection validity in TextWorldExpress cooking tasks.	76.37770758333333	6.481039050000001	19	665	6379	failed (code parsing issue)	0	0			None		
22786435429	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	rule-guided-action-validation-copy3	Evaluate whether simple cooking rules can improve action selection validity in TextWorldExpress cooking tasks.	45.78023605	1.0860154	5	369	3157	completed	1	0	deviations	inconclusive	Adding cooking rules to ReAct agent prompts did not improve performance on TextWorldExpress cooking tasks.	A comparison of ReAct agents with and without cooking domain rules showed no significant differences in performance across 10 episodes, with both achieving 10% task completion rates and ~17% valid action ratios (p>0.05 for both metrics). The results suggest that adding explicit cooking rules to the prompt did not meaningfully improve agent performance on TextWorldExpress cooking tasks, though the small sample size limits confidence in these findings.	Adding explicit cooking domain rules to a ReAct agent's prompt will improve its performance on cooking-related tasks compared to a baseline prompt without such rules.
484463239730	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	rule-guided-action-validation-copy4	Evaluate whether simple cooking rules can improve action selection validity in TextWorldExpress cooking tasks.	33.33083918333333	3.0647139	11	686	6253	failed (code parsing issue)	0	0			None		
430533900338	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	rule-guided-action-validation-copy5	Evaluate whether simple cooking rules can improve action selection validity in TextWorldExpress cooking tasks.	55.998748133333336	5.3598524	26	556	4511	failed (too many debug iterations)	0	0			None		
195508367351	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	kg-state-tracking-copy1	Using knowledge graphs to track object locations and properties in a simplified CookingWorld environment.	49.940342816666664	3.2745543500000003	15	508	4319	completed	1	1	faithful	support	Knowledge graph augmentation improved state tracking accuracy from 0.42 to 0.66 in CookingWorld environment.	A pilot comparison of text-only vs. knowledge-graph-augmented state tracking in CookingWorld showed the KG approach achieved significantly higher accuracy (0.66 vs 0.42, p=0.011, n=5 per condition). The results suggest knowledge graphs may improve state tracking, though the small sample size limits generalizability.	Knowledge graph representations of state information will improve the accuracy of language model-based state tracking in text-based games compared to text-only representations.
796444343966	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	kg-state-tracking-copy2	Using knowledge graphs to track object locations and properties in a simplified CookingWorld environment.	45.828829033333335	3.7038411	16	563	5098	completed	1	1	faithful	inconclusive	KG-augmented state tracking showed better but non-significant performance vs text-only in CookingWorld pilot study.	A pilot comparison of text-only vs. knowledge-graph-augmented state tracking in CookingWorld found the KG model achieved higher mean accuracy (0.60 vs 0.43) but the difference was not statistically significant (p=0.26). Both models showed high performance variability (0.0-1.0 accuracy range), suggesting the need for larger-scale evaluation.	Knowledge graph augmentation improves the accuracy of state tracking in text-based environments compared to text-only approaches.
762919571681	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	kg-state-tracking-copy3	Using knowledge graphs to track object locations and properties in a simplified CookingWorld environment.	28.06048816666667	1.68817785	8	446	4096	completed	1	0	faithful	inconclusive	Knowledge graphs slightly improved state tracking accuracy (15.8% vs 13.7%), but difference not significant.	A pilot experiment comparing text-only vs. knowledge-graph-augmented state tracking in CookingWorld found KG-augmented tracking (acc=0.158) performed slightly better than text-only tracking (acc=0.137), but this difference was not statistically significant (p=0.337). Both approaches showed relatively low absolute accuracy, suggesting the task was challenging for the GPT-4o-mini model.	Knowledge graph augmentation improves the accuracy of language model-based state tracking in text-based games compared to text-only approaches.
995952549697	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	kg-state-tracking-copy4	Using knowledge graphs to track object locations and properties in a simplified CookingWorld environment.	26.640009866666666	1.5974182	7	446	4516	completed	1	1	deviations	reject	Knowledge graph augmentation showed no improvement over text-only state tracking in CookingWorld (both 24% accuracy).	A pilot comparison of knowledge-graph-augmented vs. text-only state tracking in CookingWorld showed identical mean accuracies (0.24) for both conditions across 10 episodes, with bootstrap analysis showing no significant difference (p=0.527). The experiment suggests that knowledge graph augmentation did not improve state tracking performance with the current implementation.	Knowledge graph augmentation of prompts will improve an LLM's ability to track object states in a text-based environment by providing structured representation of spatial and property relationships.
75415238230	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	kg-state-tracking-copy5	Using knowledge graphs to track object locations and properties in a simplified CookingWorld environment.	21.195021783333335	1.2669177999999999	7	425	3703	completed	1	1	faithful	support	Knowledge graph augmentation improved LLM state tracking accuracy from 0.121 to 0.245 (p=0.025).	A pilot experiment comparing text-only vs. knowledge-graph-augmented state tracking in CookingWorld found that KG augmentation significantly improved tracking accuracy (KG: 0.245 vs. baseline: 0.121, p=0.025). While the absolute performance was relatively low, the consistent improvement across episodes suggests KG visualization helps LLMs maintain better state awareness, particularly for initial scene understanding.	Augmenting LLM prompts with knowledge graph visualizations improves the model's ability to track object states and locations in text-based environments.
911970036047	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	react-pattern-learning-copy1	Investigating pattern-based reasoning reuse in ReAct agents on cooking tasks.	274.40153741666666	7.2369715	26	558	4983	failed (too many debug iterations)	0	0			None		
183996836419	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	react-pattern-learning-copy2	Investigating pattern-based reasoning reuse in ReAct agents on cooking tasks.	375.54797729999996	5.587553050000001	19	658	5762	failed (hard experiment runtime limit reached)	0	0			None		
319437151158	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	react-pattern-learning-copy3	Investigating pattern-based reasoning reuse in ReAct agents on cooking tasks.	386.02167311666665	2.33933795	6	376	3412	failed (hard experiment runtime limit reached)	0	0			None		
442685836980	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	react-pattern-learning-copy4	Investigating pattern-based reasoning reuse in ReAct agents on cooking tasks.	195.73437731666667	6.684205	26	558	5039	failed (too many debug iterations)	0	0			None		
888276815690	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	react-pattern-learning-copy5	Investigating pattern-based reasoning reuse in ReAct agents on cooking tasks.	519.9860001166667	3.1425544500000004	10	405	3591	failed (hard experiment runtime limit reached)	0	0			None		
142623178601	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	progressive-state-complexity-copy1	Study how increasing state representation complexity affects LLM simulation accuracy in text-based games.	537.3052658333334	1.40839585	7	441	3823	completed	1	1	deviations	support	Simpler state representations (boolean/numerical) enable more accurate LLM world simulation than complex full states.	The experiment tested how state complexity affects LLM simulation accuracy across four complexity levels. Boolean states achieved highest accuracy (80.1%), followed by numerical (72.2%) and relational (72.8%), while full state representation performed significantly worse (31.3%, p=1.0 compared to others). This demonstrates that simpler state representations enable more accurate LLM world simulation.	Increasing state representation complexity negatively affects LLM simulation accuracy in virtual environments.
71213778464	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	progressive-state-complexity-copy2	Study how increasing state representation complexity affects LLM simulation accuracy in text-based games.	105.34547165000001	1.6126491000000003	8	416	3592	completed	1	1	deviations	reject	LLMs showed near-perfect simulation accuracy across all state complexity levels in CookingWorld.	The experiment tested LLM simulation accuracy across four complexity levels in CookingWorld, finding surprisingly high accuracy across all levels (boolean/numerical: 1.0, relational: 0.963, full: 0.952) with no statistically significant differences between levels (all p=1.0). However, the implementation may not have fully captured the intended complexity differences, suggesting these results should be interpreted cautiously.	Increasing state representation complexity in CookingWorld will decrease LLM simulation accuracy due to increased cognitive load and prediction complexity.
184901381441	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	progressive-state-complexity-copy3	Study how increasing state representation complexity affects LLM simulation accuracy in text-based games.	164.12631663333335	1.4608975	7	309	2768	completed	1	1	faithful	support	Relational state complexity improved LLM simulation accuracy, while full complexity reduced performance.	Testing four levels of state representation complexity in CookingWorld, the experiment found that relational complexity (82.40%) significantly outperformed the boolean baseline (80.82%, p=0.039), while full complexity (76.52%) performed worse. This suggests that intermediate levels of state representation complexity may be optimal for LLM simulation accuracy.	Increasing state representation complexity in CookingWorld will improve LLM simulation accuracy by providing more detailed information for predictions.
675271173941	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	progressive-state-complexity-copy4	Study how increasing state representation complexity affects LLM simulation accuracy in text-based games.	200.5659977	1.95466155	9	421	4166	completed	1	1	deviations	reject	Increasing state complexity in CookingWorld reduced LLM prediction accuracy from 94.4% to 79.0%.	The experiment tested four levels of state complexity in CookingWorld, finding that simpler representations led to better LLM predictions: boolean states achieved 94.4% accuracy, while full state complexity achieved only 79.0% accuracy. Bootstrap resampling confirmed these differences were statistically significant (p < 0.02), suggesting that increased state complexity consistently degrades LLM prediction performance.	Increasing state representation complexity will improve LLM simulation accuracy by providing more context and information for predictions.
630216925892	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	progressive-state-complexity-copy5	Study how increasing state representation complexity affects LLM simulation accuracy in text-based games.	36.22597735	0.5390922	3	350	2810	completed	1	1	faithful	support	Increased state complexity in CookingWorld significantly improved LLM prediction accuracy from 83.5% to 93.9%.	Testing four levels of state complexity in CookingWorld, the experiment found that prediction accuracy increased significantly with complexity, from 83.5% (boolean) to 93.9% (full). Bootstrap analysis showed significant improvements (p < 0.05) between boolean vs. full and numerical vs. full representations, supporting the hypothesis that increased state complexity improves LLM simulation accuracy.	Increasing state representation complexity improves LLM simulation accuracy in CookingWorld environments.
943297230472	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-decomposition-memory-copy1	Using a history of successful decompositions to guide future task solving in cooking-related text games.	140.54963136666666	6.1206482499999995	26	487	4467	failed (too many debug iterations)	0	0			None		
333031840089	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-decomposition-memory-copy2	Using a history of successful decompositions to guide future task solving in cooking-related text games.	391.32797165	4.25254715	17	603	5429	failed (hard experiment runtime limit reached)	0	0			None		
760707492923	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-decomposition-memory-copy3	Using a history of successful decompositions to guide future task solving in cooking-related text games.	377.4287038666667	4.5562311	18	498	4569	failed (hard experiment runtime limit reached)	0	0			None		
590030803866	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-decomposition-memory-copy4	Using a history of successful decompositions to guide future task solving in cooking-related text games.	157.21209636666666	6.3032588999999986	26	378	3175	completed	1	1	faithful	inconclusive	Decomposition history slightly improved agent performance (0.183 vs 0.117) but results weren't statistically significant.	The experiment compared a baseline ReAct agent against one augmented with decomposition history in TextWorldExpress CookingWorld across 10 training and 5 test episodes. The experimental agent achieved a higher mean score (0.183 vs 0.117) but this difference was not statistically significant (p=0.324), suggesting that while decomposition history might be beneficial, more data is needed for conclusive results.	Maintaining a history of successful task decompositions improves agent performance in sequential decision-making tasks.
386416286357	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-decomposition-memory-copy5	Using a history of successful decompositions to guide future task solving in cooking-related text games.	217.83142976666667	6.8913737500000005	26	604	5244	failed (too many debug iterations)	0	0			None		
411584982981	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	kg-failure-detection-copy1	Using knowledge graph features to detect action failures in text-based games	182.40777340000002	6.3758104499999995	25	622	5631	completed	1	1	deviations	reject	KG-based and text similarity failure detectors outperformed keyword detection, but all showed high false positives.	A comparison of three failure detection methods in TextWorldExpress CookingWorld showed knowledge graph-based (F1=0.36) and text similarity (F1=0.39) approaches performed similarly and significantly better than keyword detection (F1=0.15). All methods showed high false positive rates (>0.57), suggesting the need for more sophisticated failure detection mechanisms.	Knowledge graph-based failure detection will be more effective than simpler baseline methods (text similarity and keyword matching) for detecting failures in text-based games.
859737548609	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	kg-failure-detection-copy2	Using knowledge graph features to detect action failures in text-based games	121.216326	6.8112223499999995	26	650	5718	failed (too many debug iterations)	0	0			None		
317647777690	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	kg-failure-detection-copy3	Using knowledge graph features to detect action failures in text-based games	36.650547233333334	1.78669215	7	690	6123	failed (code parsing issue)	0	0			None		
711719368774	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	kg-failure-detection-copy4	Using knowledge graph features to detect action failures in text-based games	83.40543068333334	6.5480839500000005	26	654	5671	failed (too many debug iterations)	0	0			None		
341430618656	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	kg-failure-detection-copy5	Using knowledge graph features to detect action failures in text-based games	218.50076078333333	1.3375605	6	536	4304	completed	1	0	deviations	reject	Three game failure detectors performed poorly, with text similarity achieving best but still low 1.15% F1 score.	A comparison of three failure detection methods (knowledge graph, text similarity, keyword) in a cooking game environment showed uniformly poor performance, with the text similarity detector achieving the best but still very low F1 score of 1.15%. The knowledge graph and keyword approaches completely failed to detect any true failures, suggesting fundamental issues with the detection logic or thresholds.	Knowledge-graph-based failure detection will outperform simpler text similarity and keyword-based approaches for detecting failures in interactive text environments.
617676315857	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	two-level-discovery-agent-copy1	Two-level hierarchical agent that separates planning and execution for scientific measurement tasks.	402.8563518	1.24239585	2	106	873	failed (hard experiment runtime limit reached)	0	0			None		
178228209647	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	two-level-discovery-agent-copy2	Two-level hierarchical agent that separates planning and execution for scientific measurement tasks.	639.6181168833333	1.4003564000000002	2	325	2654	failed (hard experiment runtime limit reached)	0	0			None		
262065786453	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	two-level-discovery-agent-copy3	Two-level hierarchical agent that separates planning and execution for scientific measurement tasks.	244.98813533333333	1.7675178	5	677	5891	failed (code parsing issue)	0	0			None		
30588444707	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	two-level-discovery-agent-copy4	Two-level hierarchical agent that separates planning and execution for scientific measurement tasks.	475.2326506	1.29584235	2	493	4041	failed (hard experiment runtime limit reached)	0	0			None		
270713090244	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	two-level-discovery-agent-copy5	Two-level hierarchical agent that separates planning and execution for scientific measurement tasks.	598.1085464333333	1.8767372500000001	3	391	3413	failed (hard experiment runtime limit reached)	0	0			None		
295937158337	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-meta-graphs-copy1	Track ReAct agent performance using simple knowledge graphs to make mode-switching decisions on classification tasks.	416.0137796833333	3.72269035	11	469	4464	failed (hard experiment runtime limit reached)	0	0			None		
451456675057	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-meta-graphs-copy2	Track ReAct agent performance using simple knowledge graphs to make mode-switching decisions on classification tasks.	203.70800248333336	8.576673150000001	26	16	152	failed (too many debug iterations)	0	0			None		
271745499162	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-meta-graphs-copy3	Track ReAct agent performance using simple knowledge graphs to make mode-switching decisions on classification tasks.	177.82438676666666	3.9356618	11	559	5276	completed	1	0	deviations	reject	Knowledge graph-based mode switching showed no clear advantage over baseline strategies in ScienceWorld tasks.	A ReAct agent with knowledge graph-based mode switching was compared against random and fixed mode strategies across four ScienceWorld classification tasks, with 5 episodes per condition. The knowledge graph approach showed no consistent performance advantage (e.g., average scores of 25 in find-living-thing task across all conditions) and often required more computational resources, though limited sample size and incomplete logging prevent definitive conclusions.	Knowledge graph-based mode switching leads to more efficient task completion compared to random or fixed strategies in ReAct agents.
66098906146	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-meta-graphs-copy4	Track ReAct agent performance using simple knowledge graphs to make mode-switching decisions on classification tasks.	64.23606313333333	3.3566466499999996	14	423	4289	completed	1	0	deviations	reject	Knowledge-graph mode switching showed no improvement over random/fixed strategies in ScienceWorld classification tasks.	A pilot study compared knowledge-graph-based mode switching against random and fixed strategies on ScienceWorld classification tasks, with 5 episodes per condition across 4 tasks. Results showed no clear advantage for KG-based switching, with success rates of 0-20% compared to random switching's 0-40%, and similar average scores (13.4-18.6) across all conditions.	Knowledge graph-based mode switching between detailed and quick reasoning strategies leads to more efficient task completion compared to random or fixed strategies.
624286258700	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-meta-graphs-copy5	Track ReAct agent performance using simple knowledge graphs to make mode-switching decisions on classification tasks.	270.10967175	6.942891999999999	26	587	5545	failed (too many debug iterations)	0	0			None		
906018996555	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-hierarchical-beliefs-copy1	Study if simple two-level hierarchical belief graphs improve knowledge representation for temperature-related tasks.	178.0990657	5.905215450000001	26	652	5380	failed (too many debug iterations)	0	0			None		
673817574579	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-hierarchical-beliefs-copy2	Study if simple two-level hierarchical belief graphs improve knowledge representation for temperature-related tasks.	118.30885273333334	5.9642865	26	696	6067	failed (too many debug iterations)	0	0			None		
597084323811	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-hierarchical-beliefs-copy3	Study if simple two-level hierarchical belief graphs improve knowledge representation for temperature-related tasks.	82.15775785000001	5.061689399999999	26	466	3876	failed (too many debug iterations)	0	0			None		
867617571771	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-hierarchical-beliefs-copy4	Study if simple two-level hierarchical belief graphs improve knowledge representation for temperature-related tasks.	89.08089338333333	4.492274549999999	19	659	6108	failed (code parsing issue)	0	0			None		
806403557687	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-hierarchical-beliefs-copy5	Study if simple two-level hierarchical belief graphs improve knowledge representation for temperature-related tasks.	46.88074338333333	2.8608117	13	712	6058	failed (code parsing issue)	0	0			None		
721240298579	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-template-discovery-copy1	Automatically discover and use simple two-action templates from successful gameplay trajectories in CookingWorld.	27.228057850000003	2.5636989	11	704	6118	failed (code parsing issue)	0	0			None		
950545939983	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-template-discovery-copy2	Automatically discover and use simple two-action templates from successful gameplay trajectories in CookingWorld.	71.5607467	4.918305	26	519	4586	failed (too many debug iterations)	0	0			None		
672396988647	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-template-discovery-copy3	Automatically discover and use simple two-action templates from successful gameplay trajectories in CookingWorld.	99.73820004999999	6.087839700000001	26	621	5180	failed (too many debug iterations)	0	0			None		
221005629327	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-template-discovery-copy4	Automatically discover and use simple two-action templates from successful gameplay trajectories in CookingWorld.	93.70332758333333	5.669985449999999	26	553	5368	failed (too many debug iterations)	0	0			None		
811832985731	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-template-discovery-copy5	Automatically discover and use simple two-action templates from successful gameplay trajectories in CookingWorld.	86.98322505	5.61762555	22	675	6136	failed (code parsing issue)	0	0			None		
852506477667	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-abstraction-tuning-copy1	Tune program abstractions based on their success rates in cooking game tasks.	126.67565098333333	6.439796100000001	23	697	6069	failed (code parsing issue)	0	0			None		
253735891021	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-abstraction-tuning-copy2	Tune program abstractions based on their success rates in cooking game tasks.	142.78454574999998	6.9116615999999995	25	680	6197	failed (code parsing issue)	0	0			None		
704088801086	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-abstraction-tuning-copy3	Tune program abstractions based on their success rates in cooking game tasks.	47.77540873333333	3.1481076000000003	9	707	6147	failed (code parsing issue)	0	0			None		
132963729424	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-abstraction-tuning-copy4	Tune program abstractions based on their success rates in cooking game tasks.	83.24401493333333	5.94321255	21	688	6147	failed (code parsing issue)	0	0			None		
43096566006	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	simple-abstraction-tuning-copy5	Tune program abstractions based on their success rates in cooking game tasks.	142.29384704999998	6.232744800000001	21	699	6025	failed (code parsing issue)	0	0			None		
587349385663	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	hypothesis-driven-discovery-copy1	Agent that generates and tests scientific hypotheses in structured environments	71.38729736666666	2.6394486	9	646	6336	failed (code parsing issue)	0	0			None		
781213077279	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	hypothesis-driven-discovery-copy2	Agent that generates and tests scientific hypotheses in structured environments	136.13399193333333	4.711115100000001	15	641	6375	failed (code parsing issue)	0	0			None		
433745482489	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	hypothesis-driven-discovery-copy3	Agent that generates and tests scientific hypotheses in structured environments	195.81160798333335	10.652096	26	627	5757	failed (too many debug iterations)	0	0			None		
593869331626	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	hypothesis-driven-discovery-copy4	Agent that generates and tests scientific hypotheses in structured environments	75.49633576666666	2.9095632	10	622	5918	failed (code parsing issue)	0	0			None		
101555734587	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	hypothesis-driven-discovery-copy5	Agent that generates and tests scientific hypotheses in structured environments	121.39110843333333	6.833891449999999	26	592	6027	failed (too many debug iterations)	0	0			None		
730051921614	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	knowledge-graph-discovery-copy1	Build an agent that creates and updates knowledge graphs while performing scientific discovery tasks in DiscoveryWorld.	368.20192023333334	9.31116845	21	562	5246	failed (hard experiment runtime limit reached)	0	0			None		
619796367613	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	knowledge-graph-discovery-copy2	Build an agent that creates and updates knowledge graphs while performing scientific discovery tasks in DiscoveryWorld.	178.59872218333334	4.194521550000001	14	599	6104	failed (code parsing issue)	0	0			None		
13382614672	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	knowledge-graph-discovery-copy3	Build an agent that creates and updates knowledge graphs while performing scientific discovery tasks in DiscoveryWorld.	370.94183086666663	17.01919865	21	635	6061	failed (hard experiment runtime limit reached)	0	0			None		
534866361937	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	knowledge-graph-discovery-copy4	Build an agent that creates and updates knowledge graphs while performing scientific discovery tasks in DiscoveryWorld.	248.22183071666666	11.40709265	23	600	5946	completed	1	1	deviations	support	Knowledge graph agent outperformed baseline on process metrics but neither agent completed the discovery task.	A knowledge-graph agent was compared to a baseline ReAct agent on a proteomics discovery task across 10 episodes. The knowledge graph agent achieved significantly higher process scores (0.25 vs 0.083, p=0.0079) but neither agent completed the task successfully. The results suggest the knowledge graph approach improved exploration and measurement behavior but fell short of solving the full task.	A knowledge-graph-based agent that maintains and reasons over a structured representation of objects, properties, and hypotheses will perform better at scientific discovery tasks than a standard ReAct agent.
68880514923	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	knowledge-graph-discovery-copy5	Build an agent that creates and updates knowledge graphs while performing scientific discovery tasks in DiscoveryWorld.	177.58748008333333	8.4782375	26	615	6059	failed (too many debug iterations)	0	0			None		
742654095402	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	resistor-substitution-advisor-copy1	An LLM-based system that suggests viable resistor substitutions using combinations of standard-value resistors.	25.917304183333332	1.1916266999999998	7	435	4080	completed	1	1	faithful	reject	Mathematical optimization outperformed LLM and simple methods for finding resistor combinations, achieving 100% accuracy.	In a comparison of three approaches for resistor combination optimization, the mathematical optimization method significantly outperformed both the LLM-based and simple baseline approaches, achieving 100% success rate within 1% tolerance compared to 3.4% for LLM and 15% for the simple baseline. Statistical analysis confirmed these differences were significant (p < 0.05), demonstrating that mathematical optimization is superior for this task despite higher computational costs.	LLM-based approaches can effectively compete with or outperform traditional mathematical methods for suggesting resistor combinations to match target resistance values.
922412943876	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	resistor-substitution-advisor-copy2	An LLM-based system that suggests viable resistor substitutions using combinations of standard-value resistors.	33.034484033333335	1.3878409	7	455	4418	completed	1	1	deviations	reject	Mathematical and simple baselines outperformed LLM approach in resistor combination optimization task.	In a comparison of LLM-based vs baseline approaches for resistor combination optimization, the mathematical (mean error 0.050%) and simple baseline (mean error 0.102%) methods significantly outperformed the LLM approach (mean error 15.15%, p < 0.05). Both baseline methods achieved 100% success rate at 1% tolerance, while the LLM approach achieved only 18.3% success rate.	An LLM-based approach can match or exceed the performance of traditional mathematical methods for finding optimal resistor combinations.
482800065382	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	resistor-substitution-advisor-copy3	An LLM-based system that suggests viable resistor substitutions using combinations of standard-value resistors.	52.2009838	2.2690154	11	459	4334	completed	1	1	faithful	reject	Mathematical optimization outperformed LLM and simple baseline approaches for resistor combination selection.	A comparison of LLM-based, simple baseline, and mathematical optimization approaches for resistor combination selection showed the mathematical approach achieved perfect accuracy (100% within 1% tolerance) but was slowest (3s), while the simple baseline was surprisingly effective (90% within 1%) and fastest (0.003s). The LLM approach performed poorly (21.7% within 1%) despite moderate computation time (1.3s).	An LLM-based approach can effectively suggest resistor combinations that match target resistance values, potentially offering advantages over mathematical baselines.
162187344828	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	resistor-substitution-advisor-copy4	An LLM-based system that suggests viable resistor substitutions using combinations of standard-value resistors.	48.82394178333333	1.57598565	8	418	3978	completed	1	1	deviations	reject	Simple baseline outperformed LLM and mathematical approaches in finding optimal resistor combinations, achieving 100% success rate.	Comparing three approaches for resistor combination optimization, the simple baseline method significantly outperformed both LLM and mathematical approaches, achieving 100% success rate at all tolerance levels with mean error of 0.044%. The LLM approach showed moderate performance (32.7% success at 1% tolerance), while the mathematical optimization approach performed poorly (0% success at 1% tolerance), suggesting that simple heuristic approaches may be more effective than complex optimization for this task.	LLM-based approaches can effectively compete with or outperform traditional mathematical and simple baseline approaches in finding optimal resistor combinations to match target resistance values.
905510309226	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	resistor-substitution-advisor-copy5	An LLM-based system that suggests viable resistor substitutions using combinations of standard-value resistors.	16.7824409	0.8423284500000001	5	390	3518	completed	1	1	faithful	reject	Simple baseline outperformed LLM and mathematical approaches in resistor combination task, with 100% success rate.	In a comparison of three resistor combination methods, the simple baseline approach significantly outperformed both LLM-based and mathematical optimization approaches, achieving 100% success within 5% tolerance (mean error 0.048%) versus 50% and 95% respectively. The LLM approach showed inconsistent performance with mean error of 14.59% and notably longer computation times (1.24s vs 0.025s for baseline).	An LLM-based approach can effectively suggest resistor combinations that match target resistance values, potentially outperforming traditional mathematical approaches.
297088171371	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	knowledge-guided-react-copy1	A ReAct agent that builds and uses a knowledge graph while exploring text-based games to improve decision making.	137.5903784333333	5.547337450000001	21	587	5144	completed	1	1	deviations	reject	Knowledge graph enhancement to ReAct agent did not improve performance on cooking tasks, showing worse success rate.	The experiment compared a baseline ReAct agent against a knowledge-graph-enhanced version on TextWorldExpress cooking tasks across 25 episodes. The knowledge graph agent performed worse (mean score 0.235, 4% success) than baseline (mean score 0.334, 16% success), with no significant difference (p=0.977). While the knowledge graph agent was more efficient in steps taken (8.56 vs 22.4), its lower success rate suggests the enhancement may have overconstrained the agent's behavior.	Enhancing a ReAct agent with a structured knowledge graph representation of the environment and recipe state will improve its performance on TextWorldExpress cooking tasks.
978083803352	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	knowledge-guided-react-copy2	A ReAct agent that builds and uses a knowledge graph while exploring text-based games to improve decision making.	119.92713981666667	6.438466150000002	26	561	5048	failed (too many debug iterations)	0	0			None		
572258285237	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	knowledge-guided-react-copy3	A ReAct agent that builds and uses a knowledge graph while exploring text-based games to improve decision making.	83.06583166666665	4.7886033	15	662	6267	failed (code parsing issue)	0	0			None		
134263741028	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	knowledge-guided-react-copy4	A ReAct agent that builds and uses a knowledge graph while exploring text-based games to improve decision making.	74.22666525	5.9855882000000005	26	701	6219	failed (too many debug iterations)	0	0			None		
52538566242	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	knowledge-guided-react-copy5	A ReAct agent that builds and uses a knowledge graph while exploring text-based games to improve decision making.	87.80550303333332	6.2538927	26	501	4629	failed (too many debug iterations)	0	0			None		
