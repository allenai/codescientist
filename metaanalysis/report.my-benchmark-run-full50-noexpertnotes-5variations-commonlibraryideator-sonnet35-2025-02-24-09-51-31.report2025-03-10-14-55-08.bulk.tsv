id	batch_name	experiment_name	original_idea	runtime_minutes	total_cost	num_iterations_run	code_num_lines	code_num_tokens	status	status_numerical	interesting_results	faithfullness_category	hypothesis_category	results_summary	summary_medium	hypothesis
862444975485	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simulation-confidence-analysis-copy1	Investigate LLM ability to assess confidence in state predictions and correlation with accuracy.	49.99882015	2.47294825	12	504	4294	completed	1	1	faithful	inconclusive	LLM shows overconfidence in state predictions, with weak correlation between confidence and actual accuracy.	In a TextWorldExpress CookingWorld experiment with 500 predictions, gpt-4o-mini showed high confidence (mean=76.19) but low accuracy (mean=0.297), with a weak positive correlation between confidence and accuracy (r=0.287, p<0.001). The results suggest significant model overconfidence, though the relationship between confidence and accuracy was statistically significant.	LLM confidence scores are meaningfully correlated with prediction accuracy in TextWorldExpress environments, providing reliable indicators of prediction quality.
121813262920	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simulation-confidence-analysis-copy2	Investigate LLM ability to assess confidence in state predictions and correlation with accuracy.	20.8694481	0.33401155	2	249	2282	completed	1	1	faithful	reject	LLM showed severe overconfidence in game state predictions, with 1.2% accuracy despite 81% mean confidence.	The experiment tested LLM confidence calibration in TextWorldExpress environments, analyzing 500 predictions across 20 episodes. Results showed severe overconfidence, with high mean confidence (81.03) despite very low accuracy (0.012), and only weak correlation between confidence and accuracy (r=0.043, p=0.331), suggesting poor calibration and potential fundamental issues with the LLM's ability to predict game states.	LLMs' confidence scores are meaningfully calibrated to their prediction accuracy in game-like environments, with higher confidence correlating with higher accuracy.
619139286928	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simulation-confidence-analysis-copy3	Investigate LLM ability to assess confidence in state predictions and correlation with accuracy.	58.44847406666667	3.4356509500000003	17	393	3423	completed	1	1	deviations	support	LLM shows significant overconfidence in game state predictions, with 41.6% accuracy despite 94.2% mean confidence.	Analysis of 500 LLM predictions in CookingWorld showed 41.6% accuracy despite 94.24% mean confidence, with a weak positive correlation (r=0.348, p<1e-15) between confidence and accuracy. The results demonstrate significant LLM overconfidence, with performance below random chance (bootstrap p=0.0).	LLM confidence scores are meaningful indicators of prediction accuracy in text-based game environments, with higher confidence correlating with higher accuracy.
973539893561	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simulation-confidence-analysis-copy4	Investigate LLM ability to assess confidence in state predictions and correlation with accuracy.	45.69555448333333	1.9353674000000003	9	394	3698	completed	1	1	faithful	support	LLM shows overconfidence in game state predictions, with 38.4% accuracy despite 89.05% mean confidence.	In a TextWorldExpress CookingWorld experiment with 500 predictions, gpt-4o-mini showed moderate accuracy (38.4%) despite high mean confidence (89.05%), with a weak positive correlation between confidence and accuracy (r=0.351). The results suggest the model exhibits overconfidence in its predictions, though the correlation was statistically reliable.	LLMs' confidence scores are meaningfully correlated with their prediction accuracy in game state prediction tasks.
352865338061	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simulation-confidence-analysis-copy5	Investigate LLM ability to assess confidence in state predictions and correlation with accuracy.	24.8559139	0.8970423500000001	5	323	2815	completed	1	1	faithful	reject	LLM shows poor calibration: high confidence (81.38) despite low accuracy (0.34) in CookingWorld predictions.	Analysis of 478 predictions from gpt-4o-mini in CookingWorld environments revealed no significant correlation between confidence and accuracy (r=-0.035, p=0.448, 95% CI: [-0.124, 0.054]). The model showed significant overconfidence, with mean confidence of 81.38 despite mean accuracy of only 0.34, suggesting poor calibration between self-assessed confidence and actual performance.	LLMs' confidence scores are meaningfully correlated with their prediction accuracy in text-based game environments.
418143278156	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-memory-pruning-copy1	Compare time-based versus frequency-based memory pruning strategies in a ScienceWorld agent.	105.96520038333333	6.3096771	26	309	2993	failed (too many debug iterations)	0	0			None		
414458715870	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-memory-pruning-copy2	Compare time-based versus frequency-based memory pruning strategies in a ScienceWorld agent.	121.73043058333333	6.530808700000001	26	554	5372	failed (too many debug iterations)	0	0			None		
279927542251	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-memory-pruning-copy3	Compare time-based versus frequency-based memory pruning strategies in a ScienceWorld agent.	204.78155438333334	6.0602114	26	397	3608	failed (too many debug iterations)	0	0			None		
963758363958	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-memory-pruning-copy4	Compare time-based versus frequency-based memory pruning strategies in a ScienceWorld agent.	161.54731845	5.85002265	26	546	5487	failed (too many debug iterations)	0	0			None		
947535000381	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-memory-pruning-copy5	Compare time-based versus frequency-based memory pruning strategies in a ScienceWorld agent.	138.42670265	6.734540999999998	26	400	3957	failed (too many debug iterations)	0	0			None		
676475113560	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-property-verification-copy1	System to verify basic physical properties of objects against ConceptNet knowledge in ScienceWorld	73.86690591666667	5.481204449999999	26	373	3061	failed (too many debug iterations)	0	0			None		
713081399418	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-property-verification-copy2	System to verify basic physical properties of objects against ConceptNet knowledge in ScienceWorld	41.35885633333333	3.8775696	15	738	6110	failed (code parsing issue)	0	0			None		
986748828488	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-property-verification-copy3	System to verify basic physical properties of objects against ConceptNet knowledge in ScienceWorld	72.27175175	5.927813550000001	26	440	3882	failed (too many debug iterations)	0	0			None		
258425315885	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-property-verification-copy4	System to verify basic physical properties of objects against ConceptNet knowledge in ScienceWorld	79.22011396666667	7.513189649999998	26	585	4844	failed (too many debug iterations)	0	0			None		
613729835055	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-property-verification-copy5	System to verify basic physical properties of objects against ConceptNet knowledge in ScienceWorld	74.27999774999999	5.9808399	26	577	4800	failed (too many debug iterations)	0	0			None		
224151753661	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	hierarchical-elimination-copy1	Create a hierarchical filtering system that eliminates irrelevant information at multiple levels of abstraction.	49.01462611666667	4.967016	26	481	4349	failed (too many debug iterations)	0	0			None		
994758823417	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	hierarchical-elimination-copy2	Create a hierarchical filtering system that eliminates irrelevant information at multiple levels of abstraction.	129.43808348333334	5.3535585999999995	26	577	5006	failed (too many debug iterations)	0	0			None		
208779833260	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	hierarchical-elimination-copy3	Create a hierarchical filtering system that eliminates irrelevant information at multiple levels of abstraction.	44.695222449999996	4.793300700000001	26	405	3798	failed (too many debug iterations)	0	0			None		
722432563042	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	hierarchical-elimination-copy4	Create a hierarchical filtering system that eliminates irrelevant information at multiple levels of abstraction.	45.74163815	4.73166735	26	442	3819	failed (too many debug iterations)	0	0			None		
645431763462	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	hierarchical-elimination-copy5	Create a hierarchical filtering system that eliminates irrelevant information at multiple levels of abstraction.	45.764697283333334	4.71638085	26	435	3920	failed (too many debug iterations)	0	0			None		
450547276903	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-graph-cooking-simulation-copy1	Test if a static cooking knowledge graph improves LLM action prediction in CookingWorld tasks.	87.38292753333333	7.36593175	26	679	5966	failed (too many debug iterations)	0	0			None		
747639999838	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-graph-cooking-simulation-copy2	Test if a static cooking knowledge graph improves LLM action prediction in CookingWorld tasks.	78.24390731666668	6.8103249	26	476	4027	failed (too many debug iterations)	0	0			None		
626809504205	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-graph-cooking-simulation-copy3	Test if a static cooking knowledge graph improves LLM action prediction in CookingWorld tasks.	89.15392448333334	6.14166365	26	523	4842	failed (too many debug iterations)	0	0			None		
631557577540	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-graph-cooking-simulation-copy4	Test if a static cooking knowledge graph improves LLM action prediction in CookingWorld tasks.	71.00012321666667	6.453478950000001	24	652	5898	failed (code parsing issue)	0	0			None		
397192913037	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-graph-cooking-simulation-copy5	Test if a static cooking knowledge graph improves LLM action prediction in CookingWorld tasks.	106.21869126666667	6.904444649999999	26	635	5863	failed (too many debug iterations)	0	0			None		
756642589416	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	llm-graph-verification-copy1	Evaluate if LLM-based verification improves knowledge graph accuracy in text-based games.	34.554779816666674	0.9160856999999999	5	199	1601	completed	1	1	deviations	inconclusive	LLM verification removed 31% of knowledge graph triples, but accuracy improvements remain unconfirmed.	LLM verification in CookingWorld identified and removed an average of 47.7 potentially incorrect knowledge graph triples per episode, reducing graph size by 31% (from 152.7 to 105 triples per episode). While this demonstrates the LLM's ability to filter triples, without ground truth validation we cannot confirm if accuracy actually improved.	LLM-based verification can improve the accuracy of automatically extracted knowledge graphs in text-based games.
71487851113	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	llm-graph-verification-copy2	Evaluate if LLM-based verification improves knowledge graph accuracy in text-based games.	25.68587135	1.0686701	6	274	2590	completed	1	0	faithful	inconclusive	LLM verification produced simpler knowledge graphs but differences were not statistically significant.	The experiment compared baseline vs LLM-verified knowledge graphs across 10 episodes in a cooking game environment, finding verified graphs had fewer nodes (2.09 vs 2.43) and edges (1.30 vs 1.51). However, bootstrap analysis with 10,000 resamples showed these differences were not statistically significant (p=1.0), suggesting LLM verification did not meaningfully improve knowledge graph accuracy.	LLM-based verification improves the accuracy of knowledge graphs extracted from text-based game environments.
309369580682	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	llm-graph-verification-copy3	Evaluate if LLM-based verification improves knowledge graph accuracy in text-based games.	49.350941283333334	1.35235475	7	354	2909	completed	1	1	deviations	reject	LLM verification of knowledge graphs in text games reduced graph size but didn't improve accuracy.	The experiment compared baseline vs. LLM-verified knowledge graphs in CookingWorld text games across 10 episodes of 30 steps each. LLM verification removed an average of 26.4 triples per episode, resulting in smaller graphs (baseline mean: 8.05 nodes vs. experimental mean: 7.11 nodes, p=1.0), suggesting that LLM verification did not improve graph accuracy.	LLM-based verification improves the accuracy of knowledge graphs extracted from text-based game states.
550981442879	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	llm-graph-verification-copy4	Evaluate if LLM-based verification improves knowledge graph accuracy in text-based games.	17.1683374	1.1518400500000001	6	360	3514	completed	1	1	deviations	reject	LLM verification of knowledge graphs did not improve game performance and was possibly too restrictive.	An experiment comparing baseline vs LLM-verified knowledge graphs in CookingWorld showed no performance benefit from verification (baseline: 0.061, experimental: 0.049, p=0.61). The LLM verification appeared overly conservative, retaining only 8 triples compared to 258 in baseline, suggesting the verification system may need refinement.	LLM-based verification improves the accuracy of knowledge graphs extracted from text-based game states.
33046327954	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	llm-graph-verification-copy5	Evaluate if LLM-based verification improves knowledge graph accuracy in text-based games.	25.78591755	1.3273016499999999	7	366	3114	completed	1	1	deviations	inconclusive	LLM verification filtered invalid knowledge graph triples but may have been overly strict in removals.	The experiment tested LLM-based verification of knowledge graph triples extracted from text game observations across 10 episodes. LLM verification removed an average of 11.6 invalid triples per episode, reducing graph size from 48.2 to 29.3 nodes on average, though this reduction suggests possible over-filtering rather than clear accuracy improvements.	LLM-based verification improves the accuracy of knowledge graphs extracted from text-based game observations.
265353415712	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-planning-agent-copy1	Create and evaluate a basic planning agent that can break down simple cooking tasks into 2-3 step sequences and execute them.	44.40025873333333	3.29740365	12	715	6042	failed (code parsing issue)	0	0			None		
316929019695	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-planning-agent-copy2	Create and evaluate a basic planning agent that can break down simple cooking tasks into 2-3 step sequences and execute them.	45.97763345	4.4524398000000005	15	713	6192	failed (code parsing issue)	0	0			None		
963406461575	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-planning-agent-copy3	Create and evaluate a basic planning agent that can break down simple cooking tasks into 2-3 step sequences and execute them.	52.722941983333335	5.721441949999999	26	476	4201	failed (too many debug iterations)	0	0			None		
262922729309	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-planning-agent-copy4	Create and evaluate a basic planning agent that can break down simple cooking tasks into 2-3 step sequences and execute them.	73.77570770000001	6.314926200000001	26	534	4833	completed	1	1	deviations	inconclusive	Planning agent performed similarly to ReAct baseline in CookingWorld tasks; both outperformed random baseline.	A planning-enhanced agent was compared to ReAct and random baselines across 10 CookingWorld episodes. The planning agent (mean=0.394) performed similarly to ReAct (mean=0.382), with no significant difference (p=0.463), while both substantially outperformed random (mean=0.156), suggesting planning did not provide clear benefits over the ReAct baseline.	Adding a planning component to a ReAct agent will improve task performance in CookingWorld environments compared to a standard ReAct agent.
83784526686	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-planning-agent-copy5	Create and evaluate a basic planning agent that can break down simple cooking tasks into 2-3 step sequences and execute them.	57.55121765	5.7271512	22	714	6161	failed (code parsing issue)	0	0			None		
972368467095	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	basic-confidence-simulation-copy1	Evaluate LLM ability to predict action success and assign meaningful confidence scores in a cooking game environment.	56.638035116666664	4.9703179	26	445	3977	failed (too many debug iterations)	0	0			None		
153722142099	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	basic-confidence-simulation-copy2	Evaluate LLM ability to predict action success and assign meaningful confidence scores in a cooking game environment.	39.57171183333333	1.4223165500000001	7	479	4366	completed	1	1	faithful	reject	LLM predicts game actions well (73% accuracy) but shows poor confidence calibration in predictions.	The LLM achieved 73% accuracy in predicting action outcomes in the CookingWorld environment, significantly outperforming random (58%, p=0.0096) and other baselines. However, the near-identical confidence levels for correct (0.782) and incorrect (0.780) predictions suggest that while the LLM can predict outcomes better than chance, it lacks meaningful confidence calibration.	LLMs can meaningfully predict their confidence in action outcomes in text-based game environments.
222008229554	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	basic-confidence-simulation-copy3	Evaluate LLM ability to predict action success and assign meaningful confidence scores in a cooking game environment.	22.993648133333334	0.48216790000000004	3	271	2252	completed	1	1	faithful	reject	LLM showed 69% accuracy but poor confidence calibration in predicting cooking game action outcomes.	The experiment tested LLM confidence prediction in a cooking game environment across 100 actions, comparing against random and constant baselines. Results showed 69% prediction accuracy but near-zero correlation (0.067) between confidence and accuracy, with the LLM showing consistent overconfidence (0.90 confidence for both correct and incorrect predictions).	LLMs can meaningfully predict their confidence in action outcomes in a text-based game environment, with higher confidence correlating with better accuracy.
420329656499	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	basic-confidence-simulation-copy4	Evaluate LLM ability to predict action success and assign meaningful confidence scores in a cooking game environment.	56.662966366666666	4.774578749999999	26	451	4009	failed (too many debug iterations)	0	0			None		
630748612275	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	basic-confidence-simulation-copy5	Evaluate LLM ability to predict action success and assign meaningful confidence scores in a cooking game environment.	28.508198166666666	0.69532815	4	339	2837	completed	1	1	faithful	reject	LLM showed good action prediction (74%) but poor confidence calibration in TextWorldExpress environment.	In a pilot study of 100 action predictions in TextWorldExpress's CookingWorld, an LLM achieved 74% accuracy in predicting action outcomes but showed poor confidence calibration (confidence-accuracy correlation r=0.144, average confidence 0.907 for correct vs 0.900 for incorrect predictions). These results suggest that while the LLM can predict outcomes above chance, it cannot meaningfully assess its prediction confidence.	LLMs can meaningfully predict their confidence in action outcomes in text-based game environments.
96565194059	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	wordnet-cooking-exploration-copy1	Using WordNet's food-related semantic hierarchies to guide exploration in cooking tasks.	14.314906266666666	0.8336889000000001	5	324	2863	completed	1	1	faithful	inconclusive	WordNet-guided agent performed marginally better than random, but neither solved any tasks successfully.	In a comparison of WordNet-guided vs random exploration in CookingWorld tasks (n=20 episodes each), the WordNet agent showed better but non-significant performance (score: 0.28 vs 0.21, p=0.11) and required fewer steps (32.0 vs 42.8). However, neither agent achieved any successful task completions, suggesting that semantic guidance alone is insufficient for solving these tasks.	Using WordNet-based semantic knowledge to identify food-related terms will improve exploration efficiency and task performance in cooking-related text environments compared to random exploration.
637189800289	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	wordnet-cooking-exploration-copy2	Using WordNet's food-related semantic hierarchies to guide exploration in cooking tasks.	68.31675488333333	6.52252785	26	600	5323	failed (too many debug iterations)	0	0			None		
392253573879	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	wordnet-cooking-exploration-copy3	Using WordNet's food-related semantic hierarchies to guide exploration in cooking tasks.	49.2122437	5.564321250000001	26	451	3969	failed (too many debug iterations)	0	0			None		
820156653554	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	wordnet-cooking-exploration-copy4	Using WordNet's food-related semantic hierarchies to guide exploration in cooking tasks.	51.55474441666667	5.866810950000001	26	509	4412	failed (too many debug iterations)	0	0			None		
680885159739	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	wordnet-cooking-exploration-copy5	Using WordNet's food-related semantic hierarchies to guide exploration in cooking tasks.	47.00663905	5.276154900000001	26	419	3688	failed (too many debug iterations)	0	0			None		
270983721927	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	reactive-graph-confidence-copy1	Study if explicit reasoning about confidence improves belief graph accuracy in CookingWorld.	93.56602153333333	1.86311435	8	421	3626	completed	1	0	deviations	reject	Confidence-based ReAct agent performed worse than baseline in belief graph building (50.8% vs 43.4% accuracy).	A comparison of baseline ReAct and confidence-based ReAct agents for belief graph building showed the baseline performing better (50.8% vs 43.4% accuracy), though not significantly (p=0.994, n=10 episodes). The confidence-based approach did not improve graph accuracy, possibly due to implementation limitations in confidence thresholding.	Explicitly reasoning about confidence in graph updates will improve the accuracy of belief graphs built by ReAct agents in CookingWorld environments.
621240987094	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	reactive-graph-confidence-copy2	Study if explicit reasoning about confidence improves belief graph accuracy in CookingWorld.	81.26069531666667	2.4725112500000006	11	419	3369	completed	1	1	deviations	inconclusive	Confidence-based graph updates showed no accuracy advantage over baseline, but led to more conservative updating.	A comparative study of confidence-based vs. standard ReAct agents for belief graph building showed no significant performance differences (p=1.0, means=0.0) across 739 updates in a 10-episode pilot. The confidence agent made 39% fewer updates (279 vs 460), suggesting more conservative behavior, though this did not translate to improved accuracy.	Explicitly reasoning about confidence in graph updates will lead to more accurate belief graphs compared to standard graph updates.
373614134384	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	reactive-graph-confidence-copy3	Study if explicit reasoning about confidence improves belief graph accuracy in CookingWorld.	89.93849406666666	6.346019399999999	26	493	4377	failed (too many debug iterations)	0	0			None		
332849553745	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	reactive-graph-confidence-copy4	Study if explicit reasoning about confidence improves belief graph accuracy in CookingWorld.	122.19243111666667	2.93247775	11	481	4655	completed	1	0	faithful	reject	Confidence-based belief graph building (67.0% accuracy) performed similarly to baseline approach (70.6%) in CookingWorld.	A comparison of confidence-based vs standard belief graph building in CookingWorld showed the confidence agent achieved 67.0% edge accuracy while the baseline achieved 70.6% accuracy across 10 episodes. Bootstrap analysis found no significant difference between approaches (p=1.0), suggesting explicit confidence reasoning did not improve graph building accuracy in this context.	Explicitly reasoning about confidence when building belief graphs from observations will lead to more accurate graph representations compared to direct updates without confidence reasoning.
582071109988	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	reactive-graph-confidence-copy5	Study if explicit reasoning about confidence improves belief graph accuracy in CookingWorld.	103.72880228333334	2.63434685	11	503	4912	completed	1	0	faithful	inconclusive	Confidence-based and standard ReAct agents performed similarly in belief graph building, with no significant differences.	A comparison of confidence-based vs standard ReAct agents for belief graph building showed nearly identical performance (confidence mean=0.090, baseline mean=0.090, p=0.559) across 10 episodes in CookingWorld. The results suggest that explicit confidence reasoning does not significantly improve graph accuracy, though the small sample size limits strong conclusions.	Explicitly reasoning about confidence in graph updates will improve the accuracy of belief graphs built by ReAct agents in CookingWorld environments.
431966530026	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-task-reflection-copy1	Study if providing agents with their past successful experiences improves reflection quality in cooking tasks	121.34311828333333	6.996929400000002	26	627	5935	failed (too many debug iterations)	0	0			None		
635620278969	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-task-reflection-copy2	Study if providing agents with their past successful experiences improves reflection quality in cooking tasks	97.89863253333333	7.748183700000002	26	691	6127	failed (code parsing issue)	0	0			None		
291309247331	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-task-reflection-copy3	Study if providing agents with their past successful experiences improves reflection quality in cooking tasks	97.31716150000001	5.802975300000001	26	370	3015	failed (too many debug iterations)	0	0			None		
60656888632	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-task-reflection-copy4	Study if providing agents with their past successful experiences improves reflection quality in cooking tasks	134.20723675	5.561389550000001	26	396	3674	failed (too many debug iterations)	0	0			None		
129540170674	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-task-reflection-copy5	Study if providing agents with their past successful experiences improves reflection quality in cooking tasks	374.44728241666667	3.46664015	10	605	5845	failed (hard experiment runtime limit reached)	0	0			None		
448382441218	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	cooking-graph-explorer-copy1	Develop an agent that builds and uses container-relationship knowledge graphs in CookingWorld environments.	85.11842751666667	6.6122479	26	596	5564	failed (too many debug iterations)	0	0			None		
949737806997	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	cooking-graph-explorer-copy2	Develop an agent that builds and uses container-relationship knowledge graphs in CookingWorld environments.	99.58942071666667	6.320450699999999	26	509	4525	failed (too many debug iterations)	0	0			None		
705051325326	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	cooking-graph-explorer-copy3	Develop an agent that builds and uses container-relationship knowledge graphs in CookingWorld environments.	83.64267208333334	6.227724000000001	26	465	4109	failed (too many debug iterations)	0	0			None		
944825002163	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	cooking-graph-explorer-copy4	Develop an agent that builds and uses container-relationship knowledge graphs in CookingWorld environments.	76.36635493333333	6.08492775	26	520	4344	failed (too many debug iterations)	0	0			None		
589151480727	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	cooking-graph-explorer-copy5	Develop an agent that builds and uses container-relationship knowledge graphs in CookingWorld environments.	126.15962838333333	7.5624023000000005	26	624	5665	failed (too many debug iterations)	0	0			None		
174851915111	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-template-discovery-copy1	System for identifying common action patterns in successful TextWorldExpress CookingWorld trajectories.	46.04950916666667	5.332607250000001	26	568	4538	failed (too many debug iterations)	0	0			None		
798671848071	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-template-discovery-copy2	System for identifying common action patterns in successful TextWorldExpress CookingWorld trajectories.	42.91887273333334	4.66948125	26	415	3342	failed (too many debug iterations)	0	0			None		
700936968917	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-template-discovery-copy3	System for identifying common action patterns in successful TextWorldExpress CookingWorld trajectories.	91.7117106	5.271039	26	421	3603	failed (too many debug iterations)	0	0			None		
602855220958	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-template-discovery-copy4	System for identifying common action patterns in successful TextWorldExpress CookingWorld trajectories.	39.89750155	4.398076199999999	26	353	2788	failed (too many debug iterations)	0	0			None		
913248765707	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-template-discovery-copy5	System for identifying common action patterns in successful TextWorldExpress CookingWorld trajectories.	55.229894449999996	5.889176100000001	26	446	3895	failed (too many debug iterations)	0	0			None		
91024251008	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	react-knowledge-retrieval-copy1	Evaluate if adding ConceptNet knowledge retrieval to a ReAct agent improves performance on simple text-based tasks.	417.2118877166667	7.17900175	25	697	6184	failed (hard experiment runtime limit reached)	0	0			None		
404231316820	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	react-knowledge-retrieval-copy2	Evaluate if adding ConceptNet knowledge retrieval to a ReAct agent improves performance on simple text-based tasks.	291.8604536166667	4.107474900000001	16	442	4198	completed	1	1	faithful	reject	Knowledge-augmented ReAct agent showed no significant improvement over baseline on common sense tasks.	A comparison of standard and knowledge-augmented ReAct agents on TextWorldExpress common sense tasks showed both agents performing well (baseline: 0.95 score, 80% success; experimental: 0.95 score, 90% success) with no statistically significant difference (bootstrap p=0.5502). The results suggest that augmenting the ReAct agent with ConceptNet knowledge did not provide significant advantages over the base agent's already strong common sense reasoning capabilities.	Augmenting a ReAct agent with external common sense knowledge (ConceptNet) will improve its performance on common sense reasoning tasks compared to a standard ReAct agent.
877150529587	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	react-knowledge-retrieval-copy3	Evaluate if adding ConceptNet knowledge retrieval to a ReAct agent improves performance on simple text-based tasks.	147.0944399	7.403062800000001	26	469	4394	failed (too many debug iterations)	0	0			None		
966240124957	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	react-knowledge-retrieval-copy4	Evaluate if adding ConceptNet knowledge retrieval to a ReAct agent improves performance on simple text-based tasks.	371.54507363333335	4.71628875	17	531	4743	failed (hard experiment runtime limit reached)	0	0			None		
82446856425	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	react-knowledge-retrieval-copy5	Evaluate if adding ConceptNet knowledge retrieval to a ReAct agent improves performance on simple text-based tasks.	151.38740496666665	7.26401905	26	430	3931	failed (too many debug iterations)	0	0			None		
819212579636	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	progressive-state-complexity-copy1	Study how increasing state representation complexity affects LLM simulation accuracy in text-based games.	622.23323475	0.95312555	4	348	2887	completed	1	1	faithful	support	LLM prediction accuracy decreases with state complexity, except for improved performance with relational features.	The experiment tested LLM prediction accuracy across four complexity levels in CookingWorld, finding that boolean-only states achieved highest accuracy (89.6%), while full complexity performed worst (61.9%). Bootstrap analysis showed significant differences between complexity levels (p < 0.05), with an unexpected improvement in relational (77.6%) over numerical (67.9%) complexity.	Increased state complexity in TextWorldExpress environments leads to decreased LLM prediction accuracy.
905239919718	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	progressive-state-complexity-copy2	Study how increasing state representation complexity affects LLM simulation accuracy in text-based games.	35.84781723333333	1.9835589500000002	10	393	3580	completed	1	1	faithful	support	LLM prediction accuracy improves with state complexity up to relational level, then slightly decreases with full state.	The experiment tested LLM prediction accuracy across four complexity levels in TextWorldExpress, finding significant improvements from boolean (20%) to numerical (50.3%) to relational (88.3%) representations, with full state (70.7%) performing slightly worse than relational. Bootstrap analyses confirmed significant differences between adjacent complexity levels (p<0.005) except between relational and full states.	Increasing state complexity in environment representations will lead to improved LLM prediction accuracy in TextWorldExpress environments.
952869129509	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	progressive-state-complexity-copy3	Study how increasing state representation complexity affects LLM simulation accuracy in text-based games.	41.16674055	0.6779798	4	292	2518	completed	1	1	deviations	support	LLM prediction accuracy decreases significantly as state complexity increases, from 93.2% (boolean) to 54.1% (full).	The experiment tested LLM prediction accuracy across four levels of state complexity in CookingWorld environments. Results showed monotonically decreasing accuracy with increasing complexity: boolean (93.2%), numerical (90.6%), relational (64.7%), and full state complexity (54.1%), with all differences between adjacent levels being statistically significant (p < 0.05). This demonstrates that LLM prediction accuracy significantly degrades with increasing state complexity.	Increasing state complexity in environment descriptions will negatively impact LLM prediction accuracy in TextWorldExpress environments.
661784663743	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	progressive-state-complexity-copy4	Study how increasing state representation complexity affects LLM simulation accuracy in text-based games.	137.91527228333334	1.40931245	7	389	3341	completed	1	1	faithful	reject	LLM prediction accuracy was highest with full state descriptions (76.1%), contrary to expectations of simpler states performing better.	The experiment tested LLM prediction accuracy across four complexity levels in CookingWorld environments, finding accuracies of 71.9% (boolean), 69.8% (numerical), 44.3% (relational), and 76.1% (full state). Statistical analysis showed significant improvement from relational to full state (p<0.001), suggesting that complete state information, rather than simplified representations, leads to better predictions.	Increased state complexity will negatively affect LLM prediction accuracy in TextWorldExpress environments.
382678254001	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	progressive-state-complexity-copy5	Study how increasing state representation complexity affects LLM simulation accuracy in text-based games.	75.33113341666667	0.8162945	4	286	2370	completed	1	1	faithful	support	LLM prediction accuracy decreases significantly as environment state complexity increases, from 90.3% to 56.5%.	The experiment tested LLM prediction accuracy across four levels of state complexity in TextWorldExpress, finding that accuracy decreased significantly from boolean-only (90.3%) to numerical (76.5%) to full state (56.5%) representations. Bootstrap analysis confirmed statistical significance of the differences (p=1.0), strongly supporting the hypothesis that increased state complexity impairs LLM prediction accuracy.	Increased state complexity in environment representations negatively affects LLM prediction accuracy.
133884568533	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	two-stage-game-generation-copy1	Comparing single-stage versus two-stage text game generation approaches	11.927372116666668	0.5504312	4	337	3005	completed	1	0	faithful	inconclusive	Single-stage and two-stage game generation approaches both achieved perfect success rates across all metrics.	In a comparison of single-stage vs two-stage text game generation (n=5 games per condition), both approaches achieved 100% success rates across all evaluation metrics including execution success and presence of required game mechanics. While the experiment was well-implemented, the identical perfect performance across conditions suggests the evaluation framework may need more sensitive metrics to detect meaningful differences.	Two-stage game generation (separating basic mechanics from scoring/win conditions) will produce more reliable and complete text games compared to single-stage generation.
368745113254	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	two-stage-game-generation-copy2	Comparing single-stage versus two-stage text game generation approaches	13.017696466666667	0.52170205	4	261	2507	completed	1	0	faithful	inconclusive	Single-stage and two-stage game generation approaches performed equally well, both achieving perfect success rates.	A comparative experiment between single-stage and two-stage text game generation approaches was conducted with 5 games per condition. Both approaches achieved 100% success rates across all metrics (execution success, presence of required mechanics), with no observable differences between the approaches in the pilot study.	Breaking down text game generation into two stages (basic mechanics first, then scoring/win conditions) will produce more reliable and higher quality code compared to single-stage generation.
918929073267	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	two-stage-game-generation-copy3	Comparing single-stage versus two-stage text game generation approaches	13.508018600000002	0.5043931	4	263	2317	completed	1	0	faithful	inconclusive	Single-stage and two-stage game generation methods both achieved perfect success rates across all metrics.	A comparative experiment between single-stage and two-stage text game generation approaches was conducted with 5 games per condition. Both approaches achieved 100% success rates across all measured metrics (execution, movement, inventory, scoring, and win conditions), with no observable differences between the methods.	Two-stage game generation (separating basic mechanics from scoring/win conditions) will produce more reliable and complete game implementations compared to single-stage generation.
235313993153	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	two-stage-game-generation-copy4	Comparing single-stage versus two-stage text game generation approaches	8.784315283333333	0.36259389999999997	3	261	2213	completed	1	0	faithful	support	Two-stage game generation showed slightly better consistency than single-stage, but both performed nearly perfectly.	In a comparison of single-stage vs. two-stage game generation (n=5 games per condition), both approaches achieved 100% execution success with no syntax errors. The two-stage approach implemented all required mechanics in all games (100%), while the single-stage approach missed inventory mechanics in 1/5 games (90% overall mechanics implementation), suggesting marginally better consistency with the two-stage approach.	Two-stage generation of text games will produce more reliable and complete implementations than single-stage generation.
537524286158	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	two-stage-game-generation-copy5	Comparing single-stage versus two-stage text game generation approaches	10.263408483333334	0.40963689999999997	3	249	2398	completed	1	0	faithful	inconclusive	Single-stage and two-stage game generation approaches performed equally well, both achieving 100% success rates.	A comparative experiment between single-stage and two-stage text game generation approaches was conducted with 5 games per condition. Both approaches achieved 100% success rates across all metrics (execution success, required mechanics), with no observable differences between the methods, suggesting that for basic game generation tasks, both approaches are equally reliable.	Two-stage generation of text adventure games (separating basic mechanics from scoring/win-conditions) will produce more reliable and higher quality code compared to single-stage generation.
179803492274	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	two-level-cooking-planner-copy1	Create a two-level planner combining recipe planning with action execution for cooking tasks in TextWorldExpress.	225.11200345	6.3877489	26	641	5650	failed (too many debug iterations)	0	0			None		
185556811994	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	two-level-cooking-planner-copy2	Create a two-level planner combining recipe planning with action execution for cooking tasks in TextWorldExpress.	69.48465238333333	6.2867546999999995	21	654	6188	failed (code parsing issue)	0	0			None		
118182870451	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	two-level-cooking-planner-copy3	Create a two-level planner combining recipe planning with action execution for cooking tasks in TextWorldExpress.	67.83619633333333	6.3464476	26	451	3483	failed (too many debug iterations)	0	0			None		
495158121306	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	two-level-cooking-planner-copy4	Create a two-level planner combining recipe planning with action execution for cooking tasks in TextWorldExpress.	173.16899715	4.6033098	15	704	6178	failed (code parsing issue)	0	0			None		
872173267686	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	two-level-cooking-planner-copy5	Create a two-level planner combining recipe planning with action execution for cooking tasks in TextWorldExpress.	83.04059531666667	5.471230449999998	23	456	4188	completed	1	1	deviations	support	Two-level planner significantly outperformed random baseline on cooking tasks (0.346 vs 0.140 mean score).	A two-level planner was compared against a random baseline on TextWorldExpress cooking tasks across 25 episodes, with the planner achieving significantly higher mean scores (0.346 vs 0.140, p < 0.001). While the results demonstrate clear benefits of planning over random actions, the omission of a single-level ReAct baseline prevents conclusions about the specific advantages of hierarchical planning.	A two-level planning approach, combining high-level task decomposition with low-level reactive execution, will perform better than simpler approaches on TextWorldExpress cooking tasks.
726815969452	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-goal-explorer-copy1	Develop an agent that tracks predefined cooking-related goal hypotheses during game exploration.	81.15847765	5.953585650000002	26	537	5022	failed (too many debug iterations)	0	0			None		
200817757601	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-goal-explorer-copy2	Develop an agent that tracks predefined cooking-related goal hypotheses during game exploration.	81.2002444	5.9890149500000005	26	609	5829	failed (too many debug iterations)	0	0			None		
922278728410	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-goal-explorer-copy3	Develop an agent that tracks predefined cooking-related goal hypotheses during game exploration.	28.613593883333333	1.91959245	9	459	4095	completed	1	1	faithful	support	Goal-tracking agent outperformed random baseline in cooking games (0.183 vs 0.113, p=0.0247).	A goal-tracking agent using LLM-based confidence scoring was compared against a random baseline in TextWorldExpress cooking games, with 3 games and 5 episodes each. The goal-tracking agent achieved significantly better performance (mean=0.183) compared to random (mean=0.113, bootstrap p-value=0.0247), demonstrating the effectiveness of structured goal tracking in cooking tasks.	A goal-tracking agent using LLM-based confidence scoring will perform better than a random baseline in TextWorldExpress cooking games by maintaining and pursuing explicit cooking-related goals.
472960446238	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-goal-explorer-copy4	Develop an agent that tracks predefined cooking-related goal hypotheses during game exploration.	93.24184045	7.287144799999999	26	622	5782	failed (too many debug iterations)	0	0			None		
569868755664	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-goal-explorer-copy5	Develop an agent that tracks predefined cooking-related goal hypotheses during game exploration.	57.798315083333335	1.7994864	8	552	4574	completed	1	1	faithful	support	Goal-tracking agent outperforms random baseline in cooking games (0.36 vs 0.103 score, p<0.001).	A goal-tracking agent using GPT-4-mini was compared against a random baseline in TextWorldExpress cooking games across 15 episodes (3 games × 5 episodes). The goal-tracking agent significantly outperformed random behavior (0.36 vs 0.103 mean score, p < 0.001), demonstrating the effectiveness of goal-tracking in cooking task completion.	A goal-tracking agent using LLM-based confidence scoring will perform better than random action selection in TextWorldExpress cooking games.
75961913807	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-graph-alignment-copy1	Compare different similarity metrics for aligning text descriptions with graph representations of cooking game states.	19.17572991666667	0.86807475	5	449	3806	completed	1	1	deviations	support	Word overlap outperformed Jaccard and custom metrics for text-graph alignment, achieving 62% accuracy in cooking games.	In a comparison of three text-graph alignment metrics in TextWorldExpress cooking games, the simple word overlap metric significantly outperformed both Jaccard similarity and a custom graph-based metric, achieving 62% accuracy compared to 1.4% and 2.7% respectively (p < 0.001 in bootstrap tests with 10,000 resamples). While meeting the pilot success criteria of >50% accuracy, the results suggest that simple lexical matching may be more effective than the implemented graph-structural approaches for this task.	Different similarity metrics (Jaccard, word overlap, and custom graph-text) will show varying effectiveness in aligning textual observations with graph representations in TextWorldExpress cooking games, with more sophisticated graph-aware metrics potentially performing better.
95100182943	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-graph-alignment-copy2	Compare different similarity metrics for aligning text descriptions with graph representations of cooking game states.	107.29314488333333	6.201381199999999	26	619	5366	failed (too many debug iterations)	0	0			None		
692544676574	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-graph-alignment-copy3	Compare different similarity metrics for aligning text descriptions with graph representations of cooking game states.	253.22346048333333	5.937766500000001	26	619	5262	failed (too many debug iterations)	0	0			None		
779705271268	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-graph-alignment-copy4	Compare different similarity metrics for aligning text descriptions with graph representations of cooking game states.	77.2286612	4.48474215	21	494	3882	completed	1	0	deviations	inconclusive	Three text-graph similarity metrics achieved 56% matching accuracy in TextWorldExpress cooking game states.	An experiment comparing three text-graph similarity metrics (Jaccard, word overlap, and custom) in TextWorldExpress cooking games achieved 56% accuracy across all metrics in matching 50 text-graph pairs. While meeting the >50% PILOT success criterion, the identical performance across metrics suggests potential evaluation methodology issues.	Different similarity metrics (Jaccard, word overlap, and custom graph-text) will show varying effectiveness in aligning textual observations with graph representations of game states in TextWorldExpress cooking games.
270443525244	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-graph-alignment-copy5	Compare different similarity metrics for aligning text descriptions with graph representations of cooking game states.	448.82794470000005	4.1920506	20	466	4065	failed (hard experiment runtime limit reached)	0	0			None		
646690204538	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	location-graph-cooking-copy1	Using location-tracking graphs to improve efficiency in TextWorldExpress cooking games.	80.99018643333334	6.579333750000001	26	653	5531	failed (too many debug iterations)	0	0			None		
948342480104	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	location-graph-cooking-copy2	Using location-tracking graphs to improve efficiency in TextWorldExpress cooking games.	83.14475949999999	6.458908949999999	26	627	5116	failed (too many debug iterations)	0	0			None		
953679011205	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	location-graph-cooking-copy3	Using location-tracking graphs to improve efficiency in TextWorldExpress cooking games.	74.31110256666666	6.337106900000001	26	628	5241	failed (too many debug iterations)	0	0			None		
895708567794	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	location-graph-cooking-copy4	Using location-tracking graphs to improve efficiency in TextWorldExpress cooking games.	101.77215221666667	7.341529750000001	26	720	6196	failed (too many debug iterations)	0	0			None		
696074322377	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	location-graph-cooking-copy5	Using location-tracking graphs to improve efficiency in TextWorldExpress cooking games.	108.10043688333333	4.4071657	17	580	5253	completed	1	1	deviations	inconclusive	Location-tracking ReAct agent performed slightly better than baseline in cooking tasks but not significantly.	A comparison of baseline vs location-tracking enhanced ReAct agents in TextWorldExpress cooking tasks showed marginally better performance for the enhanced agent (mean score 0.508 vs 0.471), though not statistically significant (p=0.278) in a 10-episode pilot. The enhanced agent demonstrated better spatial navigation but both agents struggled with inventory constraints.	Adding location tracking capabilities to a ReAct agent will improve its performance in TextWorldExpress cooking games by enabling better navigation and planning.
549971431395	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-graph-state-tracking-copy1	Create an agent that uses simple graph-based state tracking to improve planning in CookingWorld environments.	90.67434899999999	3.7650112000000004	14	598	5391	completed	1	1	deviations	inconclusive	Graph-tracking ReAct agent outperformed baseline (30% vs 0% success), but difference not significant with n=10.	A comparison of baseline vs graph-tracking ReAct agents (n=10 episodes each) showed the graph-tracking agent achieved better average performance (0.537 vs 0.311) and success rate (30% vs 0%), though this difference was not statistically significant (p=0.151). The graph-tracking agent also required fewer steps on average (14.8 vs 20.0), suggesting that structured world state representation may improve task efficiency.	Maintaining a structured representation of world state through graph tracking will improve a ReAct agent's ability to complete complex sequential tasks compared to using only observation history.
788057884070	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-graph-state-tracking-copy2	Create an agent that uses simple graph-based state tracking to improve planning in CookingWorld environments.	61.7655693	4.2170226	15	674	6354	failed (code parsing issue)	0	0			None		
904103338688	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-graph-state-tracking-copy3	Create an agent that uses simple graph-based state tracking to improve planning in CookingWorld environments.	51.191428200000004	3.8486644500000002	14	694	6073	failed (code parsing issue)	0	0			None		
122752459739	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-graph-state-tracking-copy4	Create an agent that uses simple graph-based state tracking to improve planning in CookingWorld environments.	60.805311833333334	5.8318477500000006	26	587	5140	failed (too many debug iterations)	0	0			None		
730531600127	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-graph-state-tracking-copy5	Create an agent that uses simple graph-based state tracking to improve planning in CookingWorld environments.	57.626277433333335	4.3062968999999995	15	739	6431	failed (code parsing issue)	0	0			None		
184170135985	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	knowledge-graph-verification-copy1	Using an interactive agent to verify automatically constructed knowledge graphs through environment exploration and interaction.	58.556248833333335	5.7063334	26	419	3532	failed (too many debug iterations)	0	0			None		
520833429331	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	knowledge-graph-verification-copy2	Using an interactive agent to verify automatically constructed knowledge graphs through environment exploration and interaction.	50.004137300000004	5.198207549999999	26	517	4752	failed (too many debug iterations)	0	0			None		
170533239161	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	knowledge-graph-verification-copy3	Using an interactive agent to verify automatically constructed knowledge graphs through environment exploration and interaction.	220.60505618333335	7.1751418000000005	26	448	3825	failed (too many debug iterations)	0	0			None		
980034375074	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	knowledge-graph-verification-copy4	Using an interactive agent to verify automatically constructed knowledge graphs through environment exploration and interaction.	180.35873576666665	5.425037400000001	24	540	4780	completed	1	1	faithful	support	ReAct-based agent outperforms random baseline in verifying knowledge graph triples in text environments.	A ReAct-based verification agent was tested against a random baseline for verifying knowledge graph triples in TextWorldExpress environments across 10 episodes. The verification agent significantly outperformed the random baseline (mean accuracy 0.789 vs 0.514, p < 0.001), demonstrating effective systematic exploration and reasoning for triple verification.	A ReAct-based verification agent can more effectively verify knowledge graph triples in text-based environments compared to random action selection.
724179332984	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	knowledge-graph-verification-copy5	Using an interactive agent to verify automatically constructed knowledge graphs through environment exploration and interaction.	98.38988135	3.4977943499999995	15	572	4968	completed	1	1	deviations	support	ReAct-based verification agent outperforms random baseline in verifying knowledge graph triples (91% vs 54%).	A ReAct-based verification agent was compared to random exploration for verifying knowledge graph triples in TextWorldExpress environments. The verification agent significantly outperformed the random baseline (91% vs 54% verification rate, p<0.001) and required fewer steps (5.7 vs 31.2 average steps), while maintaining 80% accuracy in distinguishing correct from incorrect triples.	A ReAct-based agent using structured exploration and reasoning will be more effective at verifying knowledge graph triples in text environments compared to random exploration.
116835969090	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-social-graphs-copy1	Test whether simple relationship tracking using knowledge graphs improves agent social decision making.	33.363834966666666	1.492347	8	339	3286	completed	1	1	faithful	reject	Relationship graph tracking did not improve agent social decisions; led to worse performance than simpler baselines.	The experiment compared social decision-making between agents with and without relationship graph tracking across 5 scenarios with 5 episodes each. The experimental agent using relationship tracking performed worse (mean=7.96) than both baseline agents (mean=9.0 and 8.84), with bootstrap analysis showing no benefit (p=1.0, n=25). Results suggest relationship tracking may lead to overly conservative social decisions.	Simple relationship tracking using knowledge graphs improves agent social decision making by allowing agents to account for relationship dynamics.
775832629991	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-social-graphs-copy2	Test whether simple relationship tracking using knowledge graphs improves agent social decision making.	9.406584166666667	0.5039471	3	298	2811	completed	1	1	deviations	support	Agents using relationship graphs made better social decisions than those without relationship tracking.	An experiment comparing three agent types across 5 social scenarios found that relationship graph-based agents (mean score 7.68) significantly outperformed baseline (5.56) and static (4.8) agents in making appropriate social decisions. The experimental agent showed particular strength in handling scenarios with conflicting relationships, demonstrating that explicit relationship tracking improves social decision-making.	Simple relationship tracking using knowledge graphs improves agent social decision making.
988368294357	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-social-graphs-copy3	Test whether simple relationship tracking using knowledge graphs improves agent social decision making.	44.56530475	0.5589229	3	268	2594	completed	1	1	faithful	support	Relationship graph tracking improved agent social decisions, achieving 7.2/10 average appropriateness score.	The experiment tested relationship graph-based social decision making across 5 scenarios with 5 episodes each. The experimental agent using relationship tracking achieved a mean score of 7.232/10, demonstrating improved performance over baseline agents without relationship tracking. The results suggest that explicit relationship modeling helps agents make more appropriate social decisions.	Simple relationship tracking using knowledge graphs improves agent social decision making compared to agents without relationship tracking.
502207667732	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-social-graphs-copy4	Test whether simple relationship tracking using knowledge graphs improves agent social decision making.	97.93294845	1.23636595	5	350	3131	completed	1	1	deviations	support	Relationship-aware agents made more socially appropriate decisions than baselines, supporting the value of knowledge graphs.	The experiment compared relationship-aware agents using knowledge graphs against baseline agents on social decision tasks. The experimental agent consistently achieved higher social dynamics scores (8-9 vs 4-5) by making relationship-informed decisions, particularly in handling conflicting relationships. However, the implementation was limited to PILOT mode and relied primarily on LLM-based evaluation.	Simple relationship tracking using knowledge graphs improves agent social decision making.
696173824715	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-social-graphs-copy5	Test whether simple relationship tracking using knowledge graphs improves agent social decision making.	263.84879313333334	1.7744129000000002	6	406	3970	completed	1	1	faithful	inconclusive	Graph-based relationship tracking improved social decisions over baseline but underperformed assuming friendly relationships.	The experiment compared three agent types (graph-based experimental, baseline, and static friendly baseline) on social decision making across 5 scenarios with 5 episodes each. The experimental agent (mean=7.992) significantly outperformed the baseline (mean=7.604, p=0.0) but performed worse than the static baseline (mean=9.072, p=1.0), suggesting that simple graph-based relationship tracking may not improve upon assuming friendly relationships.	Simple relationship tracking using knowledge graphs improves agent social decision making compared to not tracking relationships.
829607985316	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	failure-pattern-learning-copy1	Agent that learns to recognize and avoid common failure patterns in text-based games.	130.3910734	1.23002515	5	311	3013	completed	1	1	faithful	support	Failure-tracking ReAct agent outperforms baseline in TWC games, achieving 7x higher scores with significant results.	A failure-pattern-tracking ReAct agent was compared against a baseline ReAct agent across 25 episodes in TWC games. The experimental agent significantly outperformed the baseline (mean scores: 0.41 vs 0.058, p < 0.001), completing 3 tasks while the baseline completed none, demonstrating that tracking and avoiding past failures improves performance in commonsense reasoning tasks.	Tracking and avoiding patterns of failed actions will improve a ReAct agent's performance in TextWorld Commonsense games by preventing repetition of unsuccessful strategies.
672699823247	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	failure-pattern-learning-copy2	Agent that learns to recognize and avoid common failure patterns in text-based games.	49.71915406666667	4.649902099999999	26	380	3168	failed (too many debug iterations)	0	0			None		
800109188020	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	failure-pattern-learning-copy3	Agent that learns to recognize and avoid common failure patterns in text-based games.	154.897961	6.363245	26	480	4188	failed (too many debug iterations)	0	0			None		
785899289244	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	failure-pattern-learning-copy4	Agent that learns to recognize and avoid common failure patterns in text-based games.	395.3807412166667	2.93512365	13	451	4369	failed (hard experiment runtime limit reached)	0	0			None		
812565744094	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	failure-pattern-learning-copy5	Agent that learns to recognize and avoid common failure patterns in text-based games.	91.05276643333333	3.5024166500000002	17	34	244	completed	1	1	deviations	reject	Failure-pattern-tracking did not improve ReAct agent performance on TextWorld Commonsense games.	A comparison of baseline and failure-pattern-tracking ReAct agents on TWC games showed the experimental agent performed worse (mean score 0.34 vs 0.46) and took slightly fewer steps (25.32 vs 27.4), though neither difference was statistically significant (p>0.90). The results suggest that simple failure pattern tracking does not improve ReAct agent performance on TWC tasks.	Tracking and avoiding previously failed action patterns will improve a ReAct agent's performance on TextWorld Commonsense games by reducing repeated mistakes.
159287836897	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-task-composition-copy1	Evaluating two-step task composition learning in CookingWorld using LLMs.	44.75061576666667	4.6306165	26	365	3188	failed (too many debug iterations)	0	0			None		
68919090063	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-task-composition-copy2	Evaluating two-step task composition learning in CookingWorld using LLMs.	42.25274983333334	1.85981	10	390	3346	completed	1	1	deviations	support	Task decomposition improved LLM performance on cooking tasks but didn't enable full task completion.	In a comparison of task decomposition versus end-to-end approaches for cooking tasks, the decomposition agent achieved significantly higher scores (0.178 vs 0.0, p<0.001) than the end-to-end agent across 20 episodes. However, neither agent achieved full task success, suggesting decomposition helps but doesn't fully solve the challenge.	Explicit task decomposition improves LLM performance on multi-step tasks in CookingWorld compared to end-to-end approaches.
324177767650	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-task-composition-copy3	Evaluating two-step task composition learning in CookingWorld using LLMs.	175.23732668333332	5.970724000000001	26	566	4758	failed (too many debug iterations)	0	0			None		
983016540189	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-task-composition-copy4	Evaluating two-step task composition learning in CookingWorld using LLMs.	86.76481126666665	6.82618365	26	704	5971	failed (too many debug iterations)	0	0			None		
854276605924	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-task-composition-copy5	Evaluating two-step task composition learning in CookingWorld using LLMs.	191.45701693333334	6.0087341499999996	26	423	3873	failed (too many debug iterations)	0	0			None		
577808209948	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	basic-knowledge-sharing-copy1	Study the effectiveness of basic knowledge sharing between two ReAct agents using a shared graph structure.	272.20581755	2.2887578000000004	11	394	3531	completed	1	1	deviations	inconclusive	Knowledge sharing between ReAct agents showed mixed benefits, varying by task type and collaboration mode.	A knowledge-sharing experiment between ReAct agents across three task types showed mixed results, with an overall 88.9% task completion rate and average of 8.02 steps per episode. While shared knowledge and independent agents outperformed baselines in sequence tasks (3.8-4.0 vs 25.0 steps), baseline agents performed better in goal-state tasks (5.8 vs 7.6-9.0 steps), suggesting task-dependent benefits of agent collaboration.	Knowledge sharing between ReAct agents improves task performance compared to single agents or independent agents.
588775897093	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	basic-knowledge-sharing-copy2	Study the effectiveness of basic knowledge sharing between two ReAct agents using a shared graph structure.	331.82982146666666	6.598607050000001	26	523	4622	failed (too many debug iterations)	0	0			None		
389214523208	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	basic-knowledge-sharing-copy3	Study the effectiveness of basic knowledge sharing between two ReAct agents using a shared graph structure.	97.15242841666667	3.36904465	16	432	4163	completed	1	0	deviations	reject	Knowledge sharing between ReAct agents didn't improve performance across three simple task types.	A pilot study (5 episodes/condition) compared ReAct agents with and without knowledge sharing across three task types. While all conditions achieved 100% success, shared knowledge didn't improve efficiency (e.g., sequence task: baseline=4.6 steps, shared=5.8 steps), with no statistical differences between conditions (p=1.0).	Sharing knowledge through a graph structure will improve task performance and efficiency of ReAct agents compared to independent or single agents.
325336531627	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	basic-knowledge-sharing-copy4	Study the effectiveness of basic knowledge sharing between two ReAct agents using a shared graph structure.	69.13593725000001	2.0342823500000002	10	413	3918	completed	1	1	deviations	support	Knowledge sharing between ReAct agents significantly improved task performance, especially in goal-state tasks.	A knowledge-sharing experiment between ReAct agents across three task types showed significant performance improvements when using shared knowledge graphs. Most notably, in the goal-state task, shared-knowledge agents achieved 100% success rate with 3.4 average steps, compared to baseline's 20% success rate and 20.8 steps, while finder tasks showed similar efficiency gains (1.2 vs 3.6 steps).	Knowledge sharing between ReAct agents improves task performance compared to independent or single-agent approaches.
644517944670	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	basic-knowledge-sharing-copy5	Study the effectiveness of basic knowledge sharing between two ReAct agents using a shared graph structure.	41.2266778	1.0051752	5	368	3297	completed	1	1	faithful	support	Knowledge sharing between ReAct agents improved task completion rates and efficiency, especially for sequential tasks.	The experiment compared ReAct agents with and without knowledge sharing across three tasks. Shared knowledge agents achieved 100% success rate on sequence tasks (vs 60% baseline) and required fewer steps for finder tasks (1.0 vs 3.4 average steps), though differences did not reach statistical significance (p>0.05). Results suggest knowledge sharing improves efficiency and reliability, particularly for complex sequential tasks.	Knowledge sharing between ReAct agents improves task performance compared to independent or single agents.
978890622974	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	action-outcome-tracking-copy1	Create agents that track and learn from their action successes and failures in text games.	70.09083134999999	5.863191950000001	26	407	3621	failed (too many debug iterations)	0	0			None		
404713430852	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	action-outcome-tracking-copy2	Create agents that track and learn from their action successes and failures in text games.	138.40384598333333	3.5229542999999994	14	590	5492	completed	1	1	faithful	reject	Action-tracking enhancement to ReAct agent showed no improvement over standard ReAct in cooking task environment.	The experiment compared a new action-tracking ReAct agent against baseline agents in TextWorldExpress CookingWorld across 20 episodes. While both ReAct (0.349 ± 0.192) and action-tracking ReAct (0.282 ± 0.110) significantly outperformed random (0.030 ± 0.061), the action-tracking enhancement did not improve performance over standard ReAct (p=0.937).	Adding action history tracking and context-aware action selection to a ReAct agent will improve its performance in text-based cooking tasks.
961379944206	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	action-outcome-tracking-copy3	Create agents that track and learn from their action successes and failures in text games.	134.46935563333332	1.8623742500000002	10	444	3821	completed	1	1	deviations	support	Action-tracking ReAct agent outperforms random baseline in cooking tasks (0.342 vs 0.087 score, p<0.001).	An action-tracking ReAct agent was compared against a random baseline in a cooking task environment across 20 episodes. The tracking agent significantly outperformed the random baseline (mean scores: 0.342 vs 0.087, p=0.000) with higher action success rates (0.201 vs 0.030), demonstrating that action history tracking improves task performance.	Enhancing a ReAct agent with action history tracking and context-aware action selection will improve performance in sequential decision-making tasks compared to baseline agents.
579379577318	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	action-outcome-tracking-copy4	Create agents that track and learn from their action successes and failures in text games.	84.02921696666667	4.3069876	20	578	4985	completed	1	1	deviations	inconclusive	Action tracking improved agent consistency but not overall performance compared to standard ReAct in CookingWorld.	The Action-Tracking ReAct agent significantly outperformed random (0.366 vs 0.136, p<0.001) but performed similarly to standard ReAct (0.366 vs 0.397, p=0.72) across 20 episodes. However, the tracking agent showed notably lower performance variability (std=0.062 vs 0.207) and reduced action repetition (0.135 vs 0.243), suggesting more consistent and efficient behavior.	Adding action tracking capabilities to a ReAct agent will improve its performance in TextWorldExpress CookingWorld tasks by enabling it to learn from past successes and failures.
993350105078	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	action-outcome-tracking-copy5	Create agents that track and learn from their action successes and failures in text games.	13.306248216666667	0.838128	5	308	2593	completed	1	1	faithful	inconclusive	Action-Tracking ReAct agent outperforms random baseline but not ReAct baseline in CookingWorld tasks.	In a 20-episode pilot study comparing three agents in CookingWorld, the Action-Tracking ReAct agent (score=0.235) significantly outperformed the random baseline (score=0.148, p=0.020) but performed slightly worse than the ReAct baseline (score=0.288, p=0.825). Both intelligent agents completed tasks in fewer steps than random, though the Action-Tracking agent showed high action repetition (44.3%).	Action tracking and history-based action selection will improve the performance of a ReAct agent in text-based game environments by avoiding previously failed actions and preferring previously successful ones.
781995381541	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	template-world-generation-copy1	Generating single-room text-based game environments using templates and controlled object variation.	12.386149916666666	0.8310906499999999	5	404	3413	completed	1	0	deviations	support	Template-generated and manual text game environments showed identical perfect performance in ReAct agent evaluation.	The experiment compared template-generated vs manually-designed text game environments using a ReAct agent's performance across 10 episodes each in 5 environments per condition. Both conditions achieved 100% success rates with nearly identical average steps (template: 1.02, manual: 1.0), with bootstrap analysis showing no significant difference (p=1.0), suggesting template-generated environments are equally playable for simple pick-up tasks.	Template-generated text game environments can be as playable as manually designed ones, as measured by automated agent performance.
347486937900	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	template-world-generation-copy2	Generating single-room text-based game environments using templates and controlled object variation.	60.77325748333333	5.59106475	26	376	3138	failed (too many debug iterations)	0	0			None		
729956710789	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	template-world-generation-copy3	Generating single-room text-based game environments using templates and controlled object variation.	85.26623566666667	4.8860003	22	510	4549	completed	1	1	faithful	support	Template-generated game environments performed similarly to manual ones, with minor differences in efficiency.	A comparison of template-generated vs manually-designed text game environments showed similar but slightly lower performance on template environments (66% vs 74% success rate, 11.06 vs 9.68 average steps). While differences weren't statistically significant for success rate (p=0.954), the step difference approached significance (p=0.069), suggesting template environments may be slightly more challenging but generally comparable to manual ones.	Template-generated text game environments can be as playable as manually designed environments.
378708153382	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	template-world-generation-copy4	Generating single-room text-based game environments using templates and controlled object variation.	129.21063738333334	5.612141950000001	26	449	4079	failed (too many debug iterations)	0	0			None		
152490407915	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	template-world-generation-copy5	Generating single-room text-based game environments using templates and controlled object variation.	117.04219516666667	5.7721717	26	593	5279	failed (too many debug iterations)	0	0			None		
59102029926	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	subgoal-quality-evaluation-copy1	Evaluating whether LLM-based subgoal filtering improves hierarchical agent performance in simple text games.	41.89906665	1.2340987000000003	6	465	4587	completed	1	1	deviations	reject	LLM subgoal filtering did not improve hierarchical agent performance; flat ReAct agent performed best.	In a 10-episode pilot comparing hierarchical agents with/without LLM subgoal filtering to flat ReAct and random baselines, LLM filtering did not improve performance (filtered: 0.35 score/0% success, unfiltered: 0.375/10%, ReAct: 0.45/20%). Statistical comparison between filtered and ReAct was not significant (p=0.74), with high subgoal quality scores (4.13/5) but many execution failures suggesting implementation issues.	LLM-based filtering of subgoals using quality assessment will improve the performance of hierarchical agents in TextWorld Common Sense tasks by ensuring only high-quality subgoals are pursued.
115105865653	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	subgoal-quality-evaluation-copy2	Evaluating whether LLM-based subgoal filtering improves hierarchical agent performance in simple text games.	120.56654350000001	7.485809399999999	22	623	6124	failed (code parsing issue)	0	0			None		
441039178463	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	subgoal-quality-evaluation-copy3	Evaluating whether LLM-based subgoal filtering improves hierarchical agent performance in simple text games.	59.246727666666665	2.3829035000000003	12	445	4198	completed	1	1	deviations	support	LLM-filtered hierarchical agent showed modest performance gains over unfiltered version in TWC tasks.	In a 10-episode pilot comparing hierarchical agents with and without LLM-based subgoal filtering on TWC tasks, the filtered agent achieved higher completion rates (40% vs 30%) and scores (0.625 vs 0.5) than the unfiltered agent, while both substantially outperformed random baseline (0% completion). The filtered agent rejected 24% of proposed subgoals as low quality, suggesting LLM filtering may improve efficiency.	LLM-based subgoal filtering improves the performance of hierarchical agents in TextWorldExpress TWC tasks by preventing the pursuit of low-quality subgoals.
558455047477	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	subgoal-quality-evaluation-copy4	Evaluating whether LLM-based subgoal filtering improves hierarchical agent performance in simple text games.	259.5549823333333	7.138702699999999	26	548	5324	failed (too many debug iterations)	0	0			None		
111930527078	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	subgoal-quality-evaluation-copy5	Evaluating whether LLM-based subgoal filtering improves hierarchical agent performance in simple text games.	142.57111315	3.00558565	12	480	4576	completed	1	1	deviations	inconclusive	LLM-filtered hierarchical agent showed higher success rate (50% vs 40%) but identical performance on successful episodes.	In a 10-episode pilot comparing hierarchical agents with and without LLM-based subgoal filtering, the filtered agent achieved 50% success rate versus 40% for unfiltered and 30% for flat ReAct baseline. However, on successful episodes, both hierarchical agents achieved identical mean scores (0.25) with no significant differences (p=1.0).	LLM-based subgoal filtering improves hierarchical agent performance in TextWorldExpress TWC tasks by preventing the pursuit of low-quality subgoals.
578751774116	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-affordance-exploration-copy1	Test if basic affordance predictions can improve exploration in simple science tasks.	68.15051601666667	5.520913250000001	26	487	4414	failed (too many debug iterations)	0	0			None		
952224389115	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-affordance-exploration-copy2	Test if basic affordance predictions can improve exploration in simple science tasks.	63.8792203	6.191869499999999	26	559	4977	failed (too many debug iterations)	0	0			None		
924367592781	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-affordance-exploration-copy3	Test if basic affordance predictions can improve exploration in simple science tasks.	107.63558113333333	5.036715999999999	26	480	4064	failed (too many debug iterations)	0	0			None		
516607266752	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-affordance-exploration-copy4	Test if basic affordance predictions can improve exploration in simple science tasks.	61.12402888333334	3.6172453000000004	14	558	5217	completed	1	1	faithful	support	LLM-guided agent outperformed random baseline on ScienceWorld tasks, achieving significantly higher scores.	An LLM-guided agent was compared to a random baseline on two ScienceWorld tasks over 10 episodes each. The experimental agent significantly outperformed the baseline on the thermometer task (11.4 vs 3.0 mean score, p<0.001) and showed trending improvements on the boiling task (2.1 vs 0.9 mean score, p=0.051), though it required more steps to achieve these scores.	LLM-guided affordance prediction can improve exploration efficiency in ScienceWorld tasks compared to random exploration.
982809547528	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-affordance-exploration-copy5	Test if basic affordance predictions can improve exploration in simple science tasks.	114.74797163333332	3.4951609999999995	15	512	4530	completed	1	1	faithful	support	LLM-guided exploration significantly outperformed random exploration in ScienceWorld tasks, showing 4x higher scores.	An experiment comparing random vs LLM-guided exploration in ScienceWorld tasks showed the LLM-guided agent significantly outperformed random exploration (mean scores 5.15 vs 1.15, p < 0.001, n=20). The experimental agent successfully used GPT-4-mini to predict useful actions and maintain action success rates, though neither agent consistently completed the full tasks.	LLM-guided affordance prediction can improve exploration efficiency in ScienceWorld tasks compared to random exploration.
482686132508	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	rule-guided-action-validation-copy1	Evaluate whether simple cooking rules can improve action selection validity in TextWorldExpress cooking tasks.	50.899325383333334	5.45232285	26	524	4720	failed (too many debug iterations)	0	0			None		
477767524890	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	rule-guided-action-validation-copy2	Evaluate whether simple cooking rules can improve action selection validity in TextWorldExpress cooking tasks.	40.9435786	4.127284500000002	26	337	3073	failed (too many debug iterations)	0	0			None		
859142069745	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	rule-guided-action-validation-copy3	Evaluate whether simple cooking rules can improve action selection validity in TextWorldExpress cooking tasks.	68.30993495	5.68847175	26	457	4168	failed (too many debug iterations)	0	0			None		
534559935817	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	rule-guided-action-validation-copy4	Evaluate whether simple cooking rules can improve action selection validity in TextWorldExpress cooking tasks.	55.54648306666666	5.7787719	26	572	5075	failed (too many debug iterations)	0	0			None		
607783005840	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	rule-guided-action-validation-copy5	Evaluate whether simple cooking rules can improve action selection validity in TextWorldExpress cooking tasks.	64.18061738333333	5.4950655	26	521	4910	failed (too many debug iterations)	0	0			None		
598435068634	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-metaphor-graph-copy1	Create and visualize knowledge graphs showing metaphorical relationships between objects in cooking game scenarios.	11.527954683333332	0.7893263000000001	5	294	2560	completed	1	1	deviations	support	LLM successfully identified functional metaphors between kitchen objects, distinguishing meaningful from random relationships.	The experiment analyzed metaphorical relationships between objects in 5 CookingWorld scenarios, finding an average of 45 metaphorical relationships per scenario compared to 32 spatial co-occurrences. The LLM successfully identified plausible functional similarities between related objects (e.g., fridge-oven) while appropriately rejecting unrelated pairs (e.g., yellow potato-shoe cabinet), suggesting systematic metaphor detection capabilities.	Language models can systematically identify meaningful functional metaphorical relationships between objects in a cooking environment, distinguishing these from random relationships.
438854630471	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-metaphor-graph-copy2	Create and visualize knowledge graphs showing metaphorical relationships between objects in cooking game scenarios.	12.7858923	0.63877695	4	322	2625	completed	1	1	faithful	support	LLM analysis found systematic metaphorical relationships between objects in cooking scenarios, averaging 84.4 relationships per scenario.	Analysis of 5 CookingWorld scenarios revealed an average of 84.4 metaphorical relationships between objects per scenario, with scenarios containing an average of 12.4 objects and 42.8 co-occurrences. The LLM successfully identified coherent functional similarities between objects (e.g., 'Both use heat to achieve desired results' between dishwasher and oven), demonstrating systematic metaphorical relationships in cooking environments.	Objects in cooking environments share systematic metaphorical relationships based on their functional similarities.
185285384709	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-metaphor-graph-copy3	Create and visualize knowledge graphs showing metaphorical relationships between objects in cooking game scenarios.	50.0457147	4.766624250000001	26	371	3692	failed (too many debug iterations)	0	0			None		
944997250584	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-metaphor-graph-copy4	Create and visualize knowledge graphs showing metaphorical relationships between objects in cooking game scenarios.	57.220451499999996	4.224873400000001	20	425	4714	completed	1	1	deviations	inconclusive	Kitchen object pairs showed universal metaphorical relationships, suggesting rich connections or detection bias.	Analysis of 5 cooking scenarios found an average of 8.0 objects and 6.2 co-occurrences per scenario, with all co-occurring pairs (31/31) showing metaphorical relationships. While the metaphors identified were contextually meaningful (e.g., oven-stove: 'Both provide heat for cooking food'), the 100% metaphor detection rate suggests possible over-sensitivity in the detection system.	Kitchen environments contain meaningful metaphorical relationships between objects beyond simple co-occurrence, reflecting functional similarities that can be detected systematically.
210726655021	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-metaphor-graph-copy5	Create and visualize knowledge graphs showing metaphorical relationships between objects in cooking game scenarios.	9.0054103	0.6228623	4	310	2632	completed	1	1	faithful	support	LLM successfully identified metaphorical relationships between kitchen objects, finding meaningful connections in all analyzed pairs.	The experiment analyzed metaphorical relationships between kitchen objects across 5 CookingWorld scenarios, processing 10 objects per scenario. The LLM identified an average of 45.0 metaphorical relationships per scenario, with a 100% detection rate among analyzed pairs, suggesting extensive functional interconnectedness between kitchen objects. The relationships identified were contextually appropriate and demonstrated meaningful functional similarities.	Objects in cooking environments share meaningful functional similarities that can be detected through metaphorical relationship analysis.
751129025440	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	kg-state-tracking-copy1	Using knowledge graphs to track object locations and properties in a simplified CookingWorld environment.	16.70100898333333	1.0544147499999998	6	347	3078	completed	1	1	faithful	support	Knowledge graphs significantly improved state tracking accuracy (95.7%) compared to text-only baselines (59.4%).	A pilot study comparing knowledge graph (KG) vs text-only representations for state tracking in CookingWorld found that KG representations significantly improved accuracy (95.7% vs 59.4%, p < 0.001, bootstrap test). The KG condition showed notably more consistent performance, with most steps achieving perfect accuracy compared to more variable performance in the baseline.	Knowledge graph representations improve state tracking accuracy in CookingWorld compared to text-only representations.
170147280802	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	kg-state-tracking-copy2	Using knowledge graphs to track object locations and properties in a simplified CookingWorld environment.	39.36538136666666	1.9194043	9	470	4129	completed	1	1	deviations	inconclusive	KGs improved location tracking accuracy by 11.3% but reduced property tracking accuracy by 13.2% in CookingWorld.	Knowledge graph representations significantly improved location accuracy in CookingWorld state tracking (74.3% vs 63.0%, p=0.009) but decreased property accuracy (67.8% vs 81.0%) compared to text-only baselines. The pilot experiment with 10 episodes demonstrates that KGs may enhance spatial relationship tracking while potentially hindering property tracking.	Knowledge graph representations improve state tracking accuracy in CookingWorld compared to text-only representations.
893535620307	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	kg-state-tracking-copy3	Using knowledge graphs to track object locations and properties in a simplified CookingWorld environment.	30.78788455	1.7909981499999998	9	488	4325	completed	1	1	faithful	support	Knowledge graphs improved CookingWorld state tracking accuracy from 90.2% to 96.4% (p=0.025) in pilot study.	A pilot experiment comparing knowledge graph vs text-only representations for state tracking in CookingWorld found KG representations achieved significantly higher accuracy (96.4% vs 90.2%, p=0.025) across 50 steps. The results support the hypothesis that KG representations improve state tracking, though generalizability is limited by the small sample size and use of training data.	Knowledge graph representations improve state tracking accuracy in CookingWorld compared to text-only representations.
527877513335	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	kg-state-tracking-copy4	Using knowledge graphs to track object locations and properties in a simplified CookingWorld environment.	49.2148193	2.7558885	13	593	4758	completed	1	1	faithful	support	Knowledge graphs improved state tracking accuracy (98.2% vs 95.8%) in CookingWorld pilot experiment.	A pilot experiment comparing knowledge graph vs text-only representations for state tracking in CookingWorld found KG representations achieved significantly higher accuracy (98.2% vs 95.8%, p=0.0004) across 10 episodes of 10 steps each. The results suggest KGs improve state tracking reliability, though testing was limited to a simplified environment.	Knowledge graph representations improve state tracking accuracy in text-based game environments compared to text-only representations.
711625744194	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	kg-state-tracking-copy5	Using knowledge graphs to track object locations and properties in a simplified CookingWorld environment.	25.53959765	1.6635527499999998	8	475	4351	completed	1	1	faithful	reject	Knowledge graphs reduced state tracking accuracy in CookingWorld compared to text-only baselines.	A pilot experiment comparing knowledge graph vs text-only representations for state tracking in CookingWorld found that KGs performed significantly worse, with location accuracy of 79.2% vs 96.9% (p=1.0) and property accuracy of 73.2% vs 92.5% (p=0.997). These results suggest KGs may complicate rather than improve state tracking, though the small sample size (n=5 per condition) limits generalizability.	Knowledge graph representations improve state tracking accuracy in CookingWorld compared to text-only representations.
420037895144	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	knowledge-guided-decomposition-copy1	Using knowledge graphs to guide task decomposition decisions in text-based environments.	57.10839961666667	5.81160445	26	419	3660	failed (too many debug iterations)	0	0			None		
533127392866	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	knowledge-guided-decomposition-copy2	Using knowledge graphs to guide task decomposition decisions in text-based environments.	222.4433773	6.748314100000001	26	582	5468	failed (too many debug iterations)	0	0			None		
525393548001	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	knowledge-guided-decomposition-copy3	Using knowledge graphs to guide task decomposition decisions in text-based environments.	112.14337598333333	6.21861445	26	519	5180	failed (too many debug iterations)	0	0			None		
831179924108	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	knowledge-guided-decomposition-copy4	Using knowledge graphs to guide task decomposition decisions in text-based environments.	301.4461484666666	7.576815099999999	26	685	5931	failed (too many debug iterations)	0	0			None		
823248622437	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	knowledge-guided-decomposition-copy5	Using knowledge graphs to guide task decomposition decisions in text-based environments.	148.47177086666667	6.317007199999999	26	523	4994	failed (too many debug iterations)	0	0			None		
575499445188	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-self-evaluation-copy1	Using simple self-evaluation to improve ReAct agent performance in cooking tasks.	371.8152625	2.0331862	8	466	4536	failed (hard experiment runtime limit reached)	0	0			None		
557031903027	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-self-evaluation-copy2	Using simple self-evaluation to improve ReAct agent performance in cooking tasks.	221.94831674999998	2.5332849	8	323	2846	completed	1	1	deviations	inconclusive	Self-evaluating ReAct agent showed promising but non-significant improvements over standard ReAct in cooking tasks.	A self-evaluating ReAct agent was compared against standard ReAct and random baselines in a cooking task environment over 25 episodes. The self-evaluating agent achieved higher mean scores (0.22 vs 0.168/0.157) and was the only agent to complete any tasks, though differences were not statistically significant (p>0.10). Results suggest potential benefits of action evaluation, but larger samples are needed.	Adding a self-evaluation step to a ReAct agent will improve its performance in TextWorldExpress CookingWorld tasks compared to a standard ReAct agent.
478042629515	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-self-evaluation-copy3	Using simple self-evaluation to improve ReAct agent performance in cooking tasks.	534.0000507666667	2.2168825	7	533	5026	failed (hard experiment runtime limit reached)	0	0			None		
694282057916	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-self-evaluation-copy4	Using simple self-evaluation to improve ReAct agent performance in cooking tasks.	853.7953607666666	3.87417485	11	497	4953	completed	1	1	faithful	support	Self-evaluating ReAct agent outperformed standard ReAct and random baselines on cooking tasks.	A self-evaluating ReAct agent was compared against standard ReAct and random baselines on cooking tasks, with the self-evaluating agent achieving higher scores (0.447 vs 0.325 for standard ReAct) and completion rates (28% vs 20%). Statistical analysis showed significant improvement over random (p=0.0002) and trending improvement over standard ReAct (p=0.072), suggesting self-evaluation may enhance ReAct agent performance.	Adding self-evaluation capabilities to ReAct agents will improve their performance on text-based cooking tasks by helping them make better action decisions.
193276511155	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-self-evaluation-copy5	Using simple self-evaluation to improve ReAct agent performance in cooking tasks.	86.403953	3.0636762	11	668	6185	failed (code parsing issue)	0	0			None		
575571853109	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-dual-reflection-copy1	Compare sequential two-agent reflection against single-agent reflection in simple cooking tasks.	29.973551416666666	1.20932825	6	366	3361	completed	1	1	faithful	inconclusive	Reflection slightly improved CookingWorld performance over random actions but did not enable task success.	The experiment compared baseline random actions against single and dual-agent reflection in CookingWorld tasks. Both reflection conditions achieved modestly higher scores (0.092 vs 0.060), with single-agent reflection trending towards significance (p=0.065). However, no condition achieved task success, suggesting reflection provided limited benefits for random action selection.	Reflection, particularly dual-sequential reflection, improves agent performance on TextWorldExpress CookingWorld tasks compared to no reflection.
825533695487	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-dual-reflection-copy2	Compare sequential two-agent reflection against single-agent reflection in simple cooking tasks.	21.850879766666665	0.9025123	5	295	2741	completed	1	1	faithful	support	Dual-sequential reflection significantly outperformed baseline in CookingWorld tasks, with single reflection showing positive trends.	In a comparison of reflection approaches for TextWorldExpress CookingWorld tasks, dual-sequential reflection (mean score 0.183) significantly outperformed random baseline (0.083, p=0.036), while single-agent reflection showed a positive but non-significant trend (0.127, p=0.116). Both reflection approaches generated high-quality insights (quality ratings >0.83), suggesting that sequential reflection can meaningfully improve task performance.	Sequential reflection between two agents leads to better insights and task performance compared to single-agent reflection or no reflection.
230773435963	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-dual-reflection-copy3	Compare sequential two-agent reflection against single-agent reflection in simple cooking tasks.	186.92848946666666	8.16283555	26	638	5733	failed (too many debug iterations)	0	0			None		
305488358600	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-dual-reflection-copy4	Compare sequential two-agent reflection against single-agent reflection in simple cooking tasks.	16.649149983333334	0.49839285000000005	3	275	2521	completed	1	1	faithful	inconclusive	Reflection slightly improved CookingWorld task scores compared to random actions, but differences weren't significant.	The experiment compared random actions vs single-agent and dual-agent reflection in CookingWorld tasks, with mean scores of 0.093 (baseline), 0.112 (single-agent), and 0.123 (dual-agent). While both reflection conditions showed higher scores, the differences were not statistically significant (p>0.05), and no condition achieved task success, suggesting that the implemented reflection process did not substantially improve task performance.	Dual-sequential-agent reflection will lead to better performance than single-agent reflection, which will both outperform random actions in TextWorldExpress CookingWorld tasks.
184411424706	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-dual-reflection-copy5	Compare sequential two-agent reflection against single-agent reflection in simple cooking tasks.	91.68624608333333	0.9820419999999999	5	300	2695	completed	1	1	faithful	support	Reflection improved CookingWorld performance vs random actions, but dual-sequential wasn't better than single-agent reflection.	The experiment compared random actions vs single-agent and dual-sequential reflection in CookingWorld tasks. Both reflection conditions significantly outperformed random baseline (single-agent: 0.157 vs 0.058, p<0.001; dual-sequential: 0.131 vs 0.058, p=0.008), but dual-sequential did not significantly outperform single-agent reflection (p=0.775). This suggests reflection improves task performance but questions the added value of sequential reflection.	Reflection, particularly dual-sequential reflection between agents, improves performance on TextWorldExpress CookingWorld tasks compared to random actions.
77331884086	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-decomposition-memory-copy1	Using a history of successful decompositions to guide future task solving in cooking-related text games.	107.81982611666668	2.3454067000000003	10	539	4874	completed	1	1	deviations	inconclusive	Decomposition history agent showed higher but non-significant performance (0.6 vs 0.45) in cooking tasks.	The experiment compared a ReAct agent with decomposition history against a baseline in TextWorldExpress CookingWorld across 5 test episodes. The experimental agent achieved a mean score of 0.6 versus 0.45 for baseline, but this difference was not statistically significant (p=0.324). While the agent successfully stored 19 patterns, it showed 0% pattern reuse, suggesting limited effectiveness of the decomposition history mechanism.	Maintaining a history of successful task decompositions improves agent performance in sequential decision-making tasks.
518452155619	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-decomposition-memory-copy2	Using a history of successful decompositions to guide future task solving in cooking-related text games.	28.649237733333333	1.4185686499999999	7	342	2999	completed	1	1	deviations	reject	Task decomposition history did not improve agent performance in CookingWorld; experimental agent performed worse than baseline.	A pilot study (n=5 test episodes) compared ReAct agents with and without task decomposition history in TextWorldExpress CookingWorld. The experimental agent (mean score 0.14) performed worse than baseline (mean score 0.26), with no statistical significance (p=1.0). Results suggest that the implemented decomposition history mechanism did not improve agent performance.	Maintaining and reusing a history of successful task decompositions will improve agent performance in TextWorldExpress CookingWorld tasks.
880800423906	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-decomposition-memory-copy3	Using a history of successful decompositions to guide future task solving in cooking-related text games.	209.19770613333333	5.525495899999999	21	614	5729	completed	1	1	deviations	inconclusive	Decomposition history showed small, non-significant improvement in cooking task performance (0.35 vs 0.32, p=0.48).	An experiment comparing a baseline ReAct agent against an experimental agent with decomposition history in TextWorldExpress CookingWorld showed a small, non-significant improvement in test performance (experimental: 0.35, baseline: 0.32, p=0.48, n=5). While the experimental agent successfully stored 4 decomposition patterns, it failed to reuse them, suggesting the similarity threshold may need adjustment.	Maintaining and reusing a history of successful task decompositions improves agent performance in text-based cooking tasks by enabling transfer of successful strategies across similar situations.
954948339434	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-decomposition-memory-copy4	Using a history of successful decompositions to guide future task solving in cooking-related text games.	328.23691620000005	6.94966885	26	431	4044	failed (too many debug iterations)	0	0			None		
231679987302	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-decomposition-memory-copy5	Using a history of successful decompositions to guide future task solving in cooking-related text games.	232.01692194999998	7.102206699999999	26	478	4178	failed (too many debug iterations)	0	0			None		
960227885593	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	textworld-subgoal-planning-copy1	Evaluate subgoal-based planning versus direct planning in TextWorldExpress cooking tasks.	412.96967101666667	4.83437085	17	526	5361	failed (hard experiment runtime limit reached)	0	0			None		
249282742991	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	textworld-subgoal-planning-copy2	Evaluate subgoal-based planning versus direct planning in TextWorldExpress cooking tasks.	79.51676604999999	7.0427422	26	645	5503	failed (too many debug iterations)	0	0			None		
746905234178	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	textworld-subgoal-planning-copy3	Evaluate subgoal-based planning versus direct planning in TextWorldExpress cooking tasks.	67.51482871666666	6.646235400000001	26	674	5944	failed (too many debug iterations)	0	0			None		
714108688926	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	textworld-subgoal-planning-copy4	Evaluate subgoal-based planning versus direct planning in TextWorldExpress cooking tasks.	28.728521266666668	2.8980102	9	745	6254	failed (code parsing issue)	0	0			None		
993310616405	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	textworld-subgoal-planning-copy5	Evaluate subgoal-based planning versus direct planning in TextWorldExpress cooking tasks.	79.95014865	7.417779999999999	26	561	4374	failed (too many debug iterations)	0	0			None		
216385637444	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	react-pattern-learning-copy1	Investigating pattern-based reasoning reuse in ReAct agents on cooking tasks.	206.46382379999997	5.951402800000001	26	394	3469	failed (too many debug iterations)	0	0			None		
418959599523	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	react-pattern-learning-copy2	Investigating pattern-based reasoning reuse in ReAct agents on cooking tasks.	179.8703232	5.95050895	26	521	4250	failed (too many debug iterations)	0	0			None		
718866269632	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	react-pattern-learning-copy3	Investigating pattern-based reasoning reuse in ReAct agents on cooking tasks.	235.83354751666667	7.265611000000001	26	580	5319	failed (too many debug iterations)	0	0			None		
98551022834	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	react-pattern-learning-copy4	Investigating pattern-based reasoning reuse in ReAct agents on cooking tasks.	163.44809519999998	6.978408999999997	26	600	5685	failed (too many debug iterations)	0	0			None		
371210500095	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	react-pattern-learning-copy5	Investigating pattern-based reasoning reuse in ReAct agents on cooking tasks.	139.71730553333333	6.23726785	26	505	4681	failed (too many debug iterations)	0	0			None		
355654830678	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	two-level-discovery-agent-copy1	Two-level hierarchical agent that separates planning and execution for scientific measurement tasks.	0.0	0.0	0			interrupted	0	0			None		
282159534503	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	two-level-discovery-agent-copy2	Two-level hierarchical agent that separates planning and execution for scientific measurement tasks.	129.68670801666667	2.93456625	10	669	5910	failed (code parsing issue)	0	0			None		
568037092726	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	two-level-discovery-agent-copy3	Two-level hierarchical agent that separates planning and execution for scientific measurement tasks.	160.2166885	6.660405900000001	26	516	4434	failed (too many debug iterations)	0	0			None		
6134588826	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	two-level-discovery-agent-copy4	Two-level hierarchical agent that separates planning and execution for scientific measurement tasks.	156.67869604999999	9.175975699999999	26	581	5199	failed (too many debug iterations)	0	0			None		
42805563024	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	two-level-discovery-agent-copy5	Two-level hierarchical agent that separates planning and execution for scientific measurement tasks.	145.22660148333335	6.852083400000001	22	716	6150	failed (code parsing issue)	0	0			None		
27280284219	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-template-discovery-copy1	Automatically discover and use simple two-action templates from successful gameplay trajectories in CookingWorld.	192.90190246666668	5.3951796000000005	26	561	4381	failed (too many debug iterations)	0	0			None		
48618371986	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-template-discovery-copy2	Automatically discover and use simple two-action templates from successful gameplay trajectories in CookingWorld.	272.6505311166667	4.9614674999999995	26	404	3442	failed (too many debug iterations)	0	0			None		
481252268345	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-template-discovery-copy3	Automatically discover and use simple two-action templates from successful gameplay trajectories in CookingWorld.	79.90571983333332	5.4501468000000015	26	594	5197	failed (too many debug iterations)	0	0			None		
641014688548	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-template-discovery-copy4	Automatically discover and use simple two-action templates from successful gameplay trajectories in CookingWorld.	54.713772383333335	5.61878535	26	615	5345	failed (too many debug iterations)	0	0			None		
85342904904	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-template-discovery-copy5	Automatically discover and use simple two-action templates from successful gameplay trajectories in CookingWorld.	58.06724733333333	5.075851199999999	22	702	6253	failed (code parsing issue)	0	0			None		
184558464630	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	kg-failure-detection-copy1	Using knowledge graph features to detect action failures in text-based games	126.66317269999999	6.50144985	25	322	2978	completed	1	0	deviations	reject	Knowledge graph and keyword-based failure detectors both performed poorly, with 0% accuracy and high false positives.	A knowledge-graph-based failure detector was compared against a keyword baseline in TextWorldExpress CookingWorld across 25 episodes. Both detectors performed poorly with 0% precision/recall/F1, with the KG detector generating 54 false positives and the keyword detector 66 false positives, while both missed all 7 actual failures. The results suggest the current KG-based approach is not effective for failure detection without significant refinement.	Knowledge-graph-based failure detection, which tracks object states and relationships, will be more effective at detecting failures in TextWorldExpress than simpler baseline methods.
205192828129	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	kg-failure-detection-copy2	Using knowledge graph features to detect action failures in text-based games	74.87539278333334	7.0022643	26	667	5905	failed (too many debug iterations)	0	0			None		
648284814643	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	kg-failure-detection-copy3	Using knowledge graph features to detect action failures in text-based games	103.7015125	3.0750217500000003	12	583	5690	failed (code parsing issue)	0	0			None		
794699809674	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	kg-failure-detection-copy4	Using knowledge graph features to detect action failures in text-based games	109.45313501666666	7.294170150000001	26	671	5725	failed (too many debug iterations)	0	0			None		
481142864405	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	kg-failure-detection-copy5	Using knowledge graph features to detect action failures in text-based games	92.80792228333333	7.56373065	26	622	5962	failed (too many debug iterations)	0	0			None		
316234988429	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-meta-graphs-copy1	Track ReAct agent performance using simple knowledge graphs to make mode-switching decisions on classification tasks.	405.1184256166667	0.9256491	4	393	3576	failed (hard experiment runtime limit reached)	0	0			None		
267708675464	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-meta-graphs-copy2	Track ReAct agent performance using simple knowledge graphs to make mode-switching decisions on classification tasks.	96.93665741666668	6.873430200000001	26	539	5325	failed (too many debug iterations)	0	0			None		
903594503543	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-meta-graphs-copy3	Track ReAct agent performance using simple knowledge graphs to make mode-switching decisions on classification tasks.	80.02041051666667	5.71585655	18	436	4136	completed	1	1	faithful	support	Knowledge graph mode-switching shows promise but detailed mode performs best in ScienceWorld living-thing task.	A ReAct agent experiment compared knowledge-graph-based adaptive mode switching against fixed and random strategies in ScienceWorld. The detailed mode performed best (100% success), significantly outperforming random (p=0.011), while the knowledge graph approach showed promising results (80% success) trending toward significance (p=0.081) with better token efficiency than the detailed mode.	A knowledge-graph-based adaptive mode switching strategy will improve ReAct agent performance compared to fixed strategies or random switching by learning which mode (detailed vs quick) works best in different states.
698783614562	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-meta-graphs-copy4	Track ReAct agent performance using simple knowledge graphs to make mode-switching decisions on classification tasks.	70.26772771666667	3.43779695	9	564	5070	completed	1	1	faithful	reject	Knowledge-graph mode switching outperformed quick mode but underperformed always-detailed mode in ReAct agent experiments.	A knowledge-graph-based mode switching system for ReAct agents was compared against fixed and random mode baselines on ScienceWorld tasks. The knowledge graph condition (mean score 16.8) significantly outperformed always-quick mode (-53.2, p=0.0002) but performed worse than always-detailed mode (71.8, p=1.0), suggesting that while dynamic mode switching showed some benefits, it did not improve upon the simpler strategy of always using detailed mode.	Using a knowledge graph to dynamically switch between detailed and quick modes based on historical performance will improve ReAct agent performance compared to fixed-mode or random-mode approaches.
992170823870	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-meta-graphs-copy5	Track ReAct agent performance using simple knowledge graphs to make mode-switching decisions on classification tasks.	57.92866775	4.9961605	11	478	4342	completed	1	1	deviations	support	Knowledge graph-based mode switching significantly improved ReAct agent performance versus baseline approaches.	A ReAct agent using knowledge graph-based mode switching was compared to random, always-detailed, and always-quick baselines on the ScienceWorld living-thing task. The knowledge graph condition achieved 100% success rate and significantly outperformed random (p=0.0002) and always-detailed (p=0.0098) conditions, suggesting that intelligent mode switching improves performance.	Using a knowledge graph to track historical performance and make intelligent mode-switching decisions will improve ReAct agent performance compared to fixed or random strategies.
735688727604	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-hierarchical-beliefs-copy1	Study if simple two-level hierarchical belief graphs improve knowledge representation for temperature-related tasks.	102.98277338333334	5.34593445	26	495	4435	failed (too many debug iterations)	0	0			None		
523249416132	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-hierarchical-beliefs-copy2	Study if simple two-level hierarchical belief graphs improve knowledge representation for temperature-related tasks.	62.547375699999996	5.2678142999999995	26	591	5259	failed (too many debug iterations)	0	0			None		
93815083783	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-hierarchical-beliefs-copy3	Study if simple two-level hierarchical belief graphs improve knowledge representation for temperature-related tasks.	126.7424569	6.84557915	26	634	5865	failed (too many debug iterations)	0	0			None		
946619815053	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-hierarchical-beliefs-copy4	Study if simple two-level hierarchical belief graphs improve knowledge representation for temperature-related tasks.	85.98096766666666	6.195513750000001	26	652	5922	failed (too many debug iterations)	0	0			None		
711515798595	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-hierarchical-beliefs-copy5	Study if simple two-level hierarchical belief graphs improve knowledge representation for temperature-related tasks.	68.58654405	4.56761085	20	697	6184	failed (code parsing issue)	0	0			None		
207919705341	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	hypothesis-driven-discovery-copy1	Agent that generates and tests scientific hypotheses in structured environments	838.0537067666667	0.7028896	2	402	3561	failed (hard experiment runtime limit reached)	0	0			None		
225054227470	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	hypothesis-driven-discovery-copy2	Agent that generates and tests scientific hypotheses in structured environments	107.0974792	5.952220800000001	17	686	6092	failed (code parsing issue)	0	0			None		
452019947785	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	hypothesis-driven-discovery-copy3	Agent that generates and tests scientific hypotheses in structured environments	82.99446545	6.709346550000001	26	535	5173	failed (too many debug iterations)	0	0			None		
152149794801	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	hypothesis-driven-discovery-copy4	Agent that generates and tests scientific hypotheses in structured environments	181.62959948333335	4.018545550000001	13	564	5683	completed	1	1	faithful	support	Hypothesis-driven agent outperformed baselines in plant nutrient discovery, scoring 0.75 vs 0.50/0.15 (p<0.001).	A hypothesis-driven agent was compared to ReAct and random baselines in a plant nutrient discovery task over 10 episodes each. The hypothesis-driven agent achieved consistently higher performance (0.75 normalized score) than ReAct (0.50) and random (0.15) baselines, with statistically significant differences (p<0.001) in both comparisons, though no agent achieved full task completion.	A hypothesis-driven agent that explicitly maintains and tests scientific hypotheses will perform better at scientific discovery tasks than baseline agents using standard ReAct architecture or random actions.
896296639753	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	hypothesis-driven-discovery-copy5	Agent that generates and tests scientific hypotheses in structured environments	858.9188971833333	1.4917424000000001	4	515	4859	failed (hard experiment runtime limit reached)	0	0			None		
306282002750	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	knowledge-guided-react-copy1	A ReAct agent that builds and uses a knowledge graph while exploring text-based games to improve decision making.	114.09370991666667	6.4105725499999995	24	587	5348	completed	1	1	faithful	inconclusive	Knowledge graph enhancement showed slight, non-significant improvement in ReAct agent's cooking task performance.	A pilot experiment comparing vanilla ReAct vs knowledge-graph-enhanced ReAct agents in CookingWorld showed slightly higher performance for the experimental agent (mean score 0.59 vs 0.50) but this difference was not statistically significant (p=0.17, n=10 episodes). Both agents achieved identical 30% success rates, suggesting the knowledge graph enhancement did not provide clear benefits in this limited test.	Enhancing a ReAct agent with a knowledge graph that tracks environmental state will improve its performance in sequential decision-making tasks.
253219380917	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	knowledge-guided-react-copy2	A ReAct agent that builds and uses a knowledge graph while exploring text-based games to improve decision making.	107.77945774999999	3.2775552	12	678	6246	failed (code parsing issue)	0	0			None		
215236615887	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	knowledge-guided-react-copy3	A ReAct agent that builds and uses a knowledge graph while exploring text-based games to improve decision making.	48.78320063333333	1.35132575	6	333	3085	completed	1	0	deviations	inconclusive	Knowledge graph enhancement to ReAct agent showed minimal, non-significant improvement in CookingWorld task performance.	A pilot comparison of vanilla ReAct vs knowledge-graph-enhanced ReAct agents in CookingWorld showed slightly better performance for the experimental agent (mean score 0.367 vs 0.325) but the difference was not statistically significant (p=0.305). The knowledge graph successfully tracked environment state (growing to 44 nodes/306 edges) but did not meaningfully improve agent performance.	Enhancing a ReAct agent with a knowledge graph that tracks environment state will improve its performance in text-based game environments by providing structured memory of past observations and relationships.
265407162448	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	knowledge-guided-react-copy4	A ReAct agent that builds and uses a knowledge graph while exploring text-based games to improve decision making.	39.8764074	1.7121492	8	412	3582	completed	1	1	faithful	inconclusive	Knowledge-graph ReAct agent performed slightly better than baseline in cooking tasks, but difference not significant.	A pilot comparison of baseline vs knowledge-graph-enhanced ReAct agents in CookingWorld showed slightly better performance for the KG agent (mean score 0.433 vs 0.333) but the difference was not statistically significant (p=0.322, n=5). While both agents could complete basic tasks, the results suggest that knowledge graph augmentation may provide modest benefits for recipe following, though more extensive testing is needed.	Adding a knowledge graph to track environmental state information will improve a ReAct agent's performance on cooking tasks by providing structured memory and relationships between objects.
805233879853	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	knowledge-guided-react-copy5	A ReAct agent that builds and uses a knowledge graph while exploring text-based games to improve decision making.	220.99781426666667	9.3037372	26	664	6288	failed (too many debug iterations)	0	0			None		
41946214494	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	knowledge-graph-discovery-copy1	Build an agent that creates and updates knowledge graphs while performing scientific discovery tasks in DiscoveryWorld.	321.6239940833333	24.3018094	26	589	5550	failed (too many debug iterations)	0	0			None		
121832125252	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	knowledge-graph-discovery-copy2	Build an agent that creates and updates knowledge graphs while performing scientific discovery tasks in DiscoveryWorld.	116.1439942	9.920549	26	569	4875	failed (too many debug iterations)	0	0			None		
513987382373	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	knowledge-graph-discovery-copy3	Build an agent that creates and updates knowledge graphs while performing scientific discovery tasks in DiscoveryWorld.	76.52280139999999	7.2830147499999995	26	561	4655	failed (too many debug iterations)	0	0			None		
445548743504	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	knowledge-graph-discovery-copy4	Build an agent that creates and updates knowledge graphs while performing scientific discovery tasks in DiscoveryWorld.	512.9289369333334	11.7451274	12	697	6072	failed (hard experiment runtime limit reached)	0	0			None		
540902997569	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	knowledge-graph-discovery-copy5	Build an agent that creates and updates knowledge graphs while performing scientific discovery tasks in DiscoveryWorld.	72.53720863333334	6.20958315	18	620	5825	failed (code parsing issue)	0	0			None		
42733426357	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	resistor-substitution-advisor-copy1	An LLM-based system that suggests viable resistor substitutions using combinations of standard-value resistors.	47.82189545000001	4.207458099999999	22	433	4318	completed	1	1	faithful	reject	Mathematical optimization outperformed LLM and nearest-value approaches in resistor combination selection, with LLM performing poorly.	A comparison of LLM-based, mathematical optimization, and nearest-value approaches for resistor combination selection showed the mathematical optimization method significantly outperformed others with 0.14% mean error versus 5.65% for nearest-value and 40.27% for LLM (p < 0.001). The LLM approach performed poorly, suggesting inappropriate combinations and demonstrating that current LLM technology may not be suitable for this specific engineering task.	An LLM-based approach can effectively suggest resistor combinations that match target resistance values, potentially offering advantages over traditional mathematical or simple baseline approaches.
874481223057	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	resistor-substitution-advisor-copy2	An LLM-based system that suggests viable resistor substitutions using combinations of standard-value resistors.	53.635071133333334	4.45321515	21	506	5084	completed	1	1	deviations	reject	LLM-based resistor combination advisor performed significantly worse than mathematical optimization and nearest-value approaches.	A comparison of LLM-based resistor combination suggestions against mathematical optimization and nearest-value baselines showed the LLM performing significantly worse, with mean error of 19.83% compared to 0.17% for optimization and 4.43% for nearest-value (p<0.001). The LLM particularly struggled with parallel resistance calculations, achieving only 18.2% success rate at 1% error threshold compared to 100% for optimization.	An LLM can effectively suggest combinations of standard resistors to match target resistance values, potentially matching or exceeding traditional algorithmic approaches.
571735205787	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	resistor-substitution-advisor-copy3	An LLM-based system that suggests viable resistor substitutions using combinations of standard-value resistors.	35.89006871666666	2.9046455	15	471	4627	completed	1	1	deviations	inconclusive	LLM-based resistor advisor underperformed mathematical optimization but showed potential over simple baseline.	In a comparison of LLM-based vs baseline approaches for resistor combination selection (n=20), mathematical optimization performed best (mean error 0.18%, 100% success rate) while the LLM approach showed mixed results (mean error 2.15%, 50% response validity) and nearest-value baseline performed worst (mean error 4.40%). Statistical comparison between LLM and nearest baseline was inconclusive (p=0.984).	An LLM-based approach can effectively suggest resistor combinations that match target resistance values, potentially offering advantages over traditional methods.
260958977013	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	resistor-substitution-advisor-copy4	An LLM-based system that suggests viable resistor substitutions using combinations of standard-value resistors.	13.845810216666667	0.843647	5	409	4124	completed	1	1	deviations	support	Mathematical optimization outperformed LLM-based approach in finding optimal resistor combinations, both beat baseline.	In a comparison of resistor combination methods using 20 target values, the mathematical optimization approach achieved the best performance with a mean error of 0.3%, significantly outperforming both the LLM-based approach (2.8% mean error) and the nearest-value baseline (5.2% mean error). Bootstrap analysis confirmed the optimization method's superiority (p=0.0 vs LLM), though both advanced methods substantially improved upon the simple baseline.	An LLM-based approach can effectively suggest resistor combinations that match target resistance values, potentially offering advantages over traditional methods.
231750637947	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	resistor-substitution-advisor-copy5	An LLM-based system that suggests viable resistor substitutions using combinations of standard-value resistors.	11.282389016666666	0.6203556	4	396	3885	completed	1	1	deviations	reject	Mathematical optimization outperformed LLM-based approach for resistor combination selection, with better reliability and accuracy.	In a comparison of methods for resistor combination selection across 20 random targets, the mathematical optimization baseline achieved consistently low errors (<1% in many cases), while the LLM-based approach failed validation in 40% of cases and had occasional large errors (>90%). The results suggest that for this well-defined mathematical problem, traditional optimization approaches are more reliable than LLM-based solutions.	An LLM-based advisor can effectively suggest resistor combinations that match target resistance values, potentially offering advantages over traditional mathematical approaches.
981087990814	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-abstraction-tuning-copy1	Tune program abstractions based on their success rates in cooking game tasks.	97.39143528333334	6.677220549999999	26	562	5167	failed (too many debug iterations)	0	0			None		
650254741260	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-abstraction-tuning-copy2	Tune program abstractions based on their success rates in cooking game tasks.	79.05176276666666	6.360767299999999	26	617	5503	failed (too many debug iterations)	0	0			None		
886557930298	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-abstraction-tuning-copy3	Tune program abstractions based on their success rates in cooking game tasks.	115.12230328333334	5.68765445	26	494	4450	failed (too many debug iterations)	0	0			None		
92185500578	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-abstraction-tuning-copy4	Tune program abstractions based on their success rates in cooking game tasks.	187.55228066666666	6.619443500000001	26	709	6187	failed (too many debug iterations)	0	0			None		
886151854411	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	simple-abstraction-tuning-copy5	Tune program abstractions based on their success rates in cooking game tasks.	143.07371851666667	6.9791532	26	666	5975	failed (too many debug iterations)	0	0			None		
645331972043	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	static-knowledge-comparison-copy1	Evaluate the relative effectiveness of ConceptNet versus LLM-derived knowledge for improving agent performance in ScienceWorld tasks.	217.41660786666668	7.315263699999999	26	610	5538	failed (too many debug iterations)	0	0			None		
948915428219	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	static-knowledge-comparison-copy2	Evaluate the relative effectiveness of ConceptNet versus LLM-derived knowledge for improving agent performance in ScienceWorld tasks.	361.56612863333334	6.860354000000001	22	477	4158	failed (hard experiment runtime limit reached)	0	0			None		
935232283022	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	static-knowledge-comparison-copy3	Evaluate the relative effectiveness of ConceptNet versus LLM-derived knowledge for improving agent performance in ScienceWorld tasks.	349.24531766666666	8.478289	26	201	1789	failed (too many debug iterations)	0	0			None		
58691933281	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	static-knowledge-comparison-copy4	Evaluate the relative effectiveness of ConceptNet versus LLM-derived knowledge for improving agent performance in ScienceWorld tasks.	157.45017818333332	6.394652850000001	26	351	3294	failed (too many debug iterations)	0	0			None		
208890681507	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	static-knowledge-comparison-copy5	Evaluate the relative effectiveness of ConceptNet versus LLM-derived knowledge for improving agent performance in ScienceWorld tasks.	53.13735115	2.0220593499999997	7	416	3337	completed	1	1	faithful	reject	Knowledge injection did not improve ReAct agent performance on water boiling task; baseline performed best.	The experiment compared knowledge injection methods (ConceptNet, LLM, random) against a baseline ReAct agent on a water boiling task, with 10 episodes per condition. The baseline performed best (90% success rate, 2.5 reward) compared to knowledge-augmented conditions (60-80% success rates, 1.8-2.2 rewards), though differences were not statistically significant (all p > 0.82). Results suggest injected knowledge may be unnecessary or distracting for simple physical tasks.	Injecting relevant knowledge from ConceptNet or LLMs will improve ReAct agent performance on physical reasoning tasks in ScienceWorld.
