experiment_name	batch_names	hypothesis	support_hypothesis_count	refute_hypothesis_count	inconclusive_hypothesis_count	categorization	detailed_summary	experiment_ids
action-outcome-tracking	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	Agents that maintain explicit records of their action outcomes will achieve higher success rates than baseline agents by avoiding previously failed actions and preferring previously successful ones.	0	2	2	limited information	This meta-analysis examined four experiments testing whether agents with action outcome tracking would outperform baseline agents in text-based games. All experiments used TextWorldExpress CookingWorld with similar configurations and compared performance across multiple episodes.  None of the experiments provided clear support for the hypothesis. Two experiments (copy2 and copy5) directly refuted it, showing the action-tracking agent performed similarly or slightly worse than the standard ReAct agent without statistical significance. The other two experiments were inconclusive - copy3 lacked a crucial ReAct baseline comparison, while copy4 showed no significant performance improvement but did reveal potential secondary benefits in consistency (lower standard deviation) and efficiency (lower action repetition).  Consistently across experiments, both intelligent agents (ReAct and Action-Tracking ReAct) significantly outperformed random baselines, confirming the value of structured reasoning approaches. However, the addition of explicit action outcome tracking did not consistently improve performance beyond standard ReAct capabilities.  Interestingly, experiment copy4 suggested that while action tracking may not improve absolute performance, it might lead to more consistent behavior (lower variability) and more efficient exploration (lower action repetition). This points to potential benefits beyond raw performance metrics that could be valuable in certain applications.  Limitations include the relatively small sample sizes (20 episodes per experiment in PILOT mode) and potential implementation differences across experiments. Future work might explore more sophisticated action tracking mechanisms, different environment complexities, or longer-term learning across episodes to better leverage historical action outcomes.	404713430852, 961379944206, 579379577318, 993350105078
basic-confidence-simulation	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	LLMs can meaningfully predict their confidence in action outcomes in CookingWorld, with higher confidence scores correlating with higher prediction accuracy.	0	3	0	consistent (refute)	This meta-analysis examined three experiments testing whether LLMs can meaningfully predict their confidence in action outcomes in the TextWorldExpress CookingWorld environment. All three experiments consistently refute the hypothesis that 'LLMs can meaningfully predict their confidence in action outcomes in CookingWorld, with higher confidence scores correlating with higher prediction accuracy.' The experiments were conducted in PILOT mode with 5 games and 20 actions per game (100 total actions), using gpt-4o-mini as the LLM. While the LLM demonstrated above-chance accuracy in predicting action outcomes (69-74% across experiments), all three experiments found only weak correlations between confidence scores and prediction accuracy (r=0.067 to r=0.144). Critically, the LLM assigned nearly identical confidence scores to both correct and incorrect predictions across all experiments, showing poor calibration. The LLM exhibited overconfidence, with average confidence scores consistently high (0.78-0.91) regardless of prediction correctness. This pattern was remarkably consistent across all three experimental runs, strongly suggesting that while the LLM can predict action outcomes with moderate accuracy, it lacks the ability to meaningfully assess its confidence in these predictions. The experiments were implemented faithfully according to the specifications, with proper statistical testing comparing the LLM against random and constant baselines. These findings have implications for the reliability of LLM confidence estimates in decision-critical applications, suggesting that confidence scores from current LLMs may not provide meaningful signals about prediction reliability.	153722142099, 222008229554, 630748612275
basic-knowledge-sharing	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	Two ReAct agents with access to a shared knowledge graph will perform better on information-dependent tasks compared to agents working independently.	2	1	1	mixed information	This meta-analysis examined four experiments testing whether ReAct agents with access to a shared knowledge graph perform better on information-dependent tasks compared to agents working independently. The experiments used similar designs with three conditions (single agent baseline, two independent agents, two agents with shared knowledge) across three task types (key finding, sequence execution, goal state achievement).  Results were mixed across the experiments. Two experiments (copy4 and copy5) provided support for the hypothesis, showing clear benefits of knowledge sharing in terms of success rates and efficiency, particularly for complex tasks like goal-state achievement and sequence execution. One experiment (copy3) refuted the hypothesis, with independent agents consistently outperforming shared-knowledge agents across all tasks, though all conditions achieved 100% success. One experiment (copy1) was inconclusive, with mixed results across different tasks and no clear advantage for shared knowledge.  Several factors may explain these inconsistencies. First, the experiments were run in PILOT mode with small sample sizes (5 episodes per condition), limiting statistical power. Second, task difficulty varied across implementations - in some cases, tasks may have been too simple to benefit from knowledge sharing. Third, the implementation details of the knowledge sharing mechanism likely differed between experiments.  The strongest evidence for knowledge sharing benefits appeared in more complex tasks (goal-state achievement) and when baseline performance was not already at ceiling. When tasks were simple enough for independent agents to achieve perfect or near-perfect performance, the overhead of knowledge sharing sometimes resulted in reduced efficiency.  Overall, the meta-analysis suggests that knowledge sharing between agents can be beneficial under certain conditions, particularly for complex tasks where information exchange is critical. However, the benefits are not universal and may depend on task complexity, implementation details, and the specific information being shared. Future research should use larger sample sizes, ensure sufficient task complexity to benefit from information sharing, and standardize knowledge sharing implementations across experiments.	577808209948, 389214523208, 325336531627, 644517944670
cooking-graph-explorer		An agent that maintains an explicit knowledge graph of container relationships will perform better at CookingWorld tasks than an agent without this explicit knowledge representation.	0	0	0	no information	No experiments were provided in the data for meta-analysis. The research idea aimed to compare a container-knowledge-enhanced agent against baseline agents (random and standard ReAct) in TextWorldExpress CookingWorld environments. The hypothesis posited that an agent maintaining an explicit knowledge graph of container relationships would outperform agents without this representation. The planned experiment would have measured task completion rates, average scores, and knowledge graph accuracy across different experimental modes (MINI_PILOT, PILOT, and FULL_EXPERIMENT). Without experimental results, no conclusions can be drawn regarding the validity of the hypothesis or the effectiveness of the container knowledge representation approach.	
failure-pattern-learning	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	An agent that tracks and learns from patterns in its failed actions will perform better than a baseline agent that doesn't maintain failure history.	1	1	0	limited information	This meta-analysis examined two experiments testing whether an agent that tracks and learns from patterns in its failed actions performs better than a baseline agent without failure history tracking in TextWorld Commonsense (TWC) games. The results were mixed and contradictory. In the first experiment, the failure-pattern-tracking agent significantly outperformed the baseline agent, achieving a mean score of 0.41 compared to 0.058 (p < 0.001) and successfully completing 3 tasks while the baseline completed none. However, in the second experiment, the pattern-tracking agent actually performed worse than the baseline (mean score 0.34 vs 0.46), though this difference was not statistically significant (p=0.97). Both experiments used the same setup: PILOT mode with 25 episodes, 30 steps max per episode, on 3 different TWC games. The contradictory results suggest that the effectiveness of failure pattern tracking may be highly dependent on implementation details, game characteristics, or random variations in agent behavior. The failure tracking mechanism appeared to help in the first experiment by preventing the agent from repeating mistakes, but this advantage did not materialize in the second experiment. Given the equal split between supporting and refuting evidence, and the lack of statistical significance in the second experiment, more research with larger sample sizes and varied implementations would be needed to draw definitive conclusions about the effectiveness of failure pattern tracking in text-based game agents.	829607985316, 812565744094
hierarchical-elimination		Hierarchical elimination will be more efficient and accurate than flat elimination, particularly in complex environments with many objects and areas.	0	0	0	no information	No experiment results were provided in the input. The research idea aimed to extend PET's elimination module to work hierarchically, first eliminating irrelevant high-level categories (e.g., rooms, areas) before filtering specific objects. The hypothesis was that hierarchical elimination would be more efficient and accurate than flat elimination, particularly in complex environments with many objects and areas. The planned experiment would have compared three conditions: hierarchical elimination, flat elimination, and no elimination, measuring metrics such as precision/recall of relevant object identification, computation time, and task completion rates. However, since no experiment results were provided, no conclusions can be drawn regarding the hypothesis.	
hypothesis-driven-discovery	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	An agent that explicitly generates and tests hypotheses will discover correct environment mechanics more reliably than agents that explore without structured hypothesis testing.	1	0	0	limited information	This meta-analysis examined whether an agent that explicitly generates and tests hypotheses would discover correct environment mechanics more reliably than agents that explore without structured hypothesis testing. Only one experiment was conducted (hypothesis-driven-discovery-copy4), which ran in PILOT mode with 10 episodes per agent type in DiscoveryWorld's Plant Nutrients environment. The results strongly support the hypothesis, with the hypothesis-driven agent achieving a normalized score of 0.75, significantly outperforming both the ReAct baseline (0.50, p<0.001) and random baseline (0.15, p<0.001). The hypothesis-driven agent successfully generated and tested hypotheses about nutrient types, demonstrating better scientific discovery capabilities. However, it's worth noting that no agent fully completed the task, which required selecting the final answer in the controller. While the evidence from this single experiment is promising, additional experiments with varied environments, larger sample sizes, and full task completion would strengthen the conclusion. The current evidence suggests that structured hypothesis testing does indeed lead to more reliable discovery of environment mechanics compared to standard exploration approaches.	152149794801
kg-failure-detection	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	An agent using knowledge graph features can detect action failures more accurately and quickly compared to agents using only text-based observation features.	0	1	0	limited information	This meta-analysis examined whether knowledge graph (KG) features can detect action failures more accurately and quickly than text-based methods in TextWorldExpress CookingWorld. Only one experiment was conducted (kg-failure-detection-copy1), which ran in PILOT mode with 25 episodes of 40 steps each. The results strongly refute the hypothesis. The KG-based detector performed poorly with 0% precision, recall, and F1 scores, generating 54 false positives while missing all 7 actual failures. The keyword detector (a text-based baseline) also performed poorly with similar metrics (0% precision/recall/F1, 66 false positives). While the KG implementation appeared functional in tracking object states and relationships, the failure detection logic was likely too sensitive, triggering many false alarms. The experiment deviated from the original plan by omitting the text similarity baseline and bootstrap analysis. Major limitations included lack of tuning for detection thresholds, missing statistical analysis, and a relatively small sample size. Future work should focus on improving the detection algorithms, particularly by calibrating thresholds, implementing more sophisticated graph feature extraction, and conducting more extensive testing with larger sample sizes.	184558464630
kg-state-tracking	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	Knowledge graph representations will improve accuracy in tracking object locations and properties compared to text-only representations in a simplified CookingWorld environment.	3	1	1	mixed information	This meta-analysis examined five pilot experiments testing whether knowledge graph (KG) representations improve state tracking accuracy compared to text-only representations in a simplified CookingWorld environment. Each experiment implemented the same basic design with 10 episodes (5 KG, 5 baseline) of 10 steps each, using seeds 1-5 and a consistent environment setup (2 rooms, 3 objects, no doors, unlimited inventory). Three experiments (copies 1, 3, and 4) showed statistically significant support for the hypothesis, with KG representations outperforming text-only baselines by margins ranging from 2.4 to 36.3 percentage points in overall accuracy. One experiment (copy 5) directly contradicted the hypothesis, with the baseline outperforming KG representations by substantial margins in both location tracking (17.7 percentage points) and property tracking (19.3 percentage points). One experiment (copy 2) showed mixed results, with KG representations performing better for location tracking but worse for property tracking. The inconsistency across experiments suggests that implementation details or random variations may significantly impact performance. The small sample size in each pilot (5 episodes per condition) limits the strength of conclusions. Future work should investigate why KG representations sometimes underperform, particularly for property tracking, and should include larger-scale testing with more episodes and diverse environment configurations. Overall, while the evidence leans toward supporting the hypothesis (3 supporting vs 1 refuting), the contradictory results highlight the need for more robust testing and careful consideration of when and how KG representations provide benefits for state tracking.	751129025440, 170147280802, 893535620307, 527877513335, 711625744194
knowledge-graph-discovery		An agent that explicitly maintains a knowledge graph of its discoveries will perform better at DiscoveryWorld tasks than baseline agents, by having better memory of past experiments and being able to make more informed decisions about what to try next.	0	0	0	no information	No experiments were provided for analysis. The research idea aimed to test whether a knowledge-graph-based agent would outperform a baseline ReAct agent on DiscoveryWorld tasks by maintaining an explicit knowledge graph to track discoveries, relationships, and hypotheses. The planned experiment would have compared performance metrics including task completion rate, process score, and explanatory knowledge score across different DiscoveryWorld tasks. A detailed operationalization plan was created with three pilot modes (MINI_PILOT, PILOT, FULL_EXPERIMENT) of increasing complexity, but no experiment results were provided for meta-analysis. Therefore, no conclusions can be drawn about the original hypothesis that knowledge graph agents would perform better than baseline agents in scientific discovery tasks.	
knowledge-graph-verification	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	An agent exploring and interacting with a text game environment can effectively verify the accuracy of automatically constructed knowledge graphs by attempting to validate individual triples through direct interaction.	2	0	0	limited information	This meta-analysis examined two experiments testing whether a ReAct-based verification agent could effectively verify knowledge graph triples in TextWorldExpress environments. Both experiments strongly support the hypothesis that an agent exploring and interacting with a text game environment can effectively verify the accuracy of automatically constructed knowledge graphs through direct interaction. The experiments were conducted in PILOT mode with 10 episodes each, using environments with 3 rooms and testing 10-20 KG triples per episode with an 80/20 correct/incorrect ratio.  Key findings across both experiments:  1. The verification agent significantly outperformed random exploration in both experiments, with verification rates of 78.9% and 91% compared to baseline rates of 51.4% and 54%, respectively.  2. Statistical significance was established in both experiments (p<0.001), providing strong evidence for the superiority of structured exploration over random interaction.  3. The verification agent demonstrated efficiency in triple verification, requiring substantially fewer steps per verification in the second experiment (5.7 steps vs. 31.2 steps for random exploration).  4. The agent showed consistent performance across episodes, maintaining approximately 80% accuracy in distinguishing correct from incorrect triples.  5. The ReAct architecture proved effective for knowledge graph verification, enabling systematic exploration and reasoning about triple relationships.  These results consistently demonstrate that a structured approach to environment exploration using ReAct reasoning is substantially more effective than random exploration for knowledge graph verification in text environments. The agent's ability to systematically plan verification steps, execute relevant actions, and reason about observations enabled it to efficiently validate or refute knowledge graph triples. This provides compelling evidence that interactive agents can serve as effective tools for verifying automatically constructed knowledge graphs, addressing a key gap in automated KG construction - validation of the generated graphs.	980034375074, 724179332984
knowledge-guided-decomposition		Maintaining and utilizing a knowledge graph of successful decomposition patterns will lead to more efficient task completion compared to making decomposition decisions from scratch each time.	0	0	0	no information	No experiments were provided for analysis. The research idea aimed to investigate whether maintaining and utilizing a knowledge graph of previously successful decompositions can improve an agent's ability to adaptively decompose new tasks in TextWorldExpress environments. The planned experiment would have compared three conditions: (1) a ReAct agent with knowledge graph guidance, (2) a standard ReAct agent without decomposition, and (3) random action selection. The experiment was designed to be run in three possible modes (MINI_PILOT, PILOT, and FULL_EXPERIMENT) with increasing complexity. Metrics to be tracked included task success rate, number of decomposition steps needed, total steps to completion, knowledge graph growth rate, and knowledge graph utilization rate. However, since no experimental results were provided, it is impossible to draw conclusions about whether the hypothesis was supported, refuted, or if the results were inconclusive.	
knowledge-guided-react	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	A ReAct agent that maintains and reasons over a structured knowledge graph of game mechanics and object relationships will perform better than a standard ReAct agent that only uses its prompt context.	0	0	3	limited information	This meta-analysis examined three pilot experiments comparing knowledge-graph-enhanced ReAct agents against standard ReAct agents in TextWorldExpress's CookingWorld environment. All three experiments showed a consistent pattern: the knowledge graph agents achieved slightly higher mean scores (improvements ranging from 0.042 to 0.100 points), but none of these differences reached statistical significance (p-values ranged from 0.17 to 0.322). The experiments varied in their findings regarding efficiency: one found the knowledge graph agent took more steps, another found it took fewer steps, and the third didn't clearly report step differences. The knowledge graphs successfully tracked environmental information (growing from 8-44 nodes and 10-306 edges across episodes), demonstrating that the technical implementation worked as intended. However, this environmental tracking did not translate to statistically significant performance improvements. All experiments used relatively small sample sizes (5-10 episodes), limiting statistical power. Technical challenges were noted in at least one experiment regarding graph parsing and invalid action selection. Overall, these pilot experiments suggest that knowledge graph integration may provide a small benefit to ReAct agents, but the current implementations and sample sizes are insufficient to draw strong conclusions about the hypothesis. A full-scale experiment with larger sample sizes, potentially refined knowledge graph integration, and more complex environments might better reveal whether the approach offers significant advantages.	306282002750, 215236615887, 265407162448
llm-graph-verification	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	LLM-based verification of knowledge graph triples will improve graph accuracy and consistency compared to unverified graphs.	0	2	3	limited information	This meta-analysis examined five experiments testing whether LLM-based verification improves knowledge graph accuracy in text-based games. None of the experiments provided clear support for the hypothesis, while two experiments actively refuted it, and three were inconclusive. A key limitation across all experiments was the lack of ground truth evaluation to directly measure graph accuracy. Instead, most experiments used indirect metrics like graph size, triple removal counts, or game performance.  The experiments consistently showed that LLM verification substantially reduced the number of triples in knowledge graphs (ranging from ~31% reduction to extreme cases where almost all triples were removed). However, this aggressive pruning appears problematic. In experiments 3 and 4, LLM verification actually led to worse game performance, suggesting the verification process may have removed valid and useful information. Experiments 1, 2, and 5 showed similar patterns of substantial triple removal but couldn't conclusively determine if this improved actual graph accuracy.  A consistent pattern across experiments was that LLM verification appeared overly conservative or strict, often removing large numbers of triples without adequate replacement or correction. This suggests potential issues with the verification prompt design or the LLM's tendency to be risk-averse in validation tasks.  Methodological limitations were significant across all experiments: small sample sizes (10 episodes each), short episode lengths (30 steps), low game scores indicating limited progression, and most critically, no direct evaluation against ground truth knowledge. The bootstrap analyses consistently showed no statistically significant improvements from LLM verification (p-values near 1.0).  In conclusion, the evidence from these experiments suggests that the simple LLM verification approach implemented here does not improve knowledge graph accuracy and may actually be detrimental by removing too much information. Future work should include ground truth evaluation, more sophisticated verification prompts that not only identify but also correct problematic triples, and larger sample sizes to increase statistical power.	756642589416, 71487851113, 309369580682, 550981442879, 33046327954
location-graph-cooking	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	An agent that maintains an explicit graph of object locations will complete cooking tasks more efficiently (using fewer steps) than an agent that relies solely on its working memory.	0	0	1	limited information	This meta-analysis examined whether maintaining a graph of object locations improves agent performance in TextWorldExpress cooking games. Only one pilot experiment was conducted with 10 episodes per condition. The experiment used a controlled environment with 3 locations, 2 ingredients, 2 distractor items, no doors, and limited inventory size of 1. The enhanced agent with location tracking achieved a higher mean score (0.508) compared to the baseline ReAct agent (0.471), suggesting a potential advantage. Additionally, the experimental agent demonstrated better spatial awareness by visiting 2-3 rooms per episode, while the baseline had 0 recorded visits. However, these differences were not statistically significant (bootstrap p=0.278), likely due to the small sample size. Both agents encountered similar challenges with inventory management given the size limitation. The results are promising but inconclusive, suggesting that location tracking may improve performance, but a larger-scale experiment with more episodes is needed to establish statistical significance. Future work should include the full experiment with 50 episodes per condition as originally planned, and potentially explore variations in environment complexity to better understand when location tracking provides the most benefit.	696074322377
progressive-state-complexity	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	LLMs will show degrading performance as state complexity increases, with particularly sharp drops when moving from discrete to continuous properties and when adding environment dynamics.	3	2	0	mixed information	This meta-analysis examined five experiments investigating how increasing state representation complexity affects LLM simulation accuracy in TextWorldExpress environments. The experiments tested four levels of complexity: boolean-only, numerical, relational, and full state descriptions, using gpt-4o-mini as the LLM across all conditions.  Three experiments (copies 1, 3, and 5) supported the hypothesis that LLMs show degrading performance as state complexity increases. These experiments demonstrated a general downward trend in prediction accuracy as complexity increased, with particularly notable drops when moving to relational or full state representations. Two experiments (copies 2 and 4) refuted the hypothesis, showing either improved performance with increased complexity or non-monotonic patterns where full state descriptions outperformed simpler representations.  Across all experiments, boolean-only states generally achieved high accuracy (ranging from 20% to 93.2%, with most above 70%). The performance on numerical states was more variable (50.3% to 90.6%). Relational states showed the most inconsistent results across experiments (44.3% to 88.3%), while full state descriptions typically performed worse than simpler representations in most experiments (54.1% to 76.1%).  The inconsistency between experiments suggests that factors beyond just state complexity may influence LLM prediction accuracy. These could include differences in the specific state transitions encountered, the quality of the LLM's responses to particular prompts, or variations in how the state representations were constructed and presented to the model.  Despite these inconsistencies, the majority of experiments (3 out of 5) support the hypothesis that increasing state complexity generally degrades LLM prediction accuracy. However, the specific pattern of degradation and the relative difficulty of different complexity levels varied across experiments, suggesting that the relationship between state complexity and LLM performance is nuanced and may depend on additional factors not fully controlled for in these experiments.	819212579636, 905239919718, 952869129509, 661784663743, 382678254001
react-knowledge-retrieval	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	A ReAct agent with ConceptNet knowledge retrieval will perform better than a standard ReAct agent on TextWorldExpress common sense tasks by having access to basic object relationships.	0	0	1	limited information	This meta-analysis examined whether adding ConceptNet knowledge retrieval to a ReAct agent improves performance on TextWorldExpress common sense tasks. Only one experiment was conducted, comparing a standard ReAct agent against a knowledge-augmented ReAct agent across 10 episodes in PILOT mode. The knowledge-augmented agent achieved a marginally higher success rate (90% vs. 80%) but identical average score (0.95) compared to the baseline agent. However, bootstrap statistical analysis revealed no significant difference between the two agents (p=0.5502), making the results inconclusive. The high baseline performance suggests that the underlying LLM (gpt-4o-mini) already possessed strong common sense reasoning capabilities, potentially masking any benefits from the additional ConceptNet knowledge. The experiment was implemented with proper methodology, but the small sample size (10 episodes) and high baseline performance limit the strength of conclusions that can be drawn. Future work should consider larger sample sizes, more challenging tasks that specifically require external knowledge, or using less capable base models where the knowledge augmentation might show clearer benefits.	404231316820
react-pattern-learning		A ReAct agent that stores and reuses successful reasoning patterns from past experiences will perform better on similar tasks compared to a standard ReAct agent that reasons from scratch each time.	0	0	0	no information	No experiments were provided in the input data. The experiments array was empty, making it impossible to conduct a meta-analysis of the research question investigating whether a ReAct agent with pattern reuse capabilities outperforms a standard ReAct agent on TextWorldExpress cooking tasks. While a detailed operationalization plan was created to test this hypothesis through various pilot modes (MINI_PILOT, PILOT, and FULL_EXPERIMENT), no actual experimental results were included in the provided data. Therefore, no conclusions can be drawn regarding the effectiveness of pattern-based reasoning reuse in ReAct agents.	
reactive-graph-confidence	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	Using ReAct to explicitly reason about confidence in graph updates will result in more accurate belief graphs compared to direct updates without confidence reasoning.	0	4	0	consistent (refute)	This meta-analysis examined four experiments testing whether using the ReAct framework with explicit confidence reasoning improves belief graph accuracy in CookingWorld compared to direct updates without confidence reasoning. All four experiments consistently refuted the hypothesis. In each case, the confidence-based ReAct agent either performed worse than or equal to the baseline agent that updated graphs directly without confidence reasoning. Specifically, in copy1, the baseline agent achieved 50.8% accuracy versus 43.4% for the confidence agent. In copy2, both agents showed identical (poor) performance with mean scores of 0.0. In copy4, the baseline agent achieved 70.6% accuracy versus 67.0% for the confidence agent. In copy5, both agents showed identical low accuracy of approximately 9%. While statistical significance was not reached in most comparisons (p-values ranged from 0.559 to 1.0), the consistent pattern across all experiments strongly suggests that explicit confidence reasoning does not improve graph accuracy in this context. In fact, the confidence-based agent tended to be more conservative in making updates (as noted in copy2), which may have contributed to its equal or worse performance. All experiments were conducted as pilot studies with 10 episodes of 25 steps each, using simplified CookingWorld settings. The consistent results across different implementations strengthen the conclusion that, contrary to the hypothesis, explicit confidence reasoning does not improve belief graph accuracy in this environment. Future work might explore whether confidence reasoning could be beneficial in more complex environments or with different implementations of confidence thresholding and filtering mechanisms.	270983721927, 621240987094, 332849553745, 582071109988
resistor-substitution-advisor	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	GPT-4 can successfully suggest viable resistor substitutions using standard-value resistor combinations that match target specifications within 5% tolerance.	0	4	1	consistent (refute)	This meta-analysis examined five experiments testing whether GPT-4 can successfully suggest viable resistor substitutions using standard-value resistor combinations that match target specifications within 5% tolerance. The evidence overwhelmingly refutes this hypothesis. Four experiments clearly demonstrated that the LLM approach failed to consistently meet the 5% tolerance threshold, with one experiment being inconclusive.  Across experiments, several consistent limitations of the LLM approach were observed:  1. Calculation errors: The LLM frequently made mathematical errors when calculating resistance values, particularly for parallel combinations.  2. Invalid suggestions: Multiple experiments reported that the LLM suggested resistor values that weren't part of the specified standard series (E12).  3. Inconsistent performance: Success rates at the 5% threshold ranged from as low as 36.4% to at best 90% (for valid responses only).  4. High error rates: Mean errors ranged from 2.15% to 40.27%, with several experiments reporting instances of extremely high errors (>90%).  5. Reliability issues: In one experiment, the LLM only produced valid responses for 10 out of 20 cases.  In contrast, the mathematical optimization baseline consistently outperformed the LLM approach across all experiments, achieving near-perfect results with mean errors as low as 0.14-0.18% and 100% success rates at all error thresholds in most cases.  While the LLM showed some ability to engage with the resistor combination task and occasionally produced good solutions, it lacked the consistency, reliability, and mathematical precision required for this application. The evidence suggests that traditional optimization methods remain superior for resistor substitution tasks, and the original hypothesis that GPT-4 can successfully suggest viable resistor substitutions within 5% tolerance is not supported.	42733426357, 874481223057, 571735205787, 260958977013, 231750637947
rule-guided-action-validation		Using basic cooking domain rules to filter action selections will improve the rate of valid actions and task completion compared to unfiltered random selection.	0	0	0	no information	No experiment results were provided for analysis. The research idea aimed to evaluate whether simple cooking rules could improve action selection validity in TextWorldExpress cooking tasks by comparing rule-filtered action selection against unfiltered random selection. The plan was to implement a rule-based validator with five basic cooking rules and test it on simple cooking tasks. Two agents were to be compared: one using random selection from all actions and another using random selection from filtered valid actions. Performance metrics would include percentage of valid actions and task completion rates. However, since no experiment data was provided, it is impossible to draw any conclusions about whether the hypothesis was supported, refuted, or if the results were inconclusive.	
simple-abstraction-tuning		Automated tuning of abstractions based on their success rates will improve task completion rates compared to static abstractions.	0	0	0	no information	No experiment results were provided for analysis. The research idea aimed to test whether automatically tuning text-based game abstractions based on their success rates in TextWorldExpress cooking tasks would improve task completion rates compared to static abstractions. The planned experiment would have compared a baseline condition using static abstractions against an experimental condition with tuned abstractions, measuring task completion rates and steps taken. However, since no experiment results were included in the provided data, it is impossible to draw any conclusions about the hypothesis. To properly evaluate this research idea, experiments would need to be run according to the operationalization plan, with results recorded and provided for analysis.	
simple-affordance-exploration	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	LLM-guided exploration using simple affordance predictions with success/failure tracking will find successful solutions faster than random exploration in ScienceWorld tasks.	0	1	1	limited information	This meta-analysis examined two experiments testing whether LLM-guided affordance prediction improves exploration efficiency in ScienceWorld tasks compared to random exploration. The original hypothesis specifically predicted that the LLM-guided approach would 'find successful solutions faster' than random exploration. The results present a nuanced picture. In the first experiment (copy4), the LLM-guided agent achieved significantly higher scores on both tasks (thermometer: 11.4 vs 3.0, p<0.001; boiling: 2.1 vs 0.9, p=0.051), demonstrating better task performance. However, it actually took more steps on average to complete tasks (thermometer: 19.1 vs 8.6; boiling: 17.8 vs 11.7), directly contradicting the 'faster' aspect of the hypothesis. The second experiment (copy5) showed the experimental agent significantly outperforming the baseline in overall score (5.15 vs 1.15, p<0.001), but did not provide specific information about steps-to-completion, making it inconclusive regarding the speed hypothesis. Overall, these experiments suggest that while LLM-guided affordance prediction may improve task performance and success rates, it does not necessarily lead to faster solution discovery as originally hypothesized. The experimental agent appears to be more thorough in its exploration, taking more deliberate steps that ultimately lead to higher scores but not necessarily in fewer steps. This suggests a potential trade-off between exploration efficiency (fewer steps) and task effectiveness (higher scores). Future research might focus on optimizing the balance between thorough exploration and efficient task completion.	516607266752, 982809547528
simple-decomposition-memory	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	An agent that stores and reuses successful task decomposition patterns will perform better than an agent that decomposes each task from scratch.	0	1	2	limited information	This meta-analysis examined three experiments testing whether an agent that stores and reuses successful task decomposition patterns performs better than one that decomposes each task from scratch in TextWorldExpress CookingWorld. None of the experiments provided strong support for the hypothesis. One experiment (copy2) directly refuted it, with the experimental agent performing worse than the baseline (0.14 vs 0.26 mean test scores). The other two experiments (copy1 and copy3) showed small, non-significant improvements in the experimental condition but were deemed inconclusive. A consistent finding across all experiments was the low or zero pattern reuse rate, suggesting that while the pattern storage mechanism was implemented successfully, the agents failed to effectively reuse these patterns. This may be attributed to the strict similarity matching threshold (0.7) being too restrictive, insufficient training episodes to build a useful pattern library, or fundamental limitations in the string matching approach to pattern identification. All experiments were conducted in PILOT mode with small sample sizes (5 training and 5 test episodes), limiting statistical power. The meta-analysis suggests that simple string-matching approaches to decomposition memory may be insufficient, and future work should consider more sophisticated pattern matching algorithms, lower similarity thresholds, larger training sets, or alternative approaches to representing and retrieving task decompositions.	77331884086, 518452155619, 880800423906
simple-dual-reflection	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	Sequential two-agent reflection will generate higher quality insights than single-agent reflection, as measured by task performance improvement.	1	2	1	mixed information	This meta-analysis examined four pilot experiments testing whether sequential two-agent reflection generates better insights than single-agent reflection in TextWorldExpress CookingWorld tasks. Each experiment implemented the same basic design with three conditions: baseline (random actions), single-agent reflection, and dual-sequential reflection, with 3 tasks and 5 episodes per task in each condition.  The results across experiments were mixed and largely inconclusive regarding the primary hypothesis. Only one experiment (copy2) showed results supporting the hypothesis, with dual-reflection achieving higher mean scores than single-agent reflection (0.183 vs 0.127), though this difference was not statistically significant (p=0.161). Two experiments (copy1 and copy5) showed results that refuted the hypothesis, with single-agent reflection performing similarly or better than dual-reflection. One experiment (copy4) showed inconclusive results with minimal differences between reflection conditions.  Consistently across all experiments, both reflection conditions tended to outperform the baseline random action condition, suggesting that reflection in general provides some benefit. However, the evidence does not consistently support the specific hypothesis that dual-sequential reflection outperforms single-agent reflection.  Key limitations across all experiments include small sample sizes (15 episodes per condition), which limited statistical power to detect potentially real but modest effects. Additionally, the use of random actions rather than more sophisticated action selection strategies may have constrained the potential benefits of reflection.  In conclusion, while reflection appears beneficial compared to no reflection, the current evidence does not strongly support the hypothesis that sequential two-agent reflection generates better insights than single-agent reflection in these cooking tasks. Future research should consider larger sample sizes, more sophisticated action selection strategies, and potentially different task domains to further investigate potential benefits of multi-agent reflection.	575571853109, 825533695487, 305488358600, 184411424706
simple-goal-explorer	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	An agent that explicitly tracks predefined cooking-related goal hypotheses will identify game objectives more quickly than random exploration in cooking games.	2	0	0	limited information	This meta-analysis examined two implementations of the 'simple-goal-explorer' experiment, which tested whether an agent that explicitly tracks predefined cooking-related goal hypotheses would identify game objectives more quickly than random exploration in TextWorldExpress cooking games. Both experiments were conducted in PILOT mode with 3 games and 5 episodes each, using LLM-based goal confidence tracking for the experimental agent versus a random action baseline.  The results consistently supported the research hypothesis across both implementations. In the first experiment (copy3), the goal-tracking agent achieved a mean score of 0.183 compared to the random baseline's 0.113, with this difference being statistically significant (p=0.0247). In the second experiment (copy5), the performance gap was even more pronounced, with the goal-tracking agent achieving a mean score of 0.36 versus 0.103 for the random baseline (p<0.001).  Both experiments demonstrated that the goal-tracking agent exhibited more systematic behavior in acquiring necessary items (cookbook, knife, ingredients) and following recipe steps compared to random exploration. The goal-tracking approach consistently enabled the agent to identify and pursue relevant cooking objectives, leading to higher task completion rates.  However, it's worth noting that the absolute performance of both agents was relatively low across experiments (scores well below 0.5), suggesting room for improvement in the goal-tracking strategy. The second implementation (copy5) showed substantially better performance for the goal-tracking agent (0.36 vs 0.183), indicating that implementation details significantly affect the effectiveness of the approach.  In conclusion, this meta-analysis provides strong evidence supporting the hypothesis that explicitly tracking predefined cooking-related goal hypotheses enables more effective game objective identification compared to random exploration in cooking games. Future work could focus on improving the absolute performance of the goal-tracking approach and investigating which specific aspects of goal tracking contribute most to performance improvements.	922278728410, 569868755664
simple-graph-alignment	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	Basic similarity metrics between text descriptions and graph structures can effectively capture the alignment between different representations of the same game state.	2	0	0	limited information	This meta-analysis examined two implementations of experiments testing whether basic similarity metrics can effectively align text descriptions with graph representations of cooking game states in TextWorldExpress. Both experiments were conducted in PILOT mode with 5 games and 10 episodes each, evaluating 50 text-graph pairs.  The first experiment showed mixed results across metrics: word overlap achieved 62% accuracy, significantly outperforming Jaccard (~1.4%) and custom graph-text (~2.7%) metrics. The second experiment reported consistent performance across all three metrics, with each achieving 56% accuracy. Both experiments exceeded the PILOT success criterion of >50% accuracy with at least one metric.  The discrepancy in relative performance between experiments raises questions about implementation differences. In the first experiment, the custom metric implementation was noted as basic, focusing on simple node/edge matching rather than using the proposed GPT-4-mini for relationship extraction. The second experiment's identical performance across all metrics suggests possible methodological issues in the evaluation process.  Despite these inconsistencies, both experiments demonstrate that at least some basic similarity metrics can achieve above-chance alignment between text descriptions and graph representations of game states. The word overlap metric consistently performed well across both experiments, suggesting that even simple lexical matching can capture meaningful alignment between different representations of the same game state.  Future work should address the implementation discrepancies, particularly for the custom graph-text similarity metric, and explore why Jaccard similarity performed so differently between experiments. Additionally, the relationship between similarity scores and game progress, which was part of the original research plan, was not clearly reported in either experiment's results summary.	75961913807, 779705271268
simple-graph-cooking-simulation		Providing a pre-built knowledge graph of cooking relationships as additional context will improve an LLM's ability to predict valid cooking actions in CookingWorld tasks.	0	0	0	no information	No experiments were provided in the input data. The research idea aimed to investigate whether maintaining a simple, static knowledge graph of cooking relationships in CookingWorld would improve an LLM's ability to predict valid cooking actions. The plan was to compare a baseline condition (using gpt-4o-mini without graph context) against an experimental condition (using gpt-4o-mini with relevant subgraph information). The experiment was designed to be run in three possible modes (MINI_PILOT, PILOT, and FULL_EXPERIMENT) with increasing scope. However, since no experiment results were provided, it is impossible to draw any conclusions about whether the hypothesis was supported, refuted, or if the results were inconclusive. To properly evaluate the hypothesis, the experiments would need to be run and their results analyzed.	
simple-graph-state-tracking	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	An agent maintaining even a simple graph-based representation of object locations and states will complete cooking tasks more efficiently than an equivalent agent without state tracking.	0	0	1	limited information	This meta-analysis examined whether a graph-based state tracking system improves agent performance in text-based cooking environments. Only one experiment was conducted, comparing a baseline ReAct agent against a graph-tracking ReAct agent in CookingWorld's sandwich-making task. The experiment ran in PILOT mode with 10 episodes per condition.  The results showed promising trends favoring the graph-tracking agent: it achieved a higher average score (0.537 vs 0.311), successfully completed more episodes (3/10 vs 0/10), and required fewer steps on average (14.8 vs 20.0). These metrics align with the hypothesis that graph-based state tracking improves task efficiency. However, bootstrap analysis revealed these differences were not statistically significant (p=0.151), likely due to the small sample size.  Both agent types struggled with exploration and sequential reasoning, often getting stuck in loops or failing to find ingredients. This suggests fundamental limitations in the base ReAct architecture that persist regardless of state tracking capabilities.  While the experiment shows promising directional support for the hypothesis, the lack of statistical significance and limited sample size necessitate classifying the results as inconclusive. A full experiment with the planned 100 episodes per condition would provide more definitive evidence. Future work should also address the underlying exploration and sequential reasoning challenges observed in both agent types.	549971431395
simple-hierarchical-beliefs		A two-level hierarchical belief structure will lead to more organized and complete knowledge representation compared to a flat belief structure, as measured by graph coverage of temperature-related relationships.	0	0	0	no information	No experiments were provided for analysis. The research idea aimed to investigate whether a simple two-level hierarchical belief structure could improve an agent's ability to learn and represent temperature-related relationships in ScienceWorld compared to a flat belief structure. The planned experiment would have compared agents with hierarchical versus flat belief structures on metrics including graph coverage of temperature relationships and task success rates. However, since no experiment results were provided, no conclusions can be drawn regarding the hypothesis that a two-level hierarchical belief structure leads to more organized and complete knowledge representation compared to a flat belief structure.	
simple-memory-pruning		Frequency-based memory pruning (removing least-used memories) will lead to better task performance than time-based pruning (removing oldest memories) in temperature-related tasks.	0	0	0	no information	No experiment results were provided for analysis. The research was designed to compare time-based versus frequency-based memory pruning strategies in a ScienceWorld agent on temperature-related tasks, specifically boiling water. The experiment was planned to include additional baselines of random memory pruning and no memory pruning, with metrics including task success rate, average steps to completion, and number of memory retrievals. However, without any experimental data, it is impossible to determine whether frequency-based memory pruning leads to better performance than time-based pruning as hypothesized, or to draw any other conclusions about the relative effectiveness of different memory management strategies in this context.	
simple-meta-graphs	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	A ReAct agent using simple knowledge graphs to track its past performance on specific task states will make more efficient mode-switching decisions compared to using random or fixed strategies.	1	1	1	limited information	This meta-analysis examined three experiments testing whether a ReAct agent using knowledge graphs to track past performance could make more efficient mode-switching decisions compared to random or fixed strategies on ScienceWorld classification tasks. The results across experiments were mixed. One experiment (copy5) strongly supported the hypothesis, showing the knowledge graph approach achieving a 100% success rate and significantly outperforming both random and always-detailed modes. However, another experiment (copy4) refuted the hypothesis, with the knowledge graph approach performing significantly worse than the always-detailed mode, suggesting the hypothesized benefits of dynamic mode switching were not realized. The third experiment (copy3) yielded inconclusive results, with the knowledge graph approach performing better than random and quick modes but worse than the detailed mode, while showing promise in balancing performance and efficiency. All experiments were conducted as pilot studies with only 5 episodes per condition, limiting the strength of conclusions. The inconsistency across experiments suggests that the effectiveness of knowledge graph-based mode switching may be sensitive to implementation details or task characteristics. Future research should include larger sample sizes, examine different task types, and investigate refinements to the knowledge graph approach to better understand when and how it can improve agent performance.	903594503543, 698783614562, 992170823870
simple-metaphor-graph	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	An LLM can identify meaningful metaphorical relationships between objects in TextWorldExpress CookingWorld scenarios based on their functional similarities, and these relationships can be effectively visualized in a knowledge graph format.	3	0	1	mixed information	This meta-analysis examined four experiments testing whether an LLM can identify meaningful metaphorical relationships between objects in TextWorldExpress CookingWorld scenarios based on functional similarities, and visualize these in a knowledge graph format. Three experiments provided support for the hypothesis, while one was inconclusive.  All experiments successfully implemented the planned methodology, using gpt-4o-mini to detect functional similarities between object pairs in cooking scenarios and visualizing these relationships in graph format. The experiments consistently found substantial numbers of metaphorical relationships (ranging from 6.2 to 84.4 per scenario) that were generally coherent and contextually appropriate for cooking environments.  The experiments demonstrated that the LLM could identify plausible functional metaphors (e.g., 'Both provide heat for cooking food' between oven and stove) while sometimes appropriately responding 'None' for pairs without clear functional relationships. The relationships were successfully visualized as edges in knowledge graphs, fulfilling the second part of the hypothesis.  However, some methodological limitations were noted. One experiment (copy4) found metaphorical relationships between 100% of co-occurring object pairs, raising questions about the discriminative power of the metaphor detection system. Additionally, while random baselines were implemented, detailed comparative analyses were limited. Most importantly, none of the experiments included the planned human evaluation of relationship quality, which was specified in the original research idea.  The high proportion of detected relationships across experiments (approaching 100% in some cases) suggests either strong interconnectedness of kitchen objects or potential over-generalization by the LLM. Without human evaluation to validate the quality of these relationships, it's difficult to fully assess whether all detected relationships were truly meaningful.  Despite these limitations, the weight of evidence across the four experiments supports the hypothesis that an LLM can identify meaningful metaphorical relationships between objects based on functional similarities and visualize these in a knowledge graph format. Future work should incorporate human evaluation of relationship quality to strengthen these findings.	598435068634, 438854630471, 944997250584, 210726655021
simple-planning-agent	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	A simple planning agent that breaks tasks into 2-3 sequential steps will perform better at basic cooking tasks compared to a baseline agent that attempts to achieve goals without planning.	0	0	1	limited information	This meta-analysis examined whether a planning agent that breaks tasks into 2-3 sequential steps performs better at basic cooking tasks compared to a non-planning baseline agent. Only one experiment was conducted in the PILOT mode with 10 episodes in the CookingWorld environment. The planning agent achieved a mean score of 0.394 compared to the ReAct baseline's 0.382 and random agent's 0.156. While the planning agent did show a slight numerical advantage over the ReAct baseline, bootstrap resampling analysis revealed this difference was not statistically significant (p=0.463). Both planning and ReAct agents substantially outperformed the random baseline. The planning agent exhibited high variance in performance (scores ranging from 0.111 to 1.0), suggesting potential instability in the planning approach. The experiment's small sample size (10 episodes) significantly limits its statistical power to detect meaningful differences between the planning and ReAct agents. To draw more definitive conclusions about the hypothesis, additional experiments with larger sample sizes would be necessary. Based on the available evidence, the results are deemed inconclusive regarding the original hypothesis.	262922729309
simple-property-verification		ConceptNet's temperature and state-of-matter properties for common objects contain inaccuracies that can be identified through systematic environmental interaction	0	0	0	no information	No experiments were provided in the input data. The experiments array was empty, making it impossible to conduct a meta-analysis of the research investigating whether ConceptNet's temperature and state-of-matter properties for common objects contain inaccuracies that can be identified through systematic environmental interaction. While a detailed research idea and operationalization plan were provided, no actual experiment results were included for analysis. To properly evaluate the hypothesis, experiments would need to be run comparing ConceptNet predictions against observations in ScienceWorld for various objects' temperature and state-of-matter properties.	
simple-self-evaluation	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	A ReAct agent with single-step self-evaluation will achieve higher task completion rates compared to a standard ReAct agent in TextWorldExpress CookingWorld tasks.	1	0	1	limited information	This meta-analysis examined two experiments testing whether adding a single layer of self-evaluation to a ReAct agent improves performance in TextWorldExpress CookingWorld tasks. Both experiments used the same experimental design with three conditions: a self-evaluating ReAct agent, a standard ReAct agent, and a random baseline, with 25 episodes per condition in PILOT mode. The results consistently showed that the self-evaluating ReAct agent outperformed both the standard ReAct agent and the random baseline across multiple metrics, including average score, task completion rate, and average steps to completion. In the first experiment, the self-evaluating agent achieved a mean score of 0.22 (vs 0.168 for standard ReAct) and was the only agent to complete any tasks (1/25 episodes), though these differences were not statistically significant (p=0.164). In the second experiment, the self-evaluating agent showed more substantial improvements with a higher average score (0.447 vs 0.325), higher task completion rate (28% vs 20%), and fewer steps on average (20.0 vs 28.8), with results trending toward statistical significance (p=0.072). While one experiment provided inconclusive evidence and the other provided supporting evidence for the hypothesis, the consistent direction of the effects across both experiments suggests that adding self-evaluation capabilities to ReAct agents likely improves their performance in cooking tasks. However, the relatively small sample sizes (25 episodes per condition) and the fact that only one experiment approached statistical significance indicate that more extensive testing with larger sample sizes would be beneficial to strengthen these conclusions.	557031903027, 694282057916
simple-social-graphs	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	An agent using a basic knowledge graph to track character relationships (friend/neutral/enemy) will make more socially appropriate decisions compared to an agent without relationship tracking.	3	1	1	mixed information	This meta-analysis examined five experiments testing whether agents using knowledge graphs to track social relationships make more appropriate decisions than agents without such tracking. The experiments implemented three agent types: experimental (with relationship graph), baseline (no graph), and static baseline (assumes friendly relationships).  Three experiments (copies 2, 3, and 4) supported the hypothesis, showing the experimental agent consistently outperformed the baseline agent without relationship tracking. These experiments demonstrated that relationship-aware agents made more contextually appropriate decisions, particularly in scenarios with conflicting relationships.  One experiment (copy1) refuted the hypothesis, with the experimental agent (7.96) scoring lower than both the baseline (9.0) and static baseline (8.84). This experiment suggested that relationship tracking led to overly cautious, exclusive decisions that were rated less favorably than more inclusive strategies.  One experiment (copy5) produced inconclusive results, as the experimental agent (7.992) performed significantly better than the baseline agent (7.604) but worse than the static baseline (9.072). This suggests that while relationship tracking may improve decision-making compared to no tracking, simply assuming friendly relationships might be even more effective in some contexts.  Key limitations across experiments include small sample sizes, limited scenario complexity, potential evaluation bias from using the same LLM for decisions and evaluations, and inconsistent implementation of evaluation metrics. The mixed results suggest that the effectiveness of relationship tracking may depend on specific implementation details, evaluation criteria, and scenario characteristics.  Overall, while the majority of experiments support the hypothesis that relationship tracking improves social decision-making compared to no tracking, the evidence is not unanimous. The surprising effectiveness of the static baseline (assuming friendly relationships) in some experiments suggests that the optimal approach may depend on the social context and evaluation criteria. Future research should explore more complex relationship models, diverse scenarios, and independent evaluation methods.	116835969090, 775832629991, 988368294357, 502207667732, 696173824715
simple-task-composition	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	An LLM that explicitly decomposes tasks into two-step sequences will perform better on cooking tasks than an LLM that approaches tasks end-to-end.	1	0	0	limited information	This meta-analysis examined whether explicit task decomposition improves LLM performance on two-step cooking tasks in TextWorldExpress's CookingWorld environment. The analysis is based on a single experiment that compared three agents: (1) a decomposition agent that breaks tasks into two steps before execution, (2) an end-to-end agent that approaches tasks directly, and (3) a random baseline agent. The experiment was conducted in PILOT mode with 20 episodes of 25 max steps each across 5 different tasks.  The results provide support for the original hypothesis that decomposition improves performance. The decomposition agent achieved a significantly higher average score (0.178) compared to both the end-to-end agent (0.0) and random agent (0.025), with bootstrap analysis confirming statistical significance (p < 0.001). This indicates that breaking down tasks into component steps does provide an advantage in navigating complex environments and making progress toward goals.  However, an important limitation was observed: despite the decomposition agent's superior performance, no agent achieved complete task success (all had 0.0 success rates). This suggests that while decomposition provides a measurable advantage in partial task completion, the overall two-step cooking tasks remained challenging for all approaches. The decomposition strategy appears to help the agent make more progress toward the goal, even if it doesn't lead to full task completion within the constraints of the experiment.  These findings suggest that explicit decomposition is a beneficial strategy for LLMs tackling multi-step tasks, even when complete success remains elusive. Future research could explore extending the maximum steps allowed, refining the decomposition prompts, or investigating whether the benefits of decomposition scale with task complexity. The experiment provides valuable evidence that structured approaches to problem-solving can enhance LLM performance in interactive environments, even when the tasks remain challenging overall.	68919090063
simple-task-reflection		An agent that has access to its past successful experiences when reflecting on failures will generate more effective reflections and show faster improvement compared to an agent that reflects without access to past experiences	0	0	0	no information	No experiment results were provided for analysis. The research idea aimed to investigate whether providing an agent with its own past successful experiences on similar tasks can improve its reflection process in TextWorldExpress cooking tasks. The plan was to compare a baseline ReAct agent against an experience-augmented ReAct agent across different pilot modes (MINI_PILOT, PILOT, and FULL_EXPERIMENT). However, since no experiment results were included in the data provided, it is impossible to draw any conclusions about the hypothesis. A proper meta-analysis would require the actual results from the experiments, including success rates, steps to completion, and number of attempts needed for both the baseline and experimental agents.	
simple-template-discovery		Frequently occurring action sequences from successful trajectories can serve as effective templates to improve agent performance in similar tasks.	0	0	0	no information	No experiments were provided in the input data. The experiments array was empty, making it impossible to conduct a meta-analysis of the template discovery system. The original research idea aimed to develop a frequency-based method to identify common action patterns in successful TextWorldExpress CookingWorld trajectories and evaluate whether using these patterns as templates would improve agent performance. However, without experimental results, no conclusions can be drawn regarding the hypothesis that frequently occurring action sequences from successful trajectories can serve as effective templates to improve agent performance in similar tasks.	
simple-template-discovery		Using automatically discovered two-action templates will improve agent performance compared to using only primitive actions.	0	0	0	no information	No experiments were provided in the input data. The experiments array was empty, making it impossible to conduct a meta-analysis of the research question investigating whether automatically discovering and using simple action templates can improve agent performance in TextWorldExpress CookingWorld games. While a detailed research idea and operationalization plan were provided, without actual experiment results, no conclusions can be drawn regarding the hypothesis that using automatically discovered two-action templates improves agent performance compared to using only primitive actions.	
simulation-confidence-analysis	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	LLM confidence scores will correlate with prediction accuracy, allowing for identification of potentially incorrect predictions.	3	2	0	mixed information	This meta-analysis examined five experiments testing whether LLM confidence scores correlate with prediction accuracy in TextWorldExpress CookingWorld environments. Three experiments supported the hypothesis by finding weak but statistically significant positive correlations (r=0.287, r=0.348, r=0.351) between confidence and accuracy. Two experiments refuted the hypothesis, finding either a very weak non-significant correlation (r=0.043, p=0.331) or a very weak negative correlation (r=-0.035, p=0.448). All experiments revealed a consistent pattern of significant overconfidence, with the LLM (gpt-4o-mini) maintaining high mean confidence scores (ranging from 76.19 to 94.24) despite relatively low mean accuracy rates (ranging from 0.012 to 0.416). This systematic miscalibration between confidence and accuracy suggests that while there may be a weak positive correlation in most cases, raw confidence scores from this LLM are not reliable indicators for identifying potentially incorrect predictions in practice. The variability in results across experiments (with correlations ranging from -0.035 to 0.351) also indicates that the relationship between confidence and accuracy may be sensitive to specific implementation details or environmental parameters. Overall, while the hypothesis receives more support than refutation, the practical utility of confidence scores for identifying incorrect predictions appears limited due to the LLM's persistent overconfidence bias.	862444975485, 121813262920, 619139286928, 973539893561, 352865338061
static-knowledge-comparison	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	LLM-derived task-specific knowledge will lead to better agent performance compared to general knowledge from ConceptNet, due to its ability to provide more contextually relevant information.	0	1	0	limited information	This meta-analysis examined the effectiveness of different static knowledge injection methods (ConceptNet vs. LLM) in ScienceWorld tasks, specifically focusing on a water boiling task. The original hypothesis posited that LLM-derived task-specific knowledge would outperform general knowledge from ConceptNet due to its contextual relevance. However, the experimental results strongly refute this hypothesis. The experiment was conducted in PILOT mode with 10 episodes per condition and 20 max steps per episode. Surprisingly, the baseline ReAct agent with no knowledge injection performed best with a 90% success rate and average reward of 2.5. The ConceptNet condition achieved an 80% success rate (reward 2.2), while the LLM condition performed worst at only 60% success rate (reward 1.8). The random selection between knowledge sources achieved an 80% success rate (reward 2.1). Bootstrap analysis revealed no statistically significant differences between conditions (all p > 0.82). These findings suggest that for relatively simple physical tasks like boiling water, the ReAct architecture alone may be sufficient, and additional knowledge injectionregardless of sourcemay actually be distracting or irrelevant. The experiment indicates that the value of knowledge injection is task-dependent, and for straightforward tasks with intuitive steps, a well-designed agent architecture may be more important than specialized knowledge. Future research should explore more complex tasks where specialized knowledge might provide greater benefits, and investigate whether different knowledge integration methods could improve performance.	208890681507
subgoal-quality-evaluation	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	Using an LLM to evaluate and filter generated subgoals will lead to better task performance compared to using unfiltered subgoals.	1	1	1	limited information	This meta-analysis examined three experiments testing whether LLM-based subgoal filtering improves hierarchical agent performance in TextWorldExpress tasks. The results were mixed across experiments. In one experiment, filtering clearly harmed performance, with the filtered agent achieving 0% success rate compared to 10% for the unfiltered agent. This experiment had a notably permissive filtering mechanism that only rejected 2.4% of generated subgoals. In contrast, another experiment showed modest benefits from filtering, with the filtered agent achieving higher completion rates (40% vs 30%) and better scores (0.625 vs 0.5) while filtering out 24% of subgoals. The third experiment showed a higher success rate for the filtered agent (50% vs 40%) but no statistically significant differences in performance metrics for successful episodes, making its results inconclusive. A key observation across experiments is that filtering effectiveness appears correlated with the proportion of subgoals filtered - when the filter was more selective (filtering 24-33% of subgoals), performance tended to improve or stay similar, while overly permissive filtering (2.4%) was associated with worse performance. All experiments used small sample sizes (10 episodes each) in PILOT mode, limiting statistical power. The mixed results suggest that LLM-based subgoal filtering can potentially improve performance, but its effectiveness depends on implementation details, particularly the stringency of the filtering criteria. Future work should investigate optimal filtering thresholds, use larger sample sizes, and explore more sophisticated evaluation criteria beyond the 1-5 scale used in these experiments.	59102029926, 441039178463, 111930527078
template-world-generation	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	Template-based generation with controlled object variation can create playable single-room environments that are as engaging as manually designed environments.	2	0	0	limited information	This meta-analysis examined two experiments testing whether template-generated text game environments are as playable as manually designed ones, using a ReAct agent's performance as an evaluation metric. Both experiments were run in PILOT mode with 5 template-generated and 5 manual environments, with 10 episodes per environment type.  The first experiment (copy1) showed identical perfect performance (100% success rate) for both template and manual environments, with nearly identical average steps to completion (1.02 vs 1.0 steps). Bootstrap analysis confirmed no significant difference between conditions (p=1.0). However, the perfect performance suggests the environments may have been too simple to differentiate performance.  The second experiment (copy3) introduced more complexity and showed that manually-designed environments had a slightly higher success rate (74% vs 66%) and required fewer steps on average (9.68 vs 11.06 steps). Importantly, bootstrap analysis revealed these differences were not statistically significant for success rate (p=0.954) and only approached significance for steps required (p=0.069).  Collectively, these results support the hypothesis that template-generated environments can be as playable as manually designed ones. The second experiment provides stronger evidence due to its more challenging environments that better differentiated performance. While template environments may be slightly more challenging in terms of solution efficiency, the lack of statistically significant differences suggests that automated template-based generation is a viable approach for creating playable text game environments.  Limitations include: (1) relatively small sample sizes, (2) testing with only one agent type (ReAct), (3) focus on simple single-room environments, and (4) limited metrics for measuring 'engagement' beyond task completion. Future work should explore more complex environments, diverse agent types, and additional metrics for assessing engagement and playability.	781995381541, 729956710789
textworld-subgoal-planning		Breaking down complex cooking tasks into subgoals before execution will lead to higher success rates and more efficient solutions compared to direct planning.	0	0	0	no information	No experiments were provided in the input data. The research was designed to test whether a subgoal-based planning approach would outperform direct planning in TextWorldExpress cooking tasks. The experimental design included three agent types (subgoal-based ReAct agent, direct ReAct agent, and random agent baseline) and planned to measure performance across different recipe complexities (1-3 ingredients). The experiment was structured to progress from a mini-pilot (2 episodes per condition) to a pilot (25 episodes) and eventually a full experiment (100 episodes). Metrics to be collected included average score, steps taken, task completion rate, and time efficiency. However, since no experiment results were provided, no conclusions can be drawn about the validity of the hypothesis.	
two-level-cooking-planner	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	A two-level planner that separates recipe planning from action execution will solve cooking tasks more efficiently than a flat planner or random baseline.	0	0	1	limited information	This meta-analysis examined one experiment testing whether a two-level planner for TextWorldExpress cooking tasks would outperform baseline approaches. The experiment successfully implemented a two-level planning system that combined high-level recipe planning with low-level action execution using GPT-4o-mini. The results clearly demonstrated that the two-level planner significantly outperformed a random action baseline, achieving a mean score of 0.346 compared to the random baseline's 0.140 (p<0.001). The experimental agent showed particular strength in task completion, with some episodes achieving scores of 0.625-0.750, while the random baseline rarely exceeded 0.25. However, the experiment deviated from the original specification by not implementing the single-level ReAct baseline, which was a critical component for testing the full hypothesis. This omission makes it impossible to determine whether the two-level planning approach is superior to a flat planning approach that uses the same underlying LLM capabilities. While the results support the claim that structured planning is better than random actions (an expected outcome), they are inconclusive regarding the specific benefits of hierarchical planning compared to flat planning. Future experiments should include the single-level ReAct baseline to properly evaluate the complete hypothesis and determine whether the added complexity of a two-level planning system provides meaningful performance benefits over a simpler flat planning approach.	872173267686
two-level-discovery-agent		A two-level hierarchical agent that separates planning from execution will perform better on measurement-based discovery tasks than a non-hierarchical baseline.	0	0	0	no information	No experiment results were provided for analysis. The research idea proposed testing whether a two-level hierarchical agent that separates planning from execution would perform better on measurement-based discovery tasks than a non-hierarchical baseline. The plan was to implement a high-level planner using an LLM to generate measurement plans and a low-level executor to convert these plans into actions. The experiments were intended to be run in DiscoveryWorld environments with tasks focused on measurements, comparing the hierarchical agent against baseline agents including a standard ReAct agent and a flat version of the hierarchical agent. However, since no experimental data was provided, no conclusions can be drawn regarding the hypothesis.	
two-stage-game-generation	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	A two-stage game generation approach (basic mechanics first, then scoring/win conditions) will result in higher technical validity compared to generating complete games in one pass	1	0	4	limited information	This meta-analysis examined five experimental runs comparing single-stage versus two-stage approaches for generating text-based adventure games using LLMs. The hypothesis being tested was that a two-stage approach (generating basic mechanics first, then adding scoring/win conditions) would result in higher technical validity compared to generating complete games in one pass.  Across all five experiments, both approaches demonstrated surprisingly high success rates. In four of the five experiments, both approaches achieved perfect 100% success rates across all technical validity metrics, including execution success, presence of required mechanics (movement, inventory, scoring, win conditions), and absence of syntax errors. These experiments were deemed inconclusive as they showed no detectable differences between the approaches.  Only one experiment (copy4) showed a slight advantage for the two-stage approach, where it achieved 100% implementation of all required mechanics compared to 90% for the single-stage approach (which had one game missing inventory commands). This provides weak support for the hypothesis.  The consistently high success rates across both approaches suggest several possibilities: (1) the task may have been too simple to differentiate between approaches, (2) the model used (gpt-4o-mini) may be sufficiently capable that the incremental approach offers no significant advantage for this particular task complexity, or (3) the evaluation metrics may not have been sensitive enough to detect qualitative differences between the approaches.  Limitations of this meta-analysis include the small sample size (5 games per condition in each experiment), focus on basic functionality rather than more nuanced quality metrics, and potential ceiling effects due to the high success rates. Future research should consider more complex game requirements, more sensitive evaluation metrics that go beyond basic functionality checks (such as code quality, modularity, or playability assessments), and larger sample sizes to better detect potential differences between the approaches.	133884568533, 368745113254, 918929073267, 235313993153, 537524286158
wordnet-cooking-exploration	my-benchmark-run-full50-noexpertnotes-5variations-commonlibraryideator-sonnet35-2025-02-24-09-51-31	Using WordNet's hypernym/hyponym relationships to identify food-related objects will lead to more efficient exploration in CookingWorld cooking tasks compared to random exploration.	0	0	1	limited information	This meta-analysis examined whether WordNet's semantic hierarchies can improve exploration efficiency in CookingWorld cooking tasks. The single experiment conducted in PILOT mode (20 episodes per condition) showed some promising trends favoring the WordNet-guided approach over random exploration. The WordNet agent achieved a higher average score (0.28 vs 0.21) and required fewer steps on average (32.0 vs 42.8), with approximately 51% of its actions being food-related. However, these differences did not reach statistical significance (p=0.11), and critically, neither agent achieved any successful task completions (0% success rate for both). The experiment was properly implemented according to specifications, with appropriate logging, analysis, and visualization. The results suggest that while WordNet guidance may provide some benefit in action selection efficiency, the advantage wasn't sufficient to achieve task completion in the tested environment configuration (2 locations, 2 ingredients). Given the lack of statistical significance and successful completions, the evidence is deemed inconclusive regarding the original hypothesis. Further experimentation with larger sample sizes, longer episodes, or modified environment parameters might be necessary to draw more definitive conclusions about the efficacy of WordNet-guided exploration in cooking tasks.	96565194059
