experiment_name	batch_names	hypothesis	support_hypothesis_count	refute_hypothesis_count	inconclusive_hypothesis_count	categorization	detailed_summary	experiment_ids
action-outcome-tracking	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	Agents that maintain explicit records of their action outcomes will achieve higher success rates than baseline agents by avoiding previously failed actions and preferring previously successful ones.	2	2	1	mixed information	This meta-analysis examined five implementations of an experiment testing whether agents with action outcome tracking outperform baseline agents in text-based games. The experiments were conducted in TextWorldExpress's CookingWorld environment with 10 episodes per agent in PILOT mode.  The results across the five experiments were mixed, with two supporting the hypothesis, two refuting it, and one being inconclusive. All experiments consistently showed that both ReAct variants significantly outperformed random agents, confirming the value of structured reasoning in text games.  When comparing the ReAct+ActionTracking agent to the ReAct baseline: - Mean scores: Three experiments showed modest improvements with action tracking (0.485 vs 0.469, 0.425 vs 0.406, 0.54 vs 0.47), while two showed worse performance (0.420 vs 0.460, 0.369 vs 0.455). - Success rates: Two experiments showed improved success rates with tracking (20% vs 10% in both cases), while the other three had identical success rates between the variants (0% in two cases, not specified in one). - Action repetition: All experiments that reported this metric showed reduced repetition with action tracking, suggesting more efficient exploration.  None of the experiments found statistically significant differences between the two ReAct variants, likely due to the small sample size (10 episodes). The most promising results came from experiments 2 and 5, where the tracking agent achieved both higher scores and double the success rate of the baseline.  The inconsistency across experiments suggests that implementation details matter significantly. The action tracking mechanism appears to help reduce repetitive actions, but this efficiency doesn't always translate to better task completion. The hypothesis receives partial support, but the evidence indicates that simply tracking action outcomes isn't sufficient to consistently improve performance in complex sequential decision-making tasks.  Future work should focus on running the full 50-episode experiments to achieve statistical power, refining how context similarity is determined when consulting action history, and potentially combining action tracking with other enhancements to agent reasoning.	715131414163, 735222549881, 381380443479, 482473398010, 967326509235
basic-confidence-simulation	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	LLMs can meaningfully predict their confidence in action outcomes in CookingWorld, with higher confidence scores correlating with higher prediction accuracy.	3	1	1	mixed information	This meta-analysis examined five implementations of a confidence-based prediction system for TextWorldExpress's CookingWorld environment, testing whether GPT-4o-mini could meaningfully predict action outcomes and assign calibrated confidence scores. Across experiments, the LLM consistently achieved prediction accuracies between 68-78%, outperforming random baselines (44-56%) but generally underperforming constant 'always predict success' baselines (82-86%), suggesting a bias toward successful actions in the environment. The correlation between confidence and accuracy varied considerably across experiments (r=0.099 to r=0.523), with three experiments showing moderate positive correlations (supporting the hypothesis), one showing minimal correlation (refuting the hypothesis), and one yielding inconclusive results. A consistent pattern across all experiments was the LLM's tendency toward high confidence scores (typically 0.85-0.89) with minimal discrimination between correct and incorrect predictions, indicating potential overconfidence. The strongest evidence for the hypothesis came from experiment copy4, which demonstrated both the highest correlation (r=0.523) and explicit mention of good calibration. Overall, while the majority of experiments (3/5) support the hypothesis that LLMs can meaningfully predict their confidence in action outcomes, the practical utility is limited by the LLM's overconfidence and the fact that a simple heuristic (always predicting success) often outperformed the LLM's predictions. These findings suggest that while LLMs show some ability to assign meaningful confidence scores that correlate with accuracy, significant improvements in calibration are needed for practical applications.	734557640845, 444903457359, 98980868691, 510187767935, 386054315386
basic-knowledge-sharing	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	Two ReAct agents with access to a shared knowledge graph will perform better on information-dependent tasks compared to agents working independently.	0	1	0	limited information	This meta-analysis examined whether two agents with access to a shared knowledge graph would outperform agents working independently on information-dependent tasks in a cooking game environment. The single experiment conducted (basic-knowledge-sharing-copy4) implemented three conditions: a single agent baseline, two independent agents with private knowledge graphs, and two agents with a shared knowledge graph. The experiment ran in PILOT mode with 10 episodes per condition and 25 steps per episode.  The results clearly refute the original hypothesis. The single agent baseline achieved the highest mean score (0.062 ± 0.064), while both two-agent conditions performed identically worse with mean scores of 0.011 ± 0.033. Bootstrap analysis confirmed no significant advantage for the shared graph condition (p=1.0) compared to independent agents. Although the shared knowledge graph successfully captured more environment information (mean 14.1 new facts added vs 6.4 in the independent condition), this did not translate to improved task performance.  These findings suggest that simply sharing knowledge through a graph structure is insufficient to improve collaborative performance in this task environment. Several factors may explain these results: (1) the task may not have been sufficiently information-dependent to benefit from knowledge sharing, (2) the alternating-turns design may have disrupted agent continuity and planning, (3) the knowledge representation or utilization mechanisms may have been inadequate, or (4) the overhead of maintaining and consulting the knowledge graph may have outweighed its benefits. Future research should address these limitations by designing tasks with clearer information dependencies, improving knowledge representation formats, and developing more sophisticated protocols for knowledge utilization.	825739594975
cooking-graph-explorer	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	An agent that maintains an explicit knowledge graph of container relationships will perform better at CookingWorld tasks than an agent without this explicit knowledge representation.	0	1	3	limited information	This meta-analysis examined four implementations of an experiment testing whether a ReAct agent augmented with container-relationship knowledge performs better at CookingWorld tasks than agents without this explicit knowledge representation. Only one experiment (copy1) directly tested the hypothesis by comparing the container-augmented agent against a standard ReAct agent, and it found that the container-augmented agent performed significantly worse (mean score: 0.206 vs. 0.321, p=0.9975), thus refuting the hypothesis. The other three experiments compared the container-augmented agent only against a random baseline, which is insufficient to test the stated hypothesis that specifically compares against agents 'without this explicit knowledge representation' (implying otherwise capable agents like ReAct). While these three experiments showed the container agent outperforming the random baseline (with statistical significance in copies 3 and 4), they are deemed inconclusive regarding the original hypothesis. The divergent results between copy1 (where container knowledge hurt performance against ReAct) and the other implementations (where container knowledge helped against random baselines) suggest that the value of container relationship knowledge may depend on the baseline comparison and possibly implementation details. Future work should ensure consistent comparison against both random and standard ReAct baselines, investigate why container knowledge might hinder performance in some implementations, and explore whether the benefits of container knowledge emerge more clearly in more complex environments with more objects and relationships.	184564987747, 893453181732, 273802986520, 929133254919
failure-pattern-learning	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	An agent that tracks and learns from patterns in its failed actions will perform better than a baseline agent that doesn't maintain failure history.	0	1	0	limited information	This meta-analysis examined whether a ReAct agent that tracks and learns from patterns in its failed actions performs better than a baseline agent without failure history in TextWorld Commonsense (TWC) games. Only one experiment was conducted in PILOT mode with 3 games, 10 episodes each, and max 20 steps per episode. The results refute the hypothesis: the experimental agent with failure memory (mean score 0.225) performed slightly worse than the baseline agent (mean score 0.25), with no statistical significance (p=0.8169). Both agents had 0% success rates, with the experimental agent taking more steps on average (19.17 vs 16.93). The failure memory mechanism implemented in the experimental agent tracked patterns but did not effectively prevent repeated mistakes. This suggests that simply maintaining a record of failures without more sophisticated reasoning about how to avoid them may not be sufficient to improve performance in complex text-based games. The small sample size (3 games, 10 episodes each) may have limited the ability to detect small effects, and the specific implementation of the failure memory mechanism may not have been optimal. Future research could explore more sophisticated failure pattern recognition, better integration of failure memory into decision-making, or larger-scale experiments to detect smaller effects.	758908263525
hierarchical-elimination	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	Hierarchical elimination will be more efficient and accurate than flat elimination, particularly in complex environments with many objects and areas.	1	2	1	mixed information	This meta-analysis examined four experiments testing whether hierarchical elimination (filtering first at room-level, then object-level) outperforms flat elimination (evaluating all objects directly) in ScienceWorld environments. The experiments consistently implemented the core comparison between hierarchical filtering, flat filtering, and control conditions (random filtering and no filtering).  Regarding computational efficiency, results were mixed. One experiment showed substantial speed improvements with hierarchical filtering (5.43s vs 21.06s), another showed modest improvements (6.78s vs 8.20s), while two others showed comparable computation times with high variance. Importantly, none of the experiments reported statistically significant differences in computation time.  For task performance/accuracy, only one experiment (copy3) showed clear advantages for hierarchical filtering (scores of 6.8 vs 5.16). The other three experiments found either similar performance between hierarchical and flat approaches or, in some cases, worse performance with hierarchical filtering.  All experiments were limited by small sample sizes (10 episodes in PILOT mode), high variance in performance metrics, and incomplete implementation of some planned metrics (particularly precision/recall calculations). The environments tested were also relatively simple (3-4 rooms), which may not have provided sufficient complexity to demonstrate the hypothesized advantages of hierarchical filtering.  Overall, the evidence leans against the hypothesis, with two experiments refuting it, one supporting it, and one being inconclusive. The hierarchical approach did not consistently demonstrate the expected efficiency and accuracy advantages over flat filtering across different experimental implementations. Future work should use larger sample sizes, more complex environments, and more comprehensive metrics to better evaluate the potential benefits of hierarchical filtering.	235899831457, 359589638349, 198884073892, 756482072322
hypothesis-driven-discovery		An agent that explicitly generates and tests hypotheses will discover correct environment mechanics more reliably than agents that explore without structured hypothesis testing.	0	0	0	no information	No experiments were provided for analysis. The research idea aimed to compare a hypothesis-driven agent against baseline approaches (particularly a ReAct agent) in DiscoveryWorld's Plant Nutrients environment. The hypothesis posited that structured hypothesis testing would lead to more reliable discovery of correct environment mechanics compared to standard exploration approaches. The planned experiment would have measured performance using DiscoveryWorld metrics (task completion, procedure score, knowledge score) and custom metrics (hypothesis generation count, experiment count, confirmation rate, steps per experiment). Without experimental results, no conclusions can be drawn about the effectiveness of the hypothesis-driven approach compared to baselines. Future work should implement and run the experiments as described in the operationalization plan to properly test this hypothesis.	
kg-failure-detection	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	An agent using knowledge graph features can detect action failures more accurately and quickly compared to agents using only text-based observation features.	0	2	0	limited information	This meta-analysis examined two experiments testing whether knowledge graph (KG) based features could detect action failures more accurately and quickly than text-based methods in TextWorldExpress CookingWorld. Both experiments implemented three detectors: KG-based, text similarity, and keyword-based. The first experiment showed comparable performance between KG (F1=0.36) and text similarity (F1=0.39) approaches, with both outperforming the keyword baseline (F1=0.15). The second experiment showed poor performance across all detectors, with the KG detector achieving 0% precision/recall/F1 while the text similarity detector achieved at least minimal performance (F1=1.15%). Neither experiment supported the hypothesis that KG-based detection would outperform text-based methods. In fact, the second experiment strongly refuted it, with text similarity significantly outperforming the KG approach. Both experiments had limitations, including small sample sizes and high false positive rates across all detectors. The results suggest that either the KG implementation was suboptimal or that the fundamental premise that graph-based features would outperform text-based features for failure detection may be flawed. Future work should focus on more sophisticated KG representations, better feature extraction from graphs, and larger-scale experiments with more diverse failure scenarios to conclusively evaluate the potential of KG-based failure detection.	411584982981, 341430618656
kg-state-tracking	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	Knowledge graph representations will improve accuracy in tracking object locations and properties compared to text-only representations in a simplified CookingWorld environment.	2	1	2	mixed information	This meta-analysis examined five experiments testing whether knowledge graph representations improve state tracking accuracy compared to text-only representations in a simplified CookingWorld environment. The experiments used GPT-4o-mini to predict object locations and properties across multiple episodes, with each experiment implementing the same basic design but with slight variations in implementation details.  Results across the five experiments were mixed. Two experiments (copy1 and copy5) showed statistically significant improvements in accuracy with knowledge graph augmentation, with p-values of 0.011 and 0.025 respectively. One experiment (copy4) found identical performance between conditions, directly refuting the hypothesis. Two experiments (copy2 and copy3) found numerical advantages for knowledge graph representations but failed to reach statistical significance, likely due to high variance and small sample sizes.  The absolute accuracy levels varied considerably across experiments, ranging from very low (0.121-0.158 in copy3) to moderate (0.42-0.66 in copy1). This suggests implementation details substantially affected overall performance. All experiments used the same small sample size (5 episodes per condition), limiting statistical power.  When knowledge graphs did help, they appeared to provide the most benefit for initial scene understanding and maintaining consistent object tracking. However, the inconsistency across experiments suggests that the advantage of knowledge graph representations may be sensitive to specific implementation details, prompt engineering, or evaluation metrics.  Overall, while there is some evidence supporting the hypothesis that knowledge graphs can improve state tracking, the mixed results and implementation-dependent outcomes suggest that the benefit is not robust across all implementations. Future work should use larger sample sizes, standardize implementation details, and investigate which specific aspects of knowledge graph representations (visualization format, graph structure, etc.) contribute most to any observed improvements.	195508367351, 796444343966, 762919571681, 995952549697, 75415238230
knowledge-graph-discovery	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	An agent that explicitly maintains a knowledge graph of its discoveries will perform better at DiscoveryWorld tasks than baseline agents, by having better memory of past experiments and being able to make more informed decisions about what to try next.	1	0	0	limited information	This meta-analysis examines the effectiveness of knowledge graph-based agents for scientific discovery tasks in DiscoveryWorld. The original hypothesis posited that agents maintaining explicit knowledge graphs would outperform baseline agents by better remembering past experiments and making more informed decisions.  The single experiment conducted (knowledge-graph-discovery-copy4) provides support for this hypothesis, though with some limitations. The knowledge graph agent demonstrated significantly better process scores (mean=0.25) compared to the baseline ReAct agent (mean=0.083), with statistical significance (p=0.0079). The knowledge graph agent also engaged in more meaningful scientific behaviors, successfully measuring protein levels in 4 out of 10 episodes, while the baseline agent rarely made measurements.  However, it's important to note that neither agent achieved task completion in any episode, suggesting that while the knowledge graph approach improved the discovery process, further refinements may be needed to achieve full task success. The experiment was conducted in PILOT mode with 10 episodes on the Proteomics-Easy difficulty level, which provides a limited but informative test of the hypothesis.  The implementation included most key components from the specification, though there were some deviations - notably incomplete graph visualization and bootstrap analysis. Despite these limitations, the results provide preliminary evidence that structured knowledge representation through graphs can enhance scientific discovery processes in AI agents.  To strengthen these findings, future work should include: (1) testing on more difficult tasks beyond Proteomics-Easy, (2) running more episodes for greater statistical power, (3) implementing the full experimental design including visualization components, and (4) exploring variations in knowledge graph structure and utilization strategies to further improve task completion rates.	534866361937
knowledge-graph-verification	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	An agent exploring and interacting with a text game environment can effectively verify the accuracy of automatically constructed knowledge graphs by attempting to validate individual triples through direct interaction.	0	0	1	limited information	This meta-analysis examines whether agents can effectively verify automatically constructed knowledge graphs through environment interaction in text-based games. The single experiment conducted compared a strategic ReAct agent against a random baseline in TextWorldExpress's CookingWorld environment. Both agents achieved perfect verification rates (1.0) across all 10 episodes, with high confidence scores (>0.9) for their verifications. The knowledge graphs contained 19-26 triples per episode describing room connectivity, object locations, and object properties. Statistical analysis showed no significant difference between the agents (p=1.0). While the results demonstrate that agents can indeed verify knowledge graphs through interaction (supporting part of the hypothesis), the lack of performance difference between strategic and random approaches makes the overall result inconclusive. This suggests that either: (1) the verification task was too simple, (2) the environment was too constrained, (3) the step limit was too generous, or (4) strategic planning provides no advantage for knowledge graph verification in this context. Future experiments should increase task complexity by using larger environments, more complex knowledge graphs, stricter step limits, or introducing deliberate errors in the knowledge graphs to better test verification capabilities. Additionally, comparing against other baselines mentioned in the original plan (human verification, static analysis) would provide more comprehensive insights.	790336820322
knowledge-guided-decomposition		Maintaining and utilizing a knowledge graph of successful decomposition patterns will lead to more efficient task completion compared to making decomposition decisions from scratch each time.	0	0	0	no information	No experiment results were provided for analysis. The research aimed to test whether knowledge-graph-guided decomposition improves task performance in TextWorldExpress CookingWorld compared to baseline approaches (basic decomposition without a knowledge graph, ReAct agent, and random agent). The planned experiment would have measured task scores, number of decomposition steps, total steps to completion, knowledge graph metrics, and other performance indicators across different pilot modes. Without experimental data, no conclusions can be drawn about the effectiveness of knowledge-guided decomposition for task completion efficiency.	
knowledge-guided-react	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	A ReAct agent that maintains and reasons over a structured knowledge graph of game mechanics and object relationships will perform better than a standard ReAct agent that only uses its prompt context.	0	1	0	limited information	This meta-analysis examined whether enhancing a ReAct agent with a knowledge graph improves its performance in text-based games. The single experiment conducted compared a baseline ReAct agent against a knowledge graph-enhanced version across 25 episodes in TextWorldExpress cooking tasks. The results clearly refute the hypothesis: the knowledge graph agent performed significantly worse than the baseline, achieving a mean score of 0.235 (4% success rate) compared to the baseline's 0.334 (16% success rate). Statistical analysis using bootstrap resampling confirmed no improvement from the knowledge graph (p=0.977). Interestingly, the knowledge graph agent took fewer steps on average (8.56 vs 22.4), suggesting it may have been more efficient in some ways but ultimately less effective at completing tasks. This could indicate that the knowledge graph implementation constrained the agent's exploration or decision-making process. Several factors may explain these results: (1) the knowledge graph representation may not have been optimal for this particular domain, (2) the integration between the graph and the agent's reasoning process may have been flawed, or (3) the additional complexity of maintaining and reasoning over the graph may have detracted from the agent's performance rather than enhancing it. Future work should explore different knowledge graph structures, improved integration methods, and potentially larger sample sizes to verify these findings. The experiment demonstrates that simply adding a knowledge graph to a ReAct agent does not automatically improve performance and may actually hinder it if not implemented optimally.	297088171371
llm-graph-verification	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	LLM-based verification of knowledge graph triples will improve graph accuracy and consistency compared to unverified graphs.	3	0	1	mixed information	This meta-analysis examined four experiments testing whether LLM-based verification improves knowledge graph accuracy in text-based games. Three experiments provided support for the hypothesis, while one was inconclusive due to execution failure. The successful experiments consistently showed that LLM verification detected and corrected graph inconsistencies, with correction rates ranging from 10.4 per episode to 233 corrections out of 1108 total triples. Verification accuracy was high (79.5-82.4%) across experiments. Game performance metrics showed consistent improvement with verification: scores were 0.125 vs 0.077 (p=0.155), 0.152 vs 0.067 (p=0.001), and 0.129 vs 0.075 (p=0.059) for the three successful experiments. Only one experiment achieved traditional statistical significance (p<0.05), but all showed positive trends. Graph structure analysis revealed that verified graphs tended to be more focused (fewer nodes) with higher density and better precision. Limitations across experiments included small sample sizes (10 episodes), use of random actions rather than learned policies, and lack of direct graph accuracy evaluation against ground truth. Overall, the evidence strongly suggests that LLM-based verification improves knowledge graph quality in text-based games, resulting in more accurate, focused representations that correlate with improved game performance.	127181842956, 80563508185, 751752587139, 315763595765
location-graph-cooking	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	An agent that maintains an explicit graph of object locations will complete cooking tasks more efficiently (using fewer steps) than an agent that relies solely on its working memory.	1	0	1	limited information	This meta-analysis examined two pilot experiments testing whether a ReAct agent augmented with a location-tracking graph would complete TextWorldExpress cooking tasks more efficiently than a standard ReAct agent. Both experiments implemented a similar design with 25 episodes comparing baseline and experimental conditions. The first experiment showed clear support for the hypothesis, with the location-tracking agent achieving both higher scores (0.38 vs 0.28) and requiring fewer steps (17.8 vs 24.9) on average. The second experiment found a statistically significant improvement in task scores (0.328 vs 0.158, p=0.015) but no significant difference in steps taken (18.83 vs 19.42, p=0.524). Across both experiments, the location-tracking agent consistently achieved higher task scores, suggesting improved overall performance. However, the evidence for improved efficiency specifically in terms of steps required is mixed. The first experiment showed a substantial reduction in steps, while the second showed no significant difference. This inconsistency may be due to the small sample sizes (25 episodes each), differences in implementation details, or the high variance typical in reinforcement learning tasks. Overall, the results suggest that location tracking is a promising enhancement for ReAct agents in TextWorldExpress cooking games, consistently improving task performance and potentially improving efficiency, though more extensive experiments with larger sample sizes would be needed to draw stronger conclusions about step efficiency specifically.	618535868101, 90439647588
progressive-state-complexity	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	LLMs will show degrading performance as state complexity increases, with particularly sharp drops when moving from discrete to continuous properties and when adding environment dynamics.	2	2	1	mixed information	This meta-analysis examined five experiments testing how increasing state representation complexity affects LLM simulation accuracy in CookingWorld. The experiments used gpt-4o-mini to predict state transitions across four complexity levels: boolean, numerical, relational, and full dynamics. Results were highly inconsistent across experiments. Two experiments (copy1 and copy4) strongly supported the hypothesis that performance degrades with increasing complexity, showing clear downward trends in accuracy as complexity increased, with the sharpest drops at the full complexity level. Two experiments (copy3 and copy5) refuted the hypothesis, with copy5 showing the complete opposite pattern - accuracy significantly improved with increasing complexity, reaching highest performance (93.9%) with full state representation. One experiment (copy2) was deemed inconclusive due to implementation issues and unrealistically perfect performance across conditions. The contradictory findings suggest that implementation details may significantly impact results, including how state representations are constructed, how the LLM is prompted, and how accuracy is measured. Notably, the magnitude of effects varied dramatically across experiments - from modest differences in some cases to dramatic drops (e.g., 31.3% accuracy for full complexity in copy1) in others. These inconsistencies highlight the sensitivity of LLM performance to experimental design choices and the need for standardized evaluation protocols. Future work should investigate which specific aspects of state complexity (boolean properties, numerical values, object relationships, or dynamics) most impact LLM performance, and whether these effects are consistent across different LLM architectures and sizes beyond gpt-4o-mini.	142623178601, 71213778464, 184901381441, 675271173941, 630216925892
react-knowledge-retrieval	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	A ReAct agent with ConceptNet knowledge retrieval will perform better than a standard ReAct agent on TextWorldExpress common sense tasks by having access to basic object relationships.	0	1	0	limited information	This meta-analysis examined whether adding ConceptNet knowledge retrieval to a ReAct agent improves performance on TextWorldExpress common sense tasks. The experiment compared three agent types: a random baseline, a standard ReAct agent, and a knowledge-augmented ReAct agent. The results clearly showed that both ReAct variants significantly outperformed the random baseline (p<0.001), demonstrating the effectiveness of the ReAct approach in general. However, the key finding was that there was no significant difference between the standard ReAct and knowledge-augmented ReAct agents (p=0.5321), with both achieving identical mean scores of 0.5625. This directly refutes the hypothesis that adding ConceptNet knowledge retrieval would improve performance. The implementation details suggest a potential issue with the knowledge integration mechanism, as empty knowledge retrievals were reported in the results. This could indicate that while the knowledge retrieval component was technically implemented, it may not have been effectively utilized in the agent's decision-making process. It's worth noting that the experiment was conducted in PILOT mode with a relatively small sample size (20 episodes per task), which could limit statistical power. However, the identical performance metrics between the two ReAct variants strongly suggest that, at least in this implementation, adding ConceptNet knowledge did not provide any performance advantage. Future work might explore more effective ways to integrate external knowledge into ReAct agents or investigate whether such knowledge augmentation might be more beneficial with simpler base LLMs that have less knowledge already embedded in their parameters.	535901763905
react-pattern-learning		A ReAct agent that stores and reuses successful reasoning patterns from past experiences will perform better on similar tasks compared to a standard ReAct agent that reasons from scratch each time.	0	0	0	no information	No experiments were provided for analysis. The research idea aimed to investigate whether a ReAct agent that stores and reuses successful reasoning patterns (defined as sequences of steps preceding score increases) would outperform a standard ReAct agent on TextWorldExpress cooking tasks. The planned metrics included average score achieved, average steps per episode, pattern reuse rate, and score increase rate. However, since no experimental results were submitted, it is impossible to draw any conclusions about the hypothesis. The experiment was designed to be run in three potential modes (MINI_PILOT, PILOT, and FULL_EXPERIMENT) with increasing scales, but there is no evidence that any of these were executed or what their outcomes might have been.	
reactive-graph-confidence	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	Using ReAct to explicitly reason about confidence in graph updates will result in more accurate belief graphs compared to direct updates without confidence reasoning.	1	1	1	limited information	This meta-analysis examined three experiments testing whether using the ReAct framework with explicit confidence reasoning improves belief graph accuracy in CookingWorld compared to direct updates without confidence reasoning. The results across experiments were mixed. One experiment strongly supported the hypothesis, showing the confidence-based agent achieved substantially higher verification rates for knowledge graph edges (77% vs 65% for baseline). However, another experiment refuted the hypothesis, finding no statistically significant difference in verified edges (p=0.2965) and a very weak correlation between confidence scores and edge verification (r=0.033). The third experiment showed results trending toward supporting the hypothesis (0.233 vs 0.166 verified edges, p=0.069) but remained inconclusive due to the small sample size and p-value above conventional significance thresholds. All experiments were conducted in PILOT mode with 10 episodes of 30 steps each in a 3-room environment, providing a consistent testing framework. The inconsistency in results suggests that the effectiveness of confidence-based reasoning may be sensitive to implementation details or evaluation metrics. Two experiments indicated some benefit to confidence-based reasoning (one significantly, one trending), while one showed no benefit. A key limitation across all experiments was the relatively small sample size (10 episodes), which may have limited statistical power. Additionally, the experiments varied in their implementation of confidence scoring and how they measured graph accuracy, which could explain the divergent results. Future work should include larger sample sizes, standardized implementations of confidence reasoning, and more robust statistical analyses to determine whether the observed benefits of confidence-based reasoning are reliable and generalizable.	660168784635, 651245282804, 42714548288
resistor-substitution-advisor	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	GPT-4 can successfully suggest viable resistor substitutions using standard-value resistor combinations that match target specifications within 5% tolerance.	2	1	2	mixed information	This meta-analysis examined five experiments testing whether GPT-4 can successfully suggest viable resistor substitutions using standard-value resistor combinations that match target specifications within 5% tolerance. The results show inconsistent performance across experiments. Two experiments support the hypothesis, with success rates of 79.6% and 50% at 5% tolerance. One experiment clearly refutes the hypothesis, with the LLM approach achieving less than 32.2% success even at a more lenient 10% tolerance. Two experiments were inconclusive as they reported success rates at 1% tolerance (21.7% and 18.3%) but didn't explicitly state the 5% tolerance success rate. Across all experiments, the LLM approach was consistently outperformed by both the simple baseline and mathematical optimization approaches. The simple baseline was surprisingly effective, achieving 90-100% success rates within 1% tolerance in most experiments, while being computationally efficient. The mathematical optimization approach generally achieved the highest accuracy but at the cost of longer computation times. The LLM approach showed high variability in performance, sometimes suggesting inappropriate combinations (like using parallel when series would be better), leading to large errors exceeding 90% in some cases. Overall, while GPT-4 can sometimes suggest viable resistor substitutions within 5% tolerance, its performance is inconsistent and significantly inferior to simpler algorithmic approaches, suggesting that this task may not be well-suited for LLM-based solutions.	742654095402, 922412943876, 482800065382, 162187344828, 905510309226
rule-guided-action-validation	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	Using basic cooking domain rules to filter action selections will improve the rate of valid actions and task completion compared to unfiltered random selection.	0	1	0	limited information	This meta-analysis examined whether using basic cooking domain rules to filter action selections improves performance in TextWorldExpress cooking tasks. The single experiment conducted compared two ReAct-based agents: a baseline agent with a basic prompt and an experimental agent with explicit cooking rules in its prompt. The experiment was run in PILOT mode with 10 episodes (seeds 1-10).  The results clearly refute the original hypothesis. Both conditions achieved identical task completion rates of 10%, showing no improvement from the rule-guided approach. The valid action ratios were also very similar (baseline: 16.9% vs experimental: 17.5%) with no statistically significant difference (p=0.3856).   These findings suggest that simply including cooking domain rules in a ReAct agent's prompt does not meaningfully improve performance on cooking tasks in this environment. Both agents struggled with the tasks, achieving low completion rates and valid action ratios. This indicates that either: (1) the rule-guided approach as implemented is ineffective, (2) the ReAct framework may not be leveraging the rules effectively, or (3) the tasks themselves may be too challenging for the current prompt-based approach.  Limitations of this meta-analysis include the small sample size (only 10 episodes) and high variance in agent performance, which limits the strength of conclusions. Additionally, only one implementation of the rule-guided approach was tested. Future work might explore different ways of incorporating rules, such as more structured rule enforcement mechanisms rather than simply including them in prompts.	22786435429
simple-abstraction-tuning		Automated tuning of abstractions based on their success rates will improve task completion rates compared to static abstractions.	0	0	0	no information	No experiments were provided for analysis. The research idea aimed to develop a system that automatically tunes text-based game abstractions based on their success rate in TextWorldExpress cooking tasks, with the hypothesis that automated tuning would improve task completion rates compared to static abstractions. The experiment was designed to compare three conditions: static abstractions (baseline), tuned abstractions (experimental), and random modifications (control). However, since no experiment results were provided in the input, it is impossible to determine whether the hypothesis was supported, refuted, or if the results were inconclusive. A proper meta-analysis would require data from multiple experiment runs, including information about abstraction performance before and after tuning, statistical comparisons between conditions, and detailed logs of the tuning process and outcomes.	
simple-affordance-exploration	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	LLM-guided exploration using simple affordance predictions with success/failure tracking will find successful solutions faster than random exploration in ScienceWorld tasks.	1	0	0	limited information	This meta-analysis examined whether LLM-guided affordance prediction could improve exploration efficiency in ScienceWorld tasks compared to random exploration. The single experiment conducted (simple-affordance-exploration-copy5) strongly supports the hypothesis. The experiment implemented two agents: a random baseline agent and an affordance-guided agent that used GPT-4-mini to predict useful actions. Testing was conducted across three ScienceWorld tasks of varying complexity ('find-living-thing', 'boil', and 'use-thermometer') with 10 episodes per task in PILOT mode. Results showed statistically significant improvements for the affordance-guided agent across all tasks. In the 'find-living-thing' task, the affordance agent achieved a mean score of 24.2 compared to 10.8 for the random agent (p<0.001). In the 'boil' task, the affordance agent scored 2.0 versus 0.7 for random (p=0.0015). In the 'use-thermometer' task, the affordance agent scored 11.1 versus 1.8 for random (p<0.001). The implementation included sophisticated affordance prediction with task-specific guidance, action success rate tracking, and mechanisms to prevent repetitive actions. While the sample size was relatively small (10 episodes per condition), the statistical significance of the results across all three tasks provides strong evidence supporting the hypothesis that LLM-guided exploration using affordance predictions with success/failure tracking finds successful solutions faster than random exploration in ScienceWorld tasks. Future work could benefit from larger sample sizes and testing on additional ScienceWorld tasks to further validate these findings.	440959613477
simple-decomposition-memory	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	An agent that stores and reuses successful task decomposition patterns will perform better than an agent that decomposes each task from scratch.	0	0	1	limited information	This meta-analysis examined whether maintaining a simple history of successful task decompositions improves an agent's performance on similar tasks in TextWorldExpress CookingWorld. Only one experiment was available for analysis, which tested the hypothesis that an agent storing and reusing successful decomposition patterns would outperform an agent decomposing each task from scratch. The experiment implemented two agents in a cooking environment with 1-ingredient recipes: a baseline ReAct agent without decomposition history and an experimental agent that stored and retrieved past successful decompositions using string similarity matching. The experiment used 10 training episodes and 5 test episodes in PILOT mode. Results showed that the experimental agent achieved a higher mean score (0.183) compared to the baseline (0.117), suggesting a potential benefit to the decomposition history approach. However, this difference was not statistically significant (p=0.324) according to bootstrap analysis. Both agents showed variable performance across episodes, with scores ranging from 0 to 1.0. The small sample size (5 test episodes) significantly limits the strength of conclusions that can be drawn. While the trend is in the direction supporting the hypothesis, the lack of statistical significance and limited sample size make the evidence inconclusive. Future experiments would benefit from larger sample sizes, potentially more sophisticated pattern matching mechanisms, and more detailed analysis of when and how the pattern reuse mechanism contributes to performance improvements.	590030803866
simple-dual-reflection	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	Sequential two-agent reflection will generate higher quality insights than single-agent reflection, as measured by task performance improvement.	0	2	0	limited information	This meta-analysis examined two experiments testing whether sequential dual-agent reflection improves performance on TextWorldExpress CookingWorld tasks compared to single-agent reflection. Both experiments implemented a PILOT mode with 10 episodes per condition and compared three conditions: single-agent reflection, dual-agent reflection, and random action selection. The results consistently showed no significant performance advantage for dual-agent reflection over single-agent reflection. In the first experiment, single-agent reflection actually achieved a slightly higher mean score (0.215) than dual-agent reflection (0.190), though this difference was not statistically significant (p=0.679). In the second experiment, dual-agent reflection achieved a marginally higher mean score (0.223) than single-agent reflection (0.204), but again this difference was not statistically significant (p=0.426). Both experiments found that reflection quality was high for both reflection conditions, suggesting that while agents could generate meaningful reflections, this did not translate to performance differences between the reflection types. Both experiments had relatively small sample sizes (10 episodes per condition), which limits statistical power, but the consistent pattern across both experiments strengthens the conclusion that sequential dual-agent reflection does not provide substantial benefits over single-agent reflection for these tasks. The hypothesis that sequential two-agent reflection would generate higher quality insights than single-agent reflection, as measured by task performance improvement, is not supported by these experiments.	508535943840, 363352845643
simple-goal-explorer	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	An agent that explicitly tracks predefined cooking-related goal hypotheses will identify game objectives more quickly than random exploration in cooking games.	3	0	1	mixed information	This meta-analysis examined four implementations of the 'simple-goal-explorer' experiment, which tested whether a goal-tracking ReAct agent could outperform a random baseline in TextWorldExpress cooking games. Three of the four experiments (75%) provided statistically significant evidence supporting the hypothesis that goal-tracking agents perform better than random exploration, while one experiment showed a non-significant trend in the same direction. All experiments were conducted in PILOT mode with 5 games and 5 episodes each, using similar implementations of a ReAct agent that maintained confidence scores for different cooking goals ('cook meal', 'prepare breakfast', 'make dinner', 'cook recipe'). The successful experiments showed statistically significant improvements in game scores: copy1 (0.334 vs 0.128, p<0.001), copy4 (0.127 vs 0.055, p=0.033), and copy5 (0.198 vs 0.074, p<0.001). The inconclusive experiment (copy3) showed a smaller, non-significant difference (0.147 vs 0.134, p=0.323). Despite the positive results, all experiments noted limitations including the small sample size of the pilot testing and occasional agent inefficiencies such as getting stuck in repetitive behavior patterns. The absolute performance of both agent types remained relatively low across all experiments, suggesting room for further improvement in the goal-tracking mechanism and action selection strategies. Overall, the meta-analysis provides moderately strong evidence that explicitly tracking predefined cooking-related goal hypotheses improves agent performance compared to random exploration in cooking games, though the magnitude of improvement varies across implementations.	60637110575, 98132395184, 722891225969, 184821369614
simple-graph-alignment	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	Basic similarity metrics between text descriptions and graph structures can effectively capture the alignment between different representations of the same game state.	3	2	0	mixed information	This meta-analysis examined five experiments testing whether basic similarity metrics can effectively align text descriptions with graph representations of game states in TextWorldExpress cooking games. The results show mixed evidence for the hypothesis, with three experiments supporting it and two refuting it. The experiments that supported the hypothesis demonstrated that certain metrics (particularly Jaccard similarity in one case and custom metrics incorporating structural features in two others) could significantly outperform random baselines, achieving similarity scores between 0.3-0.8. However, two experiments found that basic metrics performed poorly or even worse than random chance, with very low similarity scores (below 0.1) and no positive correlation with game progress. The contradictory results may be explained by differences in implementation details, particularly in how graphs were constructed and how similarity was calculated. The more successful experiments tended to use more sophisticated metrics that incorporated structural graph features beyond simple word overlap. This suggests that while basic similarity metrics alone may be insufficient, enhanced metrics that account for graph structure can effectively align text and graph representations. The experiments were limited by focusing primarily on initial game states rather than full trajectories and using relatively simple graph constructions. Future work should standardize graph construction methods, explore more sophisticated similarity metrics, and evaluate performance across more diverse game scenarios and complete gameplay trajectories.	692511972995, 886454208931, 641570661716, 867045194182, 445778117853
simple-graph-cooking-simulation		Providing a pre-built knowledge graph of cooking relationships as additional context will improve an LLM's ability to predict valid cooking actions in CookingWorld tasks.	0	0	0	no information	No experiments were provided for analysis. The research idea aimed to test whether a static knowledge graph of cooking relationships would improve an LLM's ability to predict valid cooking actions in CookingWorld tasks. The planned experiment would have compared a ReAct agent with and without access to a knowledge graph containing information about ingredient combinations and cooking actions. The experiment was designed to measure valid action rates, task completion rates, and steps to completion across different conditions. However, since no experiment results were provided, it is impossible to draw any conclusions about the hypothesis. Future work should implement the planned experiment to determine whether knowledge graphs enhance LLM performance in structured cooking tasks.	
simple-graph-state-tracking	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	An agent maintaining even a simple graph-based representation of object locations and states will complete cooking tasks more efficiently than an equivalent agent without state tracking.	0	0	1	limited information	This meta-analysis examined whether graph-based state tracking improves agent performance in CookingWorld environments. Only one experiment was conducted (simple-graph-state-tracking-copy2), which compared a baseline ReAct agent against a modified version that maintained a graph representation of object locations and states. The results were inconclusive regarding the hypothesis. While the graph-tracking agent achieved a slightly higher mean score (0.302 vs 0.272), this difference was not statistically significant (p=0.311). Both conditions had identical success rates of 10% (defined as score > 0.5). Notably, the graph-tracking agent took more steps on average (28.5 vs 22.3), suggesting possible overhead from graph maintenance rather than improved efficiency. The experiment successfully implemented graph representation and tracking, but the benefits were minimal in the tested configuration. The small sample size (10 episodes per condition) limits the statistical power of the experiment. Future work should consider larger sample sizes, more diverse tasks, and potential refinements to how the graph information is integrated into the agent's decision-making process. The current evidence is insufficient to draw strong conclusions about the efficacy of simple graph-based state tracking for improving agent performance in text-based environments.	762449417307
simple-hierarchical-beliefs		A two-level hierarchical belief structure will lead to more organized and complete knowledge representation compared to a flat belief structure, as measured by graph coverage of temperature-related relationships.	0	0	0	no information	No experiments were provided in the input data. The experiments array was empty, making it impossible to conduct a meta-analysis of the proposed research on whether a simple two-level hierarchical belief structure can improve an agent's ability to learn and represent temperature-related relationships in ScienceWorld compared to a flat belief structure. While the research idea and operationalization plan were well-defined, without actual experimental results, no conclusions can be drawn regarding the hypothesis that 'A two-level hierarchical belief structure will lead to more organized and complete knowledge representation compared to a flat belief structure, as measured by graph coverage of temperature-related relationships.'	
simple-memory-pruning	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	Frequency-based memory pruning (removing least-used memories) will lead to better task performance than time-based pruning (removing oldest memories) in temperature-related tasks.	0	0	1	limited information	This meta-analysis examined one experiment comparing memory pruning strategies for a ReAct agent in ScienceWorld's water boiling task. The experiment tested four strategies: time-based pruning (removing oldest memories), frequency-based pruning (removing least-used memories), no pruning, and random pruning. The original hypothesis predicted that frequency-based pruning would outperform time-based pruning. Results showed a slight numerical advantage for frequency-based pruning (avg score 0.8) over time-based pruning (avg score 0.7), which directionally aligns with the hypothesis. However, this difference was not statistically significant (p=1.0), and both experimental strategies were outperformed by the baselines (no pruning: 1.5, random pruning: 0.9). The experiment encountered significant implementation challenges, including LLM cost limit exceedances that caused agent failures and difficulties with basic action execution. These technical issues severely compromise the reliability of the results. Given these limitations and the lack of statistical significance, the evidence is insufficient to either support or refute the hypothesis. Additional experiments with more robust implementation, larger sample sizes, and successful completion of all planned episodes would be necessary to draw meaningful conclusions about the relative effectiveness of these memory pruning strategies.	454604412120
simple-meta-graphs	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	A ReAct agent using simple knowledge graphs to track its past performance on specific task states will make more efficient mode-switching decisions compared to using random or fixed strategies.	0	2	0	limited information	This meta-analysis examined two experiments testing whether a ReAct agent using knowledge graphs to track past performance would make more efficient mode-switching decisions compared to random or fixed strategies. Both experiments implemented a pilot study testing four conditions (knowledge graph-based, random, fixed-detailed, fixed-quick) across four ScienceWorld classification tasks (find-living-thing, find-non-living-thing, find-plant, find-animal) with 5 episodes per condition. The results consistently failed to support the hypothesis. In the first experiment, the knowledge graph approach showed mixed performance with no clear advantage over baseline conditions, while potentially requiring more computational resources. In the second experiment, success rates were generally low across all conditions (0-40%), with the random condition actually outperforming the knowledge graph approach on some tasks (e.g., 40% vs 20% success on the find-plant task). Average scores were similar across conditions (13.4-18.6 points), and step counts were comparable (4.4-6.4 steps). These findings suggest that, at least in the current implementation and task environment, knowledge graph-based mode switching does not provide a meaningful advantage over simpler strategies. The experiments revealed underlying challenges in the classification tasks themselves, with all approaches struggling to achieve consistent success. Future work might explore more sophisticated knowledge graph representations, different task environments, or alternative ways of leveraging historical performance data for mode-switching decisions.	271745499162, 66098906146
simple-metaphor-graph	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	An LLM can identify meaningful metaphorical relationships between objects in TextWorldExpress CookingWorld scenarios based on their functional similarities, and these relationships can be effectively visualized in a knowledge graph format.	5	0	0	consistent (support)	This meta-analysis examined five experiments testing whether LLMs can identify meaningful metaphorical relationships between objects in TextWorldExpress CookingWorld scenarios. All five experiments consistently supported the hypothesis, demonstrating that LLMs can successfully identify functional similarities between kitchen objects that constitute metaphorical relationships. Across experiments, the LLM consistently identified between 40-55 metaphorical relationships per scenario, with detailed qualitative descriptions of functional similarities (e.g., 'both fridge and oven are essential appliances in food preparation'). The experiments successfully visualized these relationships in graph format, though several experiments noted limitations in the graph structure analysis. Specifically, because kitchen objects often shared the same location, co-occurrence graphs had complete connectivity (density=1.0), making it difficult to detect structural differences between baseline and metaphor-enhanced graphs. Despite this limitation in the visualization aspect, the core finding that LLMs can identify meaningful metaphorical relationships between objects based on functional similarities was strongly supported across all experiments. The consistency of results across different implementations strengthens confidence in the conclusion that LLMs can effectively detect and articulate functional metaphors between objects in a cooking domain. Future work might explore more complex environments where objects have more varied spatial distributions to better highlight structural differences in the resulting graphs.	583347880103, 976230850404, 487864606623, 204424222646, 264647067112
simple-planning-agent	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	A simple planning agent that breaks tasks into 2-3 sequential steps will perform better at basic cooking tasks compared to a baseline agent that attempts to achieve goals without planning.	1	2	0	mixed information	This meta-analysis examined three experiments testing whether a planning agent that breaks tasks into 2-3 sequential steps performs better than baseline agents (ReAct and Random) in CookingWorld tasks. The results across experiments were mixed, with one experiment supporting the hypothesis and two refuting it. In experiment copy3, the Planning agent significantly outperformed the ReAct baseline in both task score (0.363 vs 0.273, p=0.0046) and efficiency (15.85 vs 22.45 steps). However, in experiments copy2 and copy5, the Planning agent did not demonstrate superior performance compared to the ReAct baseline. In copy2, there was no significant difference in task scores (0.188 vs 0.197, p=0.715), though the Planning agent used fewer steps. In copy5, the ReAct agent actually outperformed the Planning agent in both task score (0.210 vs 0.167) and efficiency (14.35 vs 23.60 steps). All three experiments consistently showed that both Planning and ReAct agents significantly outperformed the Random baseline. The inconsistency across experiments suggests that the effectiveness of the planning approach may be sensitive to implementation details, environment configurations, or the specific nature of the tasks. It's worth noting that all experiments were conducted in PILOT mode with only 20 episodes, which limits statistical power. Future research should consider larger sample sizes, more diverse tasks, and potential refinements to the planning mechanism to better understand when and how planning provides advantages in this domain.	291788186424, 20450292902, 478265929555
simple-property-verification		ConceptNet's temperature and state-of-matter properties for common objects contain inaccuracies that can be identified through systematic environmental interaction	0	0	0	no information	No experiments were conducted for this research idea. The original plan was to create a system that would verify temperature and state-of-matter properties of common objects in ScienceWorld against ConceptNet knowledge. The experiment was designed to be implemented in three scales (MINI_PILOT, PILOT, and FULL_EXPERIMENT), starting with testing just 3 objects (water, ice, steam) and potentially scaling up to 50 objects. The system would extract relevant properties from ConceptNet, use a simple agent in ScienceWorld to observe actual properties, and then compare predictions against observations. However, as no experiments were actually run, no data is available to evaluate the hypothesis that 'ConceptNet's temperature and state-of-matter properties for common objects contain inaccuracies that can be identified through systematic environmental interaction.' Without experimental results, it's impossible to determine whether ConceptNet contains inaccuracies in its representation of physical properties or whether the proposed methodology of using ScienceWorld for verification would be effective.	
simple-self-evaluation	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	A ReAct agent with single-step self-evaluation will achieve higher task completion rates compared to a standard ReAct agent in TextWorldExpress CookingWorld tasks.	0	1	1	limited information	This meta-analysis examined two experiments testing whether adding a self-evaluation layer to a ReAct agent improves performance in TextWorldExpress CookingWorld tasks. Both experiments were conducted in PILOT mode with 10 episodes per condition, comparing a baseline ReAct agent, an enhanced ReAct agent with self-evaluation, and a random baseline. The results were mixed and largely inconclusive. In the first experiment, the enhanced agent showed better performance (mean score 0.435 vs 0.29), but this difference was not statistically significant (p=0.174). In the second experiment, the enhanced agent actually performed slightly worse than the baseline (mean score 0.473 vs 0.500) with identical success rates (20%), though again without statistical significance (p=0.617). Both experiments had small sample sizes (10 episodes per condition), which limits statistical power. The contradictory directions of effect between the two experiments suggest that the impact of adding self-evaluation to a ReAct agent may be highly variable or context-dependent. Both experiments showed that the ReAct agents (with or without self-evaluation) significantly outperformed random baselines, confirming the general effectiveness of the ReAct approach. Overall, the evidence does not support the hypothesis that adding self-evaluation consistently improves ReAct agent performance. A larger-scale experiment with more episodes would be needed to draw more definitive conclusions about the potential benefits of self-evaluation in ReAct agents.	186468487863, 475958626388
simple-social-graphs	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	An agent using a basic knowledge graph to track character relationships (friend/neutral/enemy) will make more socially appropriate decisions compared to an agent without relationship tracking.	0	5	0	consistent (refute)	This meta-analysis examined five experiments testing whether a basic knowledge graph for tracking character relationships (friend/neutral/enemy) improves agent social decision making compared to agents without relationship tracking. All five experiments consistently refuted the hypothesis, with the relationship-tracking agent performing either slightly or significantly worse than baseline agents across all studies.  The experimental design was consistent across all five experiments, implementing three agent types: (1) an experimental agent using a relationship graph, (2) a baseline agent without relationship tracking, and (3) a baseline agent assuming static friendly relationships. Each experiment used the PILOT mode with 5 scenarios and approximately 5 decisions per scenario, evaluating decision appropriateness on a 1-5 scale.  Key findings: 1. In all five experiments, the experimental agent with relationship tracking performed worse than the baseline agents, with mean scores ranging from 4.0-4.32 compared to baseline means of 4.16-5.0. 2. Two experiments (copy3 and copy5) found statistically significant differences favoring the baseline agents, while the others showed non-significant differences in the same direction. 3. The experimental agent appeared to sometimes overweight relationship information, making decisions that explicitly accounted for relationships but in ways that were rated as less appropriate. 4. In scenarios with conflicting relationships, the experimental agent sometimes made decisions that attempted to minimize interaction between enemies but received lower scores for potentially unfair treatment. 5. The baseline LLMs demonstrated strong inherent social reasoning capabilities without explicit relationship tracking.  Limitations across the experiments included small sample sizes, potential evaluation bias (as the same LLM system was used for both decision-making and evaluation), and scenarios that may have been too simple to demonstrate potential advantages of relationship tracking.  Conclusion: The evidence consistently suggests that explicit relationship tracking using knowledge graphs does not improve—and may actually hinder—agent social decision making in these scenarios. This counterintuitive finding may indicate that modern LLMs already implicitly track relationships effectively, and that explicit tracking mechanisms may cause agents to overemphasize relationship dynamics at the expense of other important factors in social decision-making.	822321180744, 798375093323, 485603714609, 379863258608, 37447724888
simple-task-composition		An LLM that explicitly decomposes tasks into two-step sequences will perform better on cooking tasks than an LLM that approaches tasks end-to-end.	0	0	0	no information	No experiments were provided in the input data. The research was designed to test whether explicit decomposition of tasks into two-step sequences improves LLM performance in TextWorldExpress's CookingWorld environment compared to end-to-end approaches. The planned experiment would have compared a Decomposition Agent (which breaks tasks into steps before execution) against an End-to-end Agent (direct action selection) and a Random Agent baseline. Metrics would have included task scores, steps taken, completion time, and token usage. However, since no experiment results were provided, no conclusions can be drawn regarding the hypothesis that explicit task decomposition improves performance.	
simple-task-reflection	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	An agent that has access to its past successful experiences when reflecting on failures will generate more effective reflections and show faster improvement compared to an agent that reflects without access to past experiences	0	0	1	limited information	This meta-analysis examined whether providing an agent with its own past successful experiences on similar tasks can improve its reflection process in TextWorldExpress cooking tasks. The research aimed to compare three agent variants: a baseline ReAct agent without reflection, a basic reflection agent, and an experience-guided reflection agent that leverages past successful experiences. The experiment was designed to collect experiences from training seeds and evaluate all three agents on evaluation seeds, measuring metrics such as average reward, success rate, and steps to first reward.  Unfortunately, the single experiment run (simple-task-reflection-copy2) failed to execute successfully. While the code implementation appeared comprehensive and well-structured - including experience collection, embedding-based similarity matching for experience retrieval, and bootstrap resampling for statistical analysis - the execution failure resulted in no empirical data being collected. The results.json file was null and the log.json was empty, indicating a critical failure in the experiment execution.  Due to this technical failure, no conclusions can be drawn about the effectiveness of experience-guided reflection compared to basic reflection or no reflection. The hypothesis remains untested, and further experiments with successful execution would be needed to evaluate whether access to past successful experiences improves an agent's reflection quality and performance in cooking tasks. Future work should focus on resolving the execution issues and successfully implementing the experimental design to collect the necessary empirical data.	433345045625
simple-template-discovery	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	Frequently occurring action sequences from successful trajectories can serve as effective templates to improve agent performance in similar tasks.	1	0	1	limited information	This meta-analysis examined two implementations of the simple template discovery approach in TextWorldExpress CookingWorld environments. The experiments tested whether automatically discovered action templates from successful trajectories could improve agent performance compared to baseline random agents and agents using manually defined templates.  The results were mixed across the two experiments. In the first experiment (copy1), the discovered templates did not provide statistically significant performance improvements over baseline or manual template approaches. All three conditions achieved relatively similar performance metrics (success rates of 20-28% and mean scores of 0.056-0.072), with bootstrap resampling showing no significant differences (p>0.47).  However, the second experiment (copy4) showed strong support for the hypothesis. The discovered template system significantly outperformed the baseline random agent (29.6% vs 6.4% average score, p<0.001) and achieved a much higher success rate (88% vs 28%). It also slightly outperformed the manual template system (29.6% vs 25.6% average score, 88% vs 76% success rate). Additionally, both template-based approaches demonstrated improved efficiency, requiring approximately 16 steps to reach a reward compared to 28 steps for the baseline.  The divergent results may be attributed to differences in implementation details, such as the specific templates discovered, the template selection criteria, or the integration method of templates into the agent's decision-making process. The second experiment identified a particularly successful three-step template for taking and cooking items that achieved a 71% success rate when attempted.  Overall, while one experiment provides strong evidence that discovered templates can significantly improve agent performance, the inconsistency between experiments suggests that the effectiveness of template discovery may depend on specific implementation details. Future work should focus on identifying which aspects of template discovery and implementation are most critical for performance improvements and developing more robust methods that consistently yield benefits across different experimental conditions.	208240251429, 692580791223
simple-template-discovery		Using automatically discovered two-action templates will improve agent performance compared to using only primitive actions in TextWorldExpress CookingWorld games.	0	0	0	no information	No experiments were provided in the input data. The research idea aimed to investigate whether automatically discovering and using simple action templates (fixed-length sequences of successful actions) can improve agent performance in TextWorldExpress CookingWorld games. The system was designed to analyze successful gameplay trajectories to identify common 2-action sequences and use these as templates for future gameplay. The operationalization plan included different pilot modes with varying parameters for template discovery and testing. However, since no experimental results were provided, it is impossible to determine whether the hypothesis was supported, refuted, or if the results were inconclusive. A proper meta-analysis would require the actual experimental results showing the performance comparison between template-based agents and primitive action agents across multiple runs.	
simulation-confidence-analysis	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	LLM confidence scores will correlate with prediction accuracy, allowing for identification of potentially incorrect predictions.	2	0	3	limited information	This meta-analysis examined five experiments testing whether LLM confidence scores correlate with prediction accuracy in TextWorldExpress environments. All experiments used similar methodology: running multiple episodes in CookingWorld environments where an LLM (gpt-4o-mini) made state predictions with confidence scores, which were then compared to actual outcomes. The results show a consistent pattern of positive but variable correlation between confidence and accuracy. Two experiments provided clear support for the hypothesis, finding moderate positive correlations (r≈0.587 and r=0.33) between confidence scores and accuracy. Three experiments yielded inconclusive results, showing either very weak correlations (r=0.110) or highly inconsistent correlations that varied widely across episodes (ranging from strongly negative to strongly positive). No experiment definitively refuted the hypothesis. The meta-analysis suggests that LLMs do possess some metacognitive ability to assess their prediction confidence, with confidence scores showing a generally positive correlation with accuracy. However, this relationship is inconsistent and often too weak to be practically reliable for identifying incorrect predictions. The high variability in correlations across episodes indicates that LLM confidence calibration is context-dependent and unstable. These findings suggest that while confidence scores may provide some signal about prediction accuracy, they cannot be relied upon as a robust mechanism for identifying potentially incorrect predictions without additional safeguards or improvements in LLM calibration.	232279592942, 568592485353, 100526801365, 831058631624, 622091164648
static-knowledge-comparison	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	LLM-derived task-specific knowledge will lead to better agent performance compared to general knowledge from ConceptNet, due to its ability to provide more contextually relevant information.	1	0	0	limited information	This meta-analysis examined one experiment comparing the effectiveness of different static knowledge injection methods in ScienceWorld tasks. The experiment tested four conditions: baseline (no knowledge), ConceptNet knowledge, LLM-derived knowledge, and random selection between ConceptNet and LLM. The results showed that the LLM condition achieved the highest performance with a 90% success rate and average score of 2.4, compared to ConceptNet's 50% success rate and average score of 1.4. This supports the hypothesis that LLM-derived task-specific knowledge leads to better agent performance than general knowledge from ConceptNet. However, the experiment noted that bootstrap statistical testing did not show significant differences between conditions, likely due to the small sample size (10 episodes per condition). Despite this limitation, the substantial performance gap between LLM and ConceptNet conditions (40% higher success rate and 1.0 higher average score for LLM) provides meaningful evidence supporting the hypothesis. The experiment was generally faithful to the requested design, implementing all core components including the ReAct agent, knowledge injection methods, and proper data collection. Future work should increase the sample size to improve statistical power and ensure more systematic control of knowledge injection formats across conditions.	420198010958
subgoal-quality-evaluation	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	Using an LLM to evaluate and filter generated subgoals will lead to better task performance compared to using unfiltered subgoals.	0	0	1	limited information	This meta-analysis examined whether using an LLM to evaluate and filter generated subgoals improves hierarchical agent performance in TextWorldExpress TWC tasks. One experiment was conducted comparing four agent types: (1) Hierarchical ReAct with LLM filtering, (2) Hierarchical ReAct without filtering, (3) Vanilla ReAct, and (4) Random baseline. The results showed that the experimental agent with LLM filtering achieved a higher mean score (0.278) compared to the unfiltered hierarchical agent (0.144), suggesting a potential benefit of subgoal filtering. However, the bootstrap analysis yielded a p-value of 0.060, which is close to but does not meet the conventional significance threshold of 0.05. This indicates a trend toward significance rather than a definitive result. The experimental agent significantly outperformed the random baseline (p=0.000) but was not significantly different from the vanilla ReAct agent (p=0.269). These findings suggest that LLM-based subgoal filtering may improve hierarchical agent performance, but more data would be needed to draw a conclusive determination. The experiment was conducted in PILOT mode with 25 episodes per agent, which may have limited statistical power. A larger-scale experiment with more episodes would be beneficial to establish more definitive evidence regarding the effectiveness of LLM-based subgoal filtering in hierarchical agents.	976965224033
template-world-generation	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	Template-based generation with controlled object variation can create playable single-room environments that are as engaging as manually designed environments.	2	1	0	mixed information	This meta-analysis examined three experiments testing whether template-based generation can create text adventure game environments that are as engaging as manually designed ones. Two experiments strongly supported the hypothesis, while one refuted it. In the supporting experiments, template-generated environments achieved identical success rates (100%) to manual environments, with comparable or identical steps to completion. One experiment even found template environments offered significantly more action diversity (p=0.005), suggesting potentially greater engagement through more interaction options. However, one experiment showed template environments underperforming with only 84% success rate compared to 100% for manual environments, with some agents getting stuck in examination loops. All experiments were conducted in PILOT mode with relatively small sample sizes (5 template environments vs 1-2 manual environments, with 5 episodes each), which limits the strength of conclusions. The mixed results suggest that template-based generation can create environments comparable to manual design in many cases, but implementation details matter significantly. The success appears dependent on the quality of templates and generation logic, with poorly designed templates potentially creating environments where agents struggle. Future work should expand to the FULL_EXPERIMENT mode with larger sample sizes, more complex environments (varying object counts from 2-6), and more sophisticated interaction types beyond the basic examine/pickup/drop actions tested in these pilots.	462841209202, 969709661546, 932803340282
textworld-subgoal-planning		Breaking down complex cooking tasks into subgoals before execution will lead to higher success rates and more efficient solutions compared to direct planning.	0	0	0	no information	No experiment results were provided for analysis. The research plan outlined a comparative study between subgoal-based planning and direct planning in TextWorldExpress cooking tasks, with a hypothesis that breaking down complex cooking tasks into subgoals would lead to higher success rates and more efficient solutions. The plan included a structured approach with mini-pilot, pilot, and full experiment phases using increasingly complex recipes (1-3 ingredients) and larger sample sizes. The primary evaluation metric was to be Task Score (partial credit) rather than binary completion, with secondary metrics including number of steps taken and time per episode. Without actual experiment results, no conclusions can be drawn about the effectiveness of subgoal-based planning compared to direct planning in TextWorldExpress cooking tasks.	
two-level-cooking-planner		A two-level planner that separates recipe planning from action execution will solve cooking tasks more efficiently than a flat planner or random baseline.	0	0	0	no information	No experiments were provided in the input data. The experiments array is empty, so no meta-analysis can be performed. The research idea proposed developing a two-level planning system for TextWorldExpress cooking tasks that combines high-level recipe planning with low-level action execution, but no experimental results were provided to evaluate whether this approach is more efficient than a flat planner or random baseline.	
two-level-discovery-agent		A two-level hierarchical agent that separates planning from execution will perform better on measurement-based discovery tasks than a non-hierarchical baseline.	0	0	0	no information	No experiments were provided in the input data. The research plan outlined a comparison between a two-level hierarchical agent (with separate planning and execution components) and baseline agents (ReAct and flat single-level) on measurement tasks in DiscoveryWorld environments (Proteomics and Reactor Lab at Easy difficulty). The plan included three experimental modes (MINI_PILOT, PILOT, and FULL_EXPERIMENT) with varying numbers of episodes and steps. Metrics to be tracked included Task Score, number of actions, plan quality, measurement success rate, time taken, and error recoveries. However, since no experimental results were provided, it is impossible to determine whether the hypothesis was supported, refuted, or if the results were inconclusive. A proper meta-analysis would require the actual experimental data.	
two-stage-game-generation	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	A two-stage game generation approach (basic mechanics first, then scoring/win conditions) will result in higher technical validity compared to generating complete games in one pass	3	1	1	mixed information	This meta-analysis examined five experimental runs comparing single-stage versus two-stage text game generation approaches. The central hypothesis was that a two-stage approach (generating basic mechanics first, then adding scoring/win conditions) would result in higher technical validity than generating complete games in one pass.  Three experiments (copies 3, 4, and 5) supported the hypothesis, showing that the two-stage approach produced games with higher mechanical completeness and consistency. Copy4 provided the strongest evidence, with the two-stage approach achieving 100% execution success and mechanics completion compared to 93.3% execution success and only 66.7% mechanics completion for the single-stage approach. Copy5 showed that while both methods had execution success, the two-stage approach was more consistent in implementing specific mechanics like the take() method (100% vs 53.3%).  One experiment (copy1) refuted the hypothesis, finding no difference in technical validity between the approaches, with both methods achieving 100% success rates in generating valid, executable games with all required mechanics.  One experiment (copy2) was inconclusive due to data aggregation issues, though logs suggested both methods could generate functional games.  A consistent finding across all experiments was that the two-stage approach required significantly more generation time (approximately 1.5-2x longer) than the single-stage approach. This represents a clear trade-off: improved technical validity at the cost of increased generation time.  In summary, the evidence predominantly supports the hypothesis that a two-stage game generation approach results in higher technical validity compared to a single-stage approach. However, this comes with a consistent time penalty that should be considered when choosing between these approaches. The experiments were well-controlled with consistent evaluation metrics, lending credibility to these findings.	755955293195, 782415327844, 320926979841, 230389161857, 216723723933
wordnet-cooking-exploration	my-benchmark-run-full50-withexpertnotes-5variations-commonlibraryideator-sonnet35-1-2025-02-04-15-09-26	Using WordNet's hypernym/hyponym relationships to identify food-related objects will lead to more efficient exploration in CookingWorld cooking tasks compared to random exploration.	1	0	1	limited information	This meta-analysis examined two experiments testing whether WordNet-guided exploration improves efficiency in CookingWorld cooking tasks compared to random exploration. Both experiments implemented a WordNet-guided agent that used NLTK to identify food-related nouns and biased action selection (80/20) toward food-related actions, comparing it against a random baseline. The experiments were run in PILOT mode with 20 episodes and max 50 steps per episode.  The first experiment showed a slight advantage for the WordNet-guided agent (mean score 0.161 vs. 0.139 for random), but this difference was not statistically significant (p=0.2682). The second experiment demonstrated a more substantial advantage for the WordNet-guided agent (mean score 0.258 vs. 0.153 for random), with results trending toward significance (p=0.052).  Notably, neither agent in either experiment achieved full task success in any episode, suggesting that both approaches may need further refinement for the CookingWorld environment. The trending significance in the second experiment, despite the small sample size (10 seeds), suggests that WordNet guidance may indeed provide benefits for exploration efficiency.  Overall, the evidence leans toward supporting the hypothesis that WordNet-guided exploration improves efficiency, but with important caveats: (1) the effects appear modest, (2) statistical significance was not consistently achieved, and (3) complete task success remains elusive. These findings suggest that semantic knowledge from WordNet can help guide exploration in text-based environments, but may need to be combined with other techniques to achieve robust task completion. Future work should include larger sample sizes, longer episodes, or more sophisticated integration of WordNet knowledge to fully validate the approach.	158925847589, 851595307701
