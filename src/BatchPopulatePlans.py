# BatchPopulateplans.py
# Populates a list of pre-generated ideas with plans/operationalizations generated by the operationalizer

import os
import time
import json

import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# Utility for querying LLMs
from ExtractionUtils import *

# Codeblock Store
from CodeBlockStore import *
# Idea Store
from IdeaStore import *



# Load the benchmark
def loadBenchmark(filenameIn:str):
    data = []
    with open(filenameIn, "r") as f:
        data = json.load(f)
    print("Loaded " + str(len(data)) + " benchmark entries")
    return data


def populate_operationalization_one_idea_simple_method(idea:dict, model_str:str, extra_conditioning_text:str=None, include_expert_notes:bool=False):
        # Load a copy of the ideastore
        ideaStore = IdeaStore()

        # Sanitize the idea
        import copy
        idea_santized = copy.deepcopy(idea)
        # Remove the 'id', 'metadata', 'scores', 'rating' fields
        if "id" in idea_santized:
            del idea_santized["id"]
        if "metadata" in idea_santized:
            del idea_santized["metadata"]
        if "scores" in idea_santized:
            del idea_santized["scores"]
        if "rating" in idea_santized:
            del idea_santized["rating"]
        # Expert notes
        expert_notes = None
        if ("rating_notes" in idea_santized):
            expert_notes = idea_santized["rating_notes"]
            del idea_santized["rating_notes"]
        if (include_expert_notes == False):
            expert_notes = None

        # Convert the idea to an experiment prompt
        experiment_prompt = ideaStore.convert_idea_to_experiment_prompt(idea, model_str, extra_conditioning_text=extra_conditioning_text, expert_notes=expert_notes)
                # "success": True,
                # "prompt": prompt_experiment,
                # "codeblocks": codeblocks,
                # "cost": cost,
                # "time_seconds": deltaTime

        success = experiment_prompt.get("success", False)
        if (success == False):
            result = {
                "success": False,
                "error": "Failed to convert idea to experiment prompt"
            }
            return result

        # Pack the operationalization result
        try:
            result = {
                "success": True,
                "operationalization_method": "simple",
                "operationalization_model": model_str,
                "operationalization_extra_conditioning_text": extra_conditioning_text,
                "operationalization_include_expert_notes": include_expert_notes,
                "operationalization_expert_notes": expert_notes,
                "operationalization_description": experiment_prompt["prompt"],
                "operationalization_codeblocks": experiment_prompt["codeblocks"],
                "operationalization_cost": experiment_prompt["cost"],
                "operationalizatoin_time_seconds": experiment_prompt["time_seconds"]
            }
        except Exception as e:
            result = {
                "success": False,
                "error": str(e)
            }

        return result


def populate_operationalization_one_idea(idea:dict, model_str:str, operationalization_method:str="", extra_conditioning_text:str=None, include_expert_notes:bool=False):
    if (operationalization_method == "simple"):
        operationalization = populate_operationalization_one_idea_simple_method(idea, model_str, extra_conditioning_text=extra_conditioning_text, include_expert_notes=include_expert_notes)
        if (operationalization["success"] == True):
            idea["operationalization"] = operationalization
            return idea
    else:
        return None




# Main
def main(BENCHMARK_INPUT:str, BENCHMARK_OUTPUT:str, extra_conditioning_text:str=None, include_expert_notes:bool=False):
    # Load the API keys
    loadAPIKeys()

    # Load the benchmark
    benchmark = loadBenchmark(BENCHMARK_INPUT)

    model_str = "claude-3-5-sonnet-20241022"
    operationalization_method = "simple"

    # TODO: Should probably modify filename based on the operationalization method
    extra_filename_text = "." + operationalization_method + "." + model_str

    if (extra_conditioning_text is not None):
        extra_filename_text += ".conditioned"
    if (include_expert_notes == True):
        extra_filename_text += ".expert_notes"
    BENCHMARK_OUTPUT = BENCHMARK_OUTPUT.replace(".json", extra_filename_text + ".json")

    # Try to populate the plans/operationalizations
    ideas_with_operationalizations = []
    def process_operationalize_idea(idea, lock):
        id = idea.get("id", "unknown")
        print(f"Running operationalization for idea: {id}")

        try:
            #result = convert_to_simpler_idea(idea, model_str)
            operationalized_idea = populate_operationalization_one_idea(idea, model_str=model_str, operationalization_method=operationalization_method, extra_conditioning_text=extra_conditioning_text, include_expert_notes=include_expert_notes)
            if (operationalized_idea is None):
                print("ERROR: Operationalization was not successful for idea: " + id)
                return

            with lock:
                ideas_with_operationalizations.append(operationalized_idea)

        except Exception as e:
            print(f"ERROR: Could not run operationalization for idea: {idea.get('id', 'unknown')}")
            print(e)
            traceback.print_exc()

        # Sleep for a bit (optional, if you still want that delay)
        time.sleep(1)


    def run_in_threads(ideas, model_str):
        # Create a lock for the ideaStore
        lock = threading.Lock()
        from tqdm import tqdm
        # You can wrap the logic in a ThreadPoolExecutor
        with ThreadPoolExecutor(max_workers=5) as executor:
            # Submit a future for each paper set
            futures = [
                executor.submit(process_operationalize_idea, idea, lock)
                for idea in ideas
            ]

            # If you want a progress bar that updates as each future completes:
            for future in tqdm(as_completed(futures), total=len(futures)):
                # Gather the results (or catch exceptions) if needed
                try:
                    future.result()  # This will re-raise any exception that occurred
                except Exception as e:
                    print("Exception in thread:", e)
                    traceback.print_exc()

    run_in_threads(benchmark, model_str)

    # Save
    print("Operationalized " + str(len(ideas_with_operationalizations)) + " ideas")
    print("Saving " + str(len(ideas_with_operationalizations)) + " ideas to " + BENCHMARK_OUTPUT)
    with open(BENCHMARK_OUTPUT, "w") as f:
        json.dump(ideas_with_operationalizations, f, indent=4)
    # Record errors (difference between benchmark and operationalized ideas)
    num_errors = len(benchmark) - len(ideas_with_operationalizations)
    print("ERRORS: Was unable to operationalize " + str(num_errors) + " ideas")



if __name__ == "__main__":
    # The input and output files.
    # The input file should be a set of ideas from the batch ideator.
    # The output file will be the same set of ideas, but with plans/operationalizations added.
    BENCHMARK_INPUT = "batch-generation-example-output.ranked.simplified.filtered.json"
    BENCHMARK_OUTPUT = "ideastore_benchmark-batch-generation-example-output.ranked.simplified.filtered.withplans.json"

    # Extra conditioning text to use when converting ideas to experiment plans/prompts
    extra_conditioning_text = "Please use `gpt-4o-mini` for all LLM calls, because it's fast and inexpensive."

    # Enable including expert notes (if they exist)
    include_expert_notes = False
    #include_expert_notes = True

    # Run the planner
    main(
        BENCHMARK_INPUT=BENCHMARK_INPUT,
        BENCHMARK_OUTPUT=BENCHMARK_OUTPUT,
        extra_conditioning_text=extra_conditioning_text,
        include_expert_notes=include_expert_notes
    )
